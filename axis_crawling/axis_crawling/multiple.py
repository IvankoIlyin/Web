import scrapy
import logging
import datetime
import requests as rq
import re
from bs4 import BeautifulSoup
import json
import tldextract
import gc
from constants import *


def handle_error(failure):
    pass


class TamimiJudgmentsSpider(scrapy.Spider):
    name = "TamimiJudgmentsSpider"
    allowed_domains = ["tamimi.com"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.h-thumb-content')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "https://" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www.tamimi.com" + link.css("a").attrib["href"],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class RiadRiadPublicationsSpider(scrapy.Spider):
    name = "RiadRiadPublicationsSpider"
    allowed_domains = ["riad-riad.com"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip
        logging.info(
            {
                "proxy": "1",
                "clean_url": self.allowed_domains[0],
                "link": self.start_urls,
            }
        )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.has-text-color')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "https://" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"],
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://riad-riad.com" + link.css("a").attrib["href"],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class DeloitteSpider(scrapy.Spider):
    name = "DeloitteSpider"
    allowed_domains = ["deloitte.com"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.featuredpromo.section, .topic-description-content')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"],
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www2.deloitte.com" + link.css("a").attrib["href"],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class CharlesRussellSpeechlysSpider(scrapy.Spider):
    name = "CharlesRussellSpeechlysSpider"
    allowed_domains = ["charlesrussellspeechlys.com"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.col-sm-6.col-lg-4')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "https://" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"],
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www.charlesrussellspeechlys.com" + link.css("a").attrib["href"],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class FiccOrgSpider(scrapy.Spider):
    name = "FiccOrgSpider"
    allowed_domains = ["ficc.org.iq"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

        logging.info(
            {
                "proxy": "1",
                "clean_url": self.allowed_domains[0],
                "link": self.start_urls,
            }
        )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.entry-title')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "https://" in str(link.css("a").attrib["href"]) or "http://" in str(
                                    link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"],
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "http://ficc.org.iq" + link.css("a").attrib["href"],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class SfdaRegulationSpider(scrapy.Spider):
    name = "SfdaRegulationSpider"
    allowed_domains = ["sfda.gov.sa"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.warning-item')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "https://" in str(link.css(".download-doc-link").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('.m-c-title::text').get().strip(),
                                       "excerpt": 'na',
                                       "published_date": link.css('.news-date::text').get().strip() if link.css(
                                           '.news-date::text').get().strip() else 'na',
                                       "pdf_url": link.css(".download-doc-link").attrib["href"],

                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('.m-c-title::text').get().strip(),
                                       "excerpt": 'na',
                                       "published_date": link.css('.news-date::text').get().strip() if link.css(
                                           '.news-date::text').get().strip() else 'na',
                                       "pdf_url": "https://www.sfda.gov.sa" + link.css(".download-doc-link").attrib[
                                           "href"],

                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class SfdaCircularSpider(scrapy.Spider):
    name = "SfdaCircularSpider"
    allowed_domains = ["sfda.gov.sa"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                categories_links = []
                res = response.css('.download-doc-link')
                for r in res:
                    try:
                        if "https://" in str(r.css("a").attrib["href"]) or "http://" in str(r.css("a").attrib["href"]):
                            categories_links.append(r.css("a").attrib["href"])
                        else:
                            categories_links.append(
                                "https://www.sfda.gov.sa"
                                + r.css("a").attrib["href"]
                            )

                    except:
                        pass

                for i in set(categories_links):
                    try:
                        yield scrapy.Request(
                            i,
                            callback=self.article_links,
                            meta={"dont_retry": True, "download_timeout": cat_timeout,"base_url": response.url},
                            errback=handle_error,
                        )


                    except:
                        pass

    def article_links(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                base_url = response.meta.get('base_url')
                data = response.css('.warning-item,.sing-date')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "https://" in str(link.css(".download-doc-link").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": base_url,
                                       "title": link.css('.m-c-title::text').get().strip(),
                                       "excerpt": 'na',
                                       "published_date": response.css('.sing-date::text').get().strip() if response.css(
                                           '.sing-date::text').get().strip() else 'na',
                                       "link": link.css(".download-doc-link").attrib["href"],

                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": base_url,
                                       "title": link.css('.m-c-title::text').get().strip(),
                                       "excerpt": 'na',
                                       "published_date": response.css('.sing-date::text').get().strip() if response.css(
                                           '.sing-date::text').get().strip() else 'na',
                                       "link": "https://www.sfda.gov.sa" + link.css(".download-doc-link").attrib[
                                           "href"],

                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class UqnCatSpider(scrapy.Spider):
    name = "UqnCatSpider"
    allowed_domains = ["uqn.gov.sa"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.td-module-meta-info')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('a').attrib['title'].strip() if link.css('a') else 'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": link.css("a").attrib["href"],
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                        "title": link.css('a').attrib['title'].strip() if link.css('a') else 'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": "https://www.uqn.gov.sa" + link.css("a").attrib["href"]
                                       }
                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class FraCapitalSpider(scrapy.Spider):
    name = "FraCapitalSpider"
    allowed_domains = ["fra.gov.eg"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.pdf')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('a::text').get().strip(),
                                       "excerpt": link.css('p::text').get().strip(),
                                       "published_date": 'na',
                                       "pdf_url": link.css("a").attrib["href"],

                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('a::text').get().strip(),
                                       "excerpt": link.css('p::text').get().strip(),
                                       "published_date": 'na',
                                       "pdf_url": "https://fra.gov.eg" + link.css("a").attrib["href"],

                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class TobaccoControllawsSpider(scrapy.Spider):
    name = "TobaccoControllawsSpider"
    allowed_domains = ["tobaccocontrollaws.org"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                categories_links = []
                res = response.css('.result')
                for r in res:
                    try:
                        if "https://" in str(r.css("a").attrib["href"]) or "http://" in str(r.css("a").attrib["href"]):
                            categories_links.append(r.css("a").attrib["href"])
                        else:
                            categories_links.append(
                                "https://www.tobaccocontrollaws.org"
                                + r.css("a").attrib["href"]
                            )

                    except:
                        pass

                for i in set(categories_links):
                    try:
                        yield scrapy.Request(
                            i,
                            callback=self.article_links,
                            meta={"dont_retry": True, "download_timeout": cat_timeout,"base_url": response.url},
                            errback=handle_error,
                        )


                    except:
                        pass

        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1

    def article_links(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                try:
                    base_url = response.meta.get('base_url')
                    data = response.css('.downloadContainer')
                except:
                    data = []
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "https://" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": base_url,
                                       "title": response.css('.litigation_by_country > h3::text').get().strip(),
                                       "excerpt": response.css('h3+ p::text').get().strip(),
                                       "published_date": response.css(
                                           '.txt_LT:nth-child(4)::text').get() if response.css(
                                           '.txt_LT:nth-child(4)::text').get() else datetime.datetime.utcnow().strftime(
                                           '%Y-%m-%d %H:%M:%S'),
                                       "link": link.css("a").attrib["href"],

                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": base_url,
                                       "title": response.css('.litigation_by_country > h3::text').get().strip(),
                                       "excerpt": response.css('h3+ p::text').get().strip(),
                                       "published_date": response.css(
                                           '.txt_LT:nth-child(4)::text').get() if response.css(
                                           '.txt_LT:nth-child(4)::text').get() else datetime.datetime.utcnow().strftime(
                                           '%Y-%m-%d %H:%M:%S'),
                                       "link": "https://www.tobaccocontrollaws.org" + link.css("a").attrib["href"],

                                       }
                    except:
                        pass
        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1


class TobaccoControllaws2Spider(scrapy.Spider):
    name = "TobaccoControllaws2Spider"
    allowed_domains = ["tobaccocontrollaws.org"]
    not_allowed_keyword = []
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                try:
                    data = response.css('.accordion-row')
                except:
                    data = []
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('h3::text').get().strip() if link.css('h3::text') else 'na',
                                       "excerpt": link.css('.comment.pull-left').css(
                                           'p::text').get().strip() if link.css('.comment.pull-left').css(
                                           'p::text') else 'na',
                                       "published_date": link.css(
                                           '.col-4.col-md-3.pt-1.px-1.law_date.text-left::text').get().strip() if link.css(
                                           '.col-4.col-md-3.pt-1.px-1.law_date.text-left').get() else 'na',
                                       "link": link.css('a').attrib['href'] if
                                       link.css('a') else 'na'
                                       }

                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('h3::text').get().strip() if link.css('h3::text') else 'na',
                                       "excerpt": link.css('.comment.pull-left').css(
                                           'p::text').get().strip() if link.css('.comment.pull-left').css(
                                           'p::text') else 'na',
                                       "published_date": link.css(
                                           '.col-4.col-md-3.pt-1.px-1.law_date.text-left::text').get().strip() if link.css(
                                           '.col-4.col-md-3.pt-1.px-1.law_date.text-left').get() else 'na',
                                       "link": "https://www.tobaccocontrollaws.org" +
                                                      link.css('a').attrib['href'] if
                                       link.css('a') else 'na',
                                       }
                    except:
                        pass
        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1


class UqnCatArchiveSpider(scrapy.Spider):
    name = "UqnCatArchiveSpider"
    allowed_domains = ["uqn.gov.sa"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                categories_links = []
                res = response.css('.td-module-thumb')
                for r in res:
                    try:
                        if "http" in str(r.css("a").attrib["href"]) or "http://" in str(r.css("a").attrib["href"]):
                            categories_links.append(r.css("a").attrib["href"])
                        else:
                            categories_links.append(
                                "https://www.uqn.gov.sa"
                                + r.css("a").attrib["href"]
                            )

                    except:
                        pass

                for i in set(categories_links):
                    try:
                        yield scrapy.Request(
                            i,
                            callback=self.article_links,
                            meta={"dont_retry": True, "download_timeout": cat_timeout,"base_url": response.url},
                            errback=handle_error,
                        )


                    except:
                        pass

        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1

    def article_links(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                try:
                    base_url = response.meta.get('base_url')
                    data = response.css('.vc_column.tdi_60.wpb_column.vc_column_container.tdc-column.td-pb-span12')
                except:
                    data = []
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css('.wp-block-file__button').css('a').attrib['href']):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": base_url,
                                       "title": link.css('.tdb-title-text::text').get().strip(),
                                       "excerpt": 'na',
                                       "published_date": link.css(
                                           '.entry-date.updated.td-module-date::text').get() if link.css(
                                           '.entry-date.updated.td-module-date::text').get() else 'na',
                                       "pdf_url": link.css('.wp-block-file__button').css('a').attrib['href'],
                                       "topic": "UQN - KSA Official Gazette - Archive",
                                       "type": "PDF"}
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": base_url,
                                       "title": link.css('.tdb-title-text::text').get().strip(),
                                       "excerpt": 'na',
                                       "published_date": link.css(
                                           '.entry-date.updated.td-module-date::text').get() if link.css(
                                           '.entry-date.updated.td-module-date::text').get() else 'na',
                                       "pdf_url": "https://www.uqn.gov.sa" +
                                                  link.css('.wp-block-file__button').css('a').attrib['href'],
                                       "topic": "UQN - KSA Official Gazette - Archive",
                                       "type": "PDF"}
                    except:
                        pass
        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1


class MaqamLawSpider(scrapy.Spider):
    name = "MaqamLawSpider"
    allowed_domains = ["maqamlaw.com"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                self.start_urls,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

        logging.info(
            {
                "proxy": "1",
                "clean_url": self.allowed_domains[0],
                "link": self.start_urls,
            }
        )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('._0Z7nH._1vXcx.BTLOi')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "https://" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"],
                                       "type": 'article'
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www.maqamlaw.com" + link.css("a").attrib["href"],
                                       "type": 'article'
                                       }
                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class KpmgItalySpider(scrapy.Spider):
    name = "KpmgItalySpider"
    allowed_domains = ['home.kpmg']
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    cookies = {
        'gig_canary': 'false',
        'visit_settings': '%7B%22count%22%3A3%2C%22mins%22%3A30%2C%22days%22%3A14%7D',
        'gig_bootstrap_3_dAO2wSinyCBsporhy0kFd0EaIUU-fISB9pq_GaL34bTZzaKzpkZFdHtGcrL0AqKn': 'login_ver4',
        'at_check': 'true',
        'AMCVS_00B621ED542E84FD0A4C98A1%40AdobeOrg': '1',
        '_ga': 'GA1.2.340825921.1655807009',
        'gig_canary_ver': '13186-3-27598005',
        '__hstc': '214917896.8cfc5ea1aae695d6eafcaee602ca9a0f.1655882982959.1655882982959.1655882982959.1',
        'hubspotutk': '8cfc5ea1aae695d6eafcaee602ca9a0f',
        '__hssrc': '1',
        'AMCV_00B621ED542E84FD0A4C98A1%40AdobeOrg': '-408604571%7CMCIDTS%7C19165%7CMCMID%7C47947784634070791523896494648365507125%7CMCAAMLH-1656487783%7C12%7CMCAAMB-1656487783%7CRKhpRz8krg2tLO6pguXWp5olkAcUniQYPHaMWWgdJ3xzPWQmdj0y%7CMCOPTOUT-1655890183s%7CNONE%7CvVersion%7C4.6.0',
        's_ips': '821',
        's_vnum': '1656613800038%26vn%3D1',
        's_cc': 'true',
        'personalization': '%7B%22industry%22%3A%22Education%22%2C%22subindustry%22%3A%22University%22%2C%22zthesIDs%22%3A%22%22%7D',
        'visit_count': '1655882994614-2-1656998052849',
        's_evar38_per': 'Market%20Entry%20Germany',
        's_tp': '4456',
        's_ppv': 'de%253Aen%253Ahome%253Ainsights%2C18%2C18%2C821%2C1%2C5',
        's_nr30': '1655882995826-New',
        'sat_track': 'false',
        'mbox': 'PC#7c087511341343a287189306a920564c.31_0#1719051811|session#dee30317531e486c9e68a254c7b9d880#1655894344',
    }

    headers = {
        'authority': 'home.kpmg',
        'accept': '*/*',
        'accept-language': 'en-US,en;q=0.9,hi;q=0.8',
        'origin': 'https://home.kpmg',
        'referer': 'https://home.kpmg/it/it/home/insights.html',
        'sec-ch-ua': '".Not/A)Brand";v="99", "Google Chrome";v="103", "Chromium";v="103"',
        'sec-ch-ua-mobile': '?0',
        'sec-ch-ua-platform': '"macOS"',
        'sec-fetch-dest': 'empty',
        'sec-fetch-mode': 'cors',
        'sec-fetch-site': 'same-origin',
        'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36',
    }

    json_data = {
        'query': '',
        'filters': {
            'all': [
                {
                    'kpmg_tab_type': [
                        'Insights',
                    ],
                },
            ],
        },
        'result_fields': {
            'kpmg_image': {
                'raw': {},
            },
            'kpmg_description': {
                'raw': {},
            },
            'kpmg_banner_flag': {
                'raw': {},
            },
            'kpmg_primary_tag': {
                'raw': {},
            },
            'kpmg_article_date': {
                'raw': {},
            },
            'kpmg_contact_job_ttl': {
                'raw': {},
            },
            'kpmg_article_readtime': {
                'raw': {},
            },
            'kpmg_title': {
                'raw': {},
            },
            'kpmg_contact_fn': {
                'raw': {},
            },
            'kpmg_contact_ln': {
                'raw': {},
            },
            'kpmg_event_type': {
                'raw': {},
            },
            'kpmg_contact_city': {
                'raw': {},
            },
            'kpmg_event_start_time': {
                'raw': {},
            },
            'kpmg_article_date_time': {
                'raw': {},
            },
            'kpmg_contact_country': {
                'raw': {},
            },
            'kpmg_tab_type': {
                'raw': {},
            },
            'kpmg_short_desc': {
                'raw': {},
            },
            'kpmg_article_primary_format': {
                'raw': {},
            },
            'kpmg_article_type': {
                'raw': {},
            },
            'kpmg_event_startdate': {
                'raw': {},
            },
            'kpmg_url': {
                'raw': {},
            },
            'kpmg_template_type': {
                'raw': {},
            },
        },
        'page': {
            'size': 40,
            'current': 1,
        },
        'sort': {
            'kpmg_filter_date': 'desc',
        },
    }

    def parse(self, response):
        responses = rq.post('https://home.kpmg/esearch/it-it', cookies=self.cookies, headers=self.headers,
                            json=self.json_data)
        result = responses.json()['results']
        for i in result:
            yield {"clean_url": self.allowed_domains[0],
                   "base_url": self.start_urls[0],
                   'link': i['kpmg_url']['raw'],
                   "type": 'article'}


class KpmgBahrainSpider(scrapy.Spider):
    name = "KpmgBahrainSpider"
    allowed_domains = ['home.kpmg']
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    cookies = {
        'gig_canary': 'false',
        'visit_settings': '%7B%22count%22%3A3%2C%22mins%22%3A30%2C%22days%22%3A14%7D',
        'gig_bootstrap_3_dAO2wSinyCBsporhy0kFd0EaIUU-fISB9pq_GaL34bTZzaKzpkZFdHtGcrL0AqKn': 'login_ver4',
        'at_check': 'true',
        'AMCVS_00B621ED542E84FD0A4C98A1%40AdobeOrg': '1',
        '_ga': 'GA1.2.340825921.1655807009',
        'gig_canary_ver': '13186-3-27598005',
        '__hstc': '214917896.8cfc5ea1aae695d6eafcaee602ca9a0f.1655882982959.1655882982959.1655882982959.1',
        'hubspotutk': '8cfc5ea1aae695d6eafcaee602ca9a0f',
        '__hssrc': '1',
        's_ips': '821',
        's_vnum': '1656613800038%26vn%3D1',
        's_cc': 'true',
        'personalization': '%7B%22industry%22%3A%22Education%22%2C%22subindustry%22%3A%22University%22%2C%22zthesIDs%22%3A%22%22%7D',
        'visit_count': '1655882994614-2-1656998052849',
        's_evar38_per': 'Market%20Entry%20Germany',
        's_tp': '4456',
        's_ppv': 'de%253Aen%253Ahome%253Ainsights%2C18%2C18%2C821%2C1%2C5',
        's_nr30': '1655882995826-New',
        'sat_track': 'false',
        'AMCV_00B621ED542E84FD0A4C98A1%40AdobeOrg': '-408604571%7CMCIDTS%7C19173%7CMCMID%7C47947784634070791523896494648365507125%7CMCAAMLH-1657101779%7C12%7CMCAAMB-1657101779%7CRKhpRz8krg2tLO6pguXWp5olkAcUniQYPHaMWWgdJ3xzPWQmdj0y%7CMCOPTOUT-1656504179s%7CNONE%7CvVersion%7C4.6.0',
        '_gid': 'GA1.2.1052437806.1656496980',
        '_gat': '1',
        'dmdbase_cdc': 'DBSET',
        'mbox': 'PC#7c087511341343a287189306a920564c.31_0#1719051811|session#96f4cf4bc6214eeb91c4e90305b1d6c5#1656567355',
    }

    headers = {
        'authority': 'home.kpmg',
        'accept': '*/*',
        'accept-language': 'en-US,en;q=0.9,hi;q=0.8',
        'origin': 'https://home.kpmg',
        'referer': 'https://home.kpmg/bh/en/home/insights.html',
        'sec-ch-ua': '".Not/A)Brand";v="99", "Google Chrome";v="103", "Chromium";v="103"',
        'sec-ch-ua-mobile': '?0',
        'sec-ch-ua-platform': '"macOS"',
        'sec-fetch-dest': 'empty',
        'sec-fetch-mode': 'cors',
        'sec-fetch-site': 'same-origin',
        'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36',
    }

    json_data = {
        'query': '',
        'filters': {
            'all': [
                {
                    'kpmg_tab_type': [
                        'Insights',
                    ],
                },
            ],
        },
        'result_fields': {
            'kpmg_image': {
                'raw': {},
            },
            'kpmg_description': {
                'raw': {},
            },
            'kpmg_banner_flag': {
                'raw': {},
            },
            'kpmg_primary_tag': {
                'raw': {},
            },
            'kpmg_article_date': {
                'raw': {},
            },
            'kpmg_contact_job_ttl': {
                'raw': {},
            },
            'kpmg_article_readtime': {
                'raw': {},
            },
            'kpmg_title': {
                'raw': {},
            },
            'kpmg_contact_fn': {
                'raw': {},
            },
            'kpmg_contact_ln': {
                'raw': {},
            },
            'kpmg_event_type': {
                'raw': {},
            },
            'kpmg_contact_city': {
                'raw': {},
            },
            'kpmg_event_start_time': {
                'raw': {},
            },
            'kpmg_article_date_time': {
                'raw': {},
            },
            'kpmg_contact_country': {
                'raw': {},
            },
            'kpmg_tab_type': {
                'raw': {},
            },
            'kpmg_short_desc': {
                'raw': {},
            },
            'kpmg_article_primary_format': {
                'raw': {},
            },
            'kpmg_article_type': {
                'raw': {},
            },
            'kpmg_event_startdate': {
                'raw': {},
            },
            'kpmg_url': {
                'raw': {},
            },
            'kpmg_template_type': {
                'raw': {},
            },
        },
        'page': {
            'size': 40,
            'current': 1,
        },
        'sort': {
            'kpmg_filter_date': 'desc',
        },
    }

    def parse(self, response):
        responses = rq.post('https://home.kpmg/esearch/bh-en', cookies=self.cookies, headers=self.headers,
                            json=self.json_data)
        result = responses.json()['results']
        for i in result:
            yield {"clean_url": self.allowed_domains[0],
                   "base_url": self.start_urls[0],
                   'link': i['kpmg_url']['raw'],
                   "type": 'article'}


class KpmgEgyptSpider(scrapy.Spider):
    name = "KpmgEgyptSpider"
    allowed_domains = ['home.kpmg']
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    cookies = {
        'gig_canary': 'false',
        'visit_settings': '%7B%22count%22%3A3%2C%22mins%22%3A30%2C%22days%22%3A14%7D',
        'gig_bootstrap_3_dAO2wSinyCBsporhy0kFd0EaIUU-fISB9pq_GaL34bTZzaKzpkZFdHtGcrL0AqKn': 'login_ver4',
        'at_check': 'true',
        'AMCVS_00B621ED542E84FD0A4C98A1%40AdobeOrg': '1',
        '_ga': 'GA1.2.340825921.1655807009',
        '__hstc': '214917896.8cfc5ea1aae695d6eafcaee602ca9a0f.1655882982959.1655882982959.1655882982959.1',
        'hubspotutk': '8cfc5ea1aae695d6eafcaee602ca9a0f',
        '__hssrc': '1',
        's_ips': '821',
        's_vnum': '1656613800038%26vn%3D1',
        's_cc': 'true',
        'personalization': '%7B%22industry%22%3A%22Education%22%2C%22subindustry%22%3A%22University%22%2C%22zthesIDs%22%3A%22%22%7D',
        'visit_count': '1655882994614-2-1656998052849',
        's_evar38_per': 'Market%20Entry%20Germany',
        's_tp': '4456',
        's_ppv': 'de%253Aen%253Ahome%253Ainsights%2C18%2C18%2C821%2C1%2C5',
        's_nr30': '1655882995826-New',
        'sat_track': 'false',
        'AMCV_00B621ED542E84FD0A4C98A1%40AdobeOrg': '-408604571%7CMCIDTS%7C19173%7CMCMID%7C47947784634070791523896494648365507125%7CMCAAMLH-1657101779%7C12%7CMCAAMB-1657101779%7CRKhpRz8krg2tLO6pguXWp5olkAcUniQYPHaMWWgdJ3xzPWQmdj0y%7CMCOPTOUT-1656504179s%7CNONE%7CvVersion%7C4.6.0',
        '_gid': 'GA1.2.1052437806.1656496980',
        'dmdbase_cdc': 'DBSET',
        '_gat': '1',
        'gig_canary_ver': '13196-3-27609435',
        'mbox': 'PC#7c087511341343a287189306a920564c.31_0#1719051811|session#96f4cf4bc6214eeb91c4e90305b1d6c5#1656567716',
    }

    headers = {
        'authority': 'home.kpmg',
        'accept': '*/*',
        'accept-language': 'en-US,en;q=0.9,hi;q=0.8',
        'origin': 'https://home.kpmg',
        'referer': 'https://home.kpmg/eg/en/home/insights0.html',
        'sec-ch-ua': '".Not/A)Brand";v="99", "Google Chrome";v="103", "Chromium";v="103"',
        'sec-ch-ua-mobile': '?0',
        'sec-ch-ua-platform': '"macOS"',
        'sec-fetch-dest': 'empty',
        'sec-fetch-mode': 'cors',
        'sec-fetch-site': 'same-origin',
        'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36',
    }

    json_data = {
        'query': '',
        'filters': {
            'all': [
                {
                    'kpmg_tab_type': [
                        'Insights',
                    ],
                },
                {
                    'kpmg_article_type': [
                        'Article-General',
                    ],
                },
                {
                    'kpmg_template_type': [
                        'article-details-template',
                        'insights-flexible-template',
                        'editable-flex-template',
                        'editable-campaign-template',
                    ],
                },
            ],
        },
        'result_fields': {
            'kpmg_image': {
                'raw': {},
            },
            'kpmg_description': {
                'raw': {},
            },
            'kpmg_banner_flag': {
                'raw': {},
            },
            'kpmg_primary_tag': {
                'raw': {},
            },
            'kpmg_article_date': {
                'raw': {},
            },
            'kpmg_contact_job_ttl': {
                'raw': {},
            },
            'kpmg_article_readtime': {
                'raw': {},
            },
            'kpmg_title': {
                'raw': {},
            },
            'kpmg_contact_fn': {
                'raw': {},
            },
            'kpmg_contact_ln': {
                'raw': {},
            },
            'kpmg_event_type': {
                'raw': {},
            },
            'kpmg_contact_city': {
                'raw': {},
            },
            'kpmg_event_start_time': {
                'raw': {},
            },
            'kpmg_article_date_time': {
                'raw': {},
            },
            'kpmg_contact_country': {
                'raw': {},
            },
            'kpmg_tab_type': {
                'raw': {},
            },
            'kpmg_short_desc': {
                'raw': {},
            },
            'kpmg_article_primary_format': {
                'raw': {},
            },
            'kpmg_article_type': {
                'raw': {},
            },
            'kpmg_event_startdate': {
                'raw': {},
            },
            'kpmg_url': {
                'raw': {},
            },
            'kpmg_template_type': {
                'raw': {},
            },
        },
        'page': {
            'size': 20,
            'current': 1,
        },
        'sort': {
            'kpmg_filter_date': 'desc',
        },
    }

    def parse(self, response):
        responses = rq.post('https://home.kpmg/esearch/eg-en', cookies=self.cookies, headers=self.headers,
                            json=self.json_data)
        result = responses.json()['results']
        for i in result:
            yield {"clean_url": self.allowed_domains[0],
                   "base_url": self.start_urls[0],
                   'link': i['kpmg_url']['raw'],
                   "type": 'article'}


class KpmgKuwaitSpider(scrapy.Spider):
    name = "KpmgKuwaitSpider"
    allowed_domains = ['home.kpmg']
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    cookies = {
        'gig_canary': 'false',
        'visit_settings': '%7B%22count%22%3A3%2C%22mins%22%3A30%2C%22days%22%3A14%7D',
        'gig_bootstrap_3_dAO2wSinyCBsporhy0kFd0EaIUU-fISB9pq_GaL34bTZzaKzpkZFdHtGcrL0AqKn': 'login_ver4',
        'at_check': 'true',
        'AMCVS_00B621ED542E84FD0A4C98A1%40AdobeOrg': '1',
        '_ga': 'GA1.2.340825921.1655807009',
        '__hstc': '214917896.8cfc5ea1aae695d6eafcaee602ca9a0f.1655882982959.1655882982959.1655882982959.1',
        'hubspotutk': '8cfc5ea1aae695d6eafcaee602ca9a0f',
        '__hssrc': '1',
        's_ips': '821',
        's_vnum': '1656613800038%26vn%3D1',
        's_cc': 'true',
        'personalization': '%7B%22industry%22%3A%22Education%22%2C%22subindustry%22%3A%22University%22%2C%22zthesIDs%22%3A%22%22%7D',
        'visit_count': '1655882994614-2-1656998052849',
        's_evar38_per': 'Market%20Entry%20Germany',
        's_tp': '4456',
        's_ppv': 'de%253Aen%253Ahome%253Ainsights%2C18%2C18%2C821%2C1%2C5',
        's_nr30': '1655882995826-New',
        'sat_track': 'false',
        'AMCV_00B621ED542E84FD0A4C98A1%40AdobeOrg': '-408604571%7CMCIDTS%7C19173%7CMCMID%7C47947784634070791523896494648365507125%7CMCAAMLH-1657101779%7C12%7CMCAAMB-1657101779%7CRKhpRz8krg2tLO6pguXWp5olkAcUniQYPHaMWWgdJ3xzPWQmdj0y%7CMCOPTOUT-1656504179s%7CNONE%7CvVersion%7C4.6.0',
        '_gid': 'GA1.2.1052437806.1656496980',
        'dmdbase_cdc': 'DBSET',
        'gig_canary_ver': '13196-3-27609435',
        'gig_bootstrap_3_VyMqpWHqLLXp5GxP4hr_mjtF4DLrrHwkesmXXYENo_JX5GzK3mAxG5FLocdrfv-n': 'login_ver4',
        '_gat': '1',
        'mbox': 'PC#7c087511341343a287189306a920564c.31_0#1719051811|session#96f4cf4bc6214eeb91c4e90305b1d6c5#1656568151',
    }

    headers = {
        'authority': 'home.kpmg',
        'accept': '*/*',
        'accept-language': 'en-US,en;q=0.9,hi;q=0.8',
        'origin': 'https://home.kpmg',
        'referer': 'https://home.kpmg/kw/en/home/insights.html',
        'sec-ch-ua': '".Not/A)Brand";v="99", "Google Chrome";v="103", "Chromium";v="103"',
        'sec-ch-ua-mobile': '?0',
        'sec-ch-ua-platform': '"macOS"',
        'sec-fetch-dest': 'empty',
        'sec-fetch-mode': 'cors',
        'sec-fetch-site': 'same-origin',
        'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36',
    }

    json_data = {
        'query': '',
        'filters': {
            'all': [
                {
                    'kpmg_tab_type': [
                        'Insights',
                    ],
                },
            ],
        },
        'result_fields': {
            'kpmg_image': {
                'raw': {},
            },
            'kpmg_description': {
                'raw': {},
            },
            'kpmg_banner_flag': {
                'raw': {},
            },
            'kpmg_primary_tag': {
                'raw': {},
            },
            'kpmg_article_date': {
                'raw': {},
            },
            'kpmg_contact_job_ttl': {
                'raw': {},
            },
            'kpmg_article_readtime': {
                'raw': {},
            },
            'kpmg_title': {
                'raw': {},
            },
            'kpmg_contact_fn': {
                'raw': {},
            },
            'kpmg_contact_ln': {
                'raw': {},
            },
            'kpmg_event_type': {
                'raw': {},
            },
            'kpmg_contact_city': {
                'raw': {},
            },
            'kpmg_event_start_time': {
                'raw': {},
            },
            'kpmg_article_date_time': {
                'raw': {},
            },
            'kpmg_contact_country': {
                'raw': {},
            },
            'kpmg_tab_type': {
                'raw': {},
            },
            'kpmg_short_desc': {
                'raw': {},
            },
            'kpmg_article_primary_format': {
                'raw': {},
            },
            'kpmg_article_type': {
                'raw': {},
            },
            'kpmg_event_startdate': {
                'raw': {},
            },
            'kpmg_url': {
                'raw': {},
            },
            'kpmg_template_type': {
                'raw': {},
            },
        },
        'page': {
            'size': 40,
            'current': 1,
        },
        'sort': {
            'kpmg_filter_date': 'desc',
        },
    }

    def parse(self, response):
        responses = rq.post('https://home.kpmg/esearch/kw-en', cookies=self.cookies, headers=self.headers,
                            json=self.json_data)
        result = responses.json()['results']
        for i in result:
            yield {"clean_url": self.allowed_domains[0],
                   "base_url": self.start_urls[0],
                   'link': i['kpmg_url']['raw'],
                   "type": 'article'}


class KpmgOmanSpider(scrapy.Spider):
    name = "KpmgOmanSpider"
    allowed_domains = ['home.kpmg']
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    cookies = {
        'gig_canary': 'false',
        'visit_settings': '%7B%22count%22%3A3%2C%22mins%22%3A30%2C%22days%22%3A14%7D',
        'gig_bootstrap_3_dAO2wSinyCBsporhy0kFd0EaIUU-fISB9pq_GaL34bTZzaKzpkZFdHtGcrL0AqKn': 'login_ver4',
        'at_check': 'true',
        'AMCVS_00B621ED542E84FD0A4C98A1%40AdobeOrg': '1',
        '_ga': 'GA1.2.340825921.1655807009',
        '__hstc': '214917896.8cfc5ea1aae695d6eafcaee602ca9a0f.1655882982959.1655882982959.1655882982959.1',
        'hubspotutk': '8cfc5ea1aae695d6eafcaee602ca9a0f',
        '__hssrc': '1',
        's_ips': '821',
        's_cc': 'true',
        'personalization': '%7B%22industry%22%3A%22Education%22%2C%22subindustry%22%3A%22University%22%2C%22zthesIDs%22%3A%22%22%7D',
        'visit_count': '1655882994614-2-1656998052849',
        's_evar38_per': 'Market%20Entry%20Germany',
        '_gid': 'GA1.2.1052437806.1656496980',
        'dmdbase_cdc': 'DBSET',
        'gig_canary_ver': '13196-3-27609435',
        'gig_bootstrap_3_VyMqpWHqLLXp5GxP4hr_mjtF4DLrrHwkesmXXYENo_JX5GzK3mAxG5FLocdrfv-n': 'login_ver4',
        'sat_track': 'true',
        'mbox': 'PC#7c087511341343a287189306a920564c.31_0#1719051811|session#96f4cf4bc6214eeb91c4e90305b1d6c5#1656568351',
        'AMCV_00B621ED542E84FD0A4C98A1%40AdobeOrg': '-408604571%7CMCIDTS%7C19173%7CMCMID%7C47947784634070791523896494648365507125%7CMCAAMLH-1657171291%7C12%7CMCAAMB-1657171291%7CRKhpRz8krg2tLO6pguXWp5olkAcUniQYPHaMWWgdJ3xzPWQmdj0y%7CMCOPTOUT-1656573691s%7CNONE%7CvVersion%7C4.6.0',
        's_tp': '4396',
        's_ppv': 'om%253Aen%253Ahome%253Ainsights%2C19%2C19%2C821%2C1%2C5',
        's_nr30': '1656566491451-Repeat',
        's_pmf': 'om',
        's_vnum': '1656613800038%26vn%3D2',
        's_invisit': 'true',
        '_gat': '1',
    }

    headers = {
        'authority': 'home.kpmg',
        'accept': '*/*',
        'accept-language': 'en-US,en;q=0.9,hi;q=0.8',
        'origin': 'https://home.kpmg',
        'referer': 'https://home.kpmg/om/en/home/insights.html',
        'sec-ch-ua': '".Not/A)Brand";v="99", "Google Chrome";v="103", "Chromium";v="103"',
        'sec-ch-ua-mobile': '?0',
        'sec-ch-ua-platform': '"macOS"',
        'sec-fetch-dest': 'empty',
        'sec-fetch-mode': 'cors',
        'sec-fetch-site': 'same-origin',
        'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36',
    }

    json_data = {
        'query': '',
        'filters': {
            'all': [
                {
                    'kpmg_tab_type': [
                        'Insights',
                    ],
                },
            ],
        },
        'result_fields': {
            'kpmg_image': {
                'raw': {},
            },
            'kpmg_description': {
                'raw': {},
            },
            'kpmg_banner_flag': {
                'raw': {},
            },
            'kpmg_primary_tag': {
                'raw': {},
            },
            'kpmg_article_date': {
                'raw': {},
            },
            'kpmg_contact_job_ttl': {
                'raw': {},
            },
            'kpmg_article_readtime': {
                'raw': {},
            },
            'kpmg_title': {
                'raw': {},
            },
            'kpmg_contact_fn': {
                'raw': {},
            },
            'kpmg_contact_ln': {
                'raw': {},
            },
            'kpmg_event_type': {
                'raw': {},
            },
            'kpmg_contact_city': {
                'raw': {},
            },
            'kpmg_event_start_time': {
                'raw': {},
            },
            'kpmg_article_date_time': {
                'raw': {},
            },
            'kpmg_contact_country': {
                'raw': {},
            },
            'kpmg_tab_type': {
                'raw': {},
            },
            'kpmg_short_desc': {
                'raw': {},
            },
            'kpmg_article_primary_format': {
                'raw': {},
            },
            'kpmg_article_type': {
                'raw': {},
            },
            'kpmg_event_startdate': {
                'raw': {},
            },
            'kpmg_url': {
                'raw': {},
            },
            'kpmg_template_type': {
                'raw': {},
            },
        },
        'page': {
            'size': 40,
            'current': 1,
        },
        'sort': {
            'kpmg_filter_date': 'desc',
        },
    }

    def parse(self, response):
        responses = rq.post('https://home.kpmg/esearch/om-en', cookies=self.cookies, headers=self.headers,
                            json=self.json_data)
        result = responses.json()['results']
        for i in result:
            yield {"clean_url": self.allowed_domains[0],
                   "base_url": self.start_urls[0],
                   'link': i['kpmg_url']['raw'],
                   "type": 'article'}


class KpmgPakistanSpider(scrapy.Spider):
    name = "KpmgPakistanSpider"
    allowed_domains = ['home.kpmg']
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    cookies = {
        'gig_canary': 'false',
        'visit_settings': '%7B%22count%22%3A3%2C%22mins%22%3A30%2C%22days%22%3A14%7D',
        'gig_bootstrap_3_dAO2wSinyCBsporhy0kFd0EaIUU-fISB9pq_GaL34bTZzaKzpkZFdHtGcrL0AqKn': 'login_ver4',
        'at_check': 'true',
        'AMCVS_00B621ED542E84FD0A4C98A1%40AdobeOrg': '1',
        '_ga': 'GA1.2.340825921.1655807009',
        '__hstc': '214917896.8cfc5ea1aae695d6eafcaee602ca9a0f.1655882982959.1655882982959.1655882982959.1',
        'hubspotutk': '8cfc5ea1aae695d6eafcaee602ca9a0f',
        '__hssrc': '1',
        's_ips': '821',
        's_cc': 'true',
        'personalization': '%7B%22industry%22%3A%22Education%22%2C%22subindustry%22%3A%22University%22%2C%22zthesIDs%22%3A%22%22%7D',
        'visit_count': '1655882994614-2-1656998052849',
        '_gid': 'GA1.2.1052437806.1656496980',
        'gig_canary_ver': '13196-3-27609435',
        'gig_bootstrap_3_VyMqpWHqLLXp5GxP4hr_mjtF4DLrrHwkesmXXYENo_JX5GzK3mAxG5FLocdrfv-n': 'login_ver4',
        'AMCV_00B621ED542E84FD0A4C98A1%40AdobeOrg': '-408604571%7CMCIDTS%7C19173%7CMCMID%7C47947784634070791523896494648365507125%7CMCAAMLH-1657171291%7C12%7CMCAAMB-1657171291%7CRKhpRz8krg2tLO6pguXWp5olkAcUniQYPHaMWWgdJ3xzPWQmdj0y%7CMCOPTOUT-1656573691s%7CNONE%7CvVersion%7C4.6.0',
        's_pmf': 'om',
        's_vnum': '1656613800038%26vn%3D2',
        's_invisit': 'true',
        's_evar38_per': 'Redefining%20resilience%20',
        's_tp': '4213',
        's_ppv': 'om%253Aen%253Ahome%253Ainsights%2C19%2C19%2C821%2C1%2C5',
        's_nr30': '1656567159370-Repeat',
        'sat_track': 'false',
        '_gat': '1',
        'dmdbase_cdc': 'DBSET',
        'mbox': 'PC#7c087511341343a287189306a920564c.31_0#1719051811|session#96f4cf4bc6214eeb91c4e90305b1d6c5#1656569251',
    }

    headers = {
        'authority': 'home.kpmg',
        'accept': '*/*',
        'accept-language': 'en-US,en;q=0.9,hi;q=0.8',
        'origin': 'https://home.kpmg',
        'referer': 'https://home.kpmg/pk/en/home/insights.html',
        'sec-ch-ua': '".Not/A)Brand";v="99", "Google Chrome";v="103", "Chromium";v="103"',
        'sec-ch-ua-mobile': '?0',
        'sec-ch-ua-platform': '"macOS"',
        'sec-fetch-dest': 'empty',
        'sec-fetch-mode': 'cors',
        'sec-fetch-site': 'same-origin',
        'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36',
    }

    json_data = {
        'query': '',
        'filters': {
            'all': [
                {
                    'kpmg_tab_type': [
                        'Insights',
                    ],
                },
                {
                    'kpmg_template_type': [
                        'editable-flex-template',
                    ],
                },
            ],
        },
        'result_fields': {
            'kpmg_image': {
                'raw': {},
            },
            'kpmg_description': {
                'raw': {},
            },
            'kpmg_banner_flag': {
                'raw': {},
            },
            'kpmg_primary_tag': {
                'raw': {},
            },
            'kpmg_article_date': {
                'raw': {},
            },
            'kpmg_contact_job_ttl': {
                'raw': {},
            },
            'kpmg_article_readtime': {
                'raw': {},
            },
            'kpmg_title': {
                'raw': {},
            },
            'kpmg_contact_fn': {
                'raw': {},
            },
            'kpmg_contact_ln': {
                'raw': {},
            },
            'kpmg_event_type': {
                'raw': {},
            },
            'kpmg_contact_city': {
                'raw': {},
            },
            'kpmg_event_start_time': {
                'raw': {},
            },
            'kpmg_article_date_time': {
                'raw': {},
            },
            'kpmg_contact_country': {
                'raw': {},
            },
            'kpmg_tab_type': {
                'raw': {},
            },
            'kpmg_short_desc': {
                'raw': {},
            },
            'kpmg_article_primary_format': {
                'raw': {},
            },
            'kpmg_article_type': {
                'raw': {},
            },
            'kpmg_event_startdate': {
                'raw': {},
            },
            'kpmg_url': {
                'raw': {},
            },
            'kpmg_template_type': {
                'raw': {},
            },
        },
        'page': {
            'size': 40,
            'current': 1,
        },
        'sort': {
            'kpmg_filter_date': 'desc',
        },
    }

    def parse(self, response):
        responses = rq.post('https://home.kpmg/esearch/pk-en', cookies=self.cookies, headers=self.headers,
                            json=self.json_data)
        result = responses.json()['results']
        for i in result:
            yield {"clean_url": self.allowed_domains[0],
                   "base_url": self.start_urls[0],
                   'link': i['kpmg_url']['raw'],
                   "type": 'article'}


class KpmgQatarSpider(scrapy.Spider):
    name = "KpmgQatarSpider"
    allowed_domains = ['home.kpmg']
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    cookies = {
        'gig_canary': 'false',
        'visit_settings': '%7B%22count%22%3A3%2C%22mins%22%3A30%2C%22days%22%3A14%7D',
        'gig_bootstrap_3_dAO2wSinyCBsporhy0kFd0EaIUU-fISB9pq_GaL34bTZzaKzpkZFdHtGcrL0AqKn': 'login_ver4',
        'at_check': 'true',
        'AMCVS_00B621ED542E84FD0A4C98A1%40AdobeOrg': '1',
        '_ga': 'GA1.2.340825921.1655807009',
        '__hstc': '214917896.8cfc5ea1aae695d6eafcaee602ca9a0f.1655882982959.1655882982959.1655882982959.1',
        'hubspotutk': '8cfc5ea1aae695d6eafcaee602ca9a0f',
        '__hssrc': '1',
        's_ips': '821',
        's_cc': 'true',
        'personalization': '%7B%22industry%22%3A%22Education%22%2C%22subindustry%22%3A%22University%22%2C%22zthesIDs%22%3A%22%22%7D',
        'visit_count': '1655882994614-2-1656998052849',
        '_gid': 'GA1.2.1052437806.1656496980',
        'gig_canary_ver': '13196-3-27609435',
        'gig_bootstrap_3_VyMqpWHqLLXp5GxP4hr_mjtF4DLrrHwkesmXXYENo_JX5GzK3mAxG5FLocdrfv-n': 'login_ver4',
        'AMCV_00B621ED542E84FD0A4C98A1%40AdobeOrg': '-408604571%7CMCIDTS%7C19173%7CMCMID%7C47947784634070791523896494648365507125%7CMCAAMLH-1657171291%7C12%7CMCAAMB-1657171291%7CRKhpRz8krg2tLO6pguXWp5olkAcUniQYPHaMWWgdJ3xzPWQmdj0y%7CMCOPTOUT-1656573691s%7CNONE%7CvVersion%7C4.6.0',
        's_pmf': 'om',
        's_vnum': '1656613800038%26vn%3D2',
        's_invisit': 'true',
        's_evar38_per': 'Redefining%20resilience%20',
        's_tp': '4213',
        's_ppv': 'om%253Aen%253Ahome%253Ainsights%2C19%2C19%2C821%2C1%2C5',
        's_nr30': '1656567159370-Repeat',
        'sat_track': 'false',
        'dmdbase_cdc': 'DBSET',
        '_gat': '1',
        'mbox': 'PC#7c087511341343a287189306a920564c.31_0#1719051811|session#96f4cf4bc6214eeb91c4e90305b1d6c5#1656569647',
    }

    headers = {
        'authority': 'home.kpmg',
        'accept': '*/*',
        'accept-language': 'en-US,en;q=0.9,hi;q=0.8',
        'origin': 'https://home.kpmg',
        'referer': 'https://home.kpmg/qa/en/home/insights.html',
        'sec-ch-ua': '".Not/A)Brand";v="99", "Google Chrome";v="103", "Chromium";v="103"',
        'sec-ch-ua-mobile': '?0',
        'sec-ch-ua-platform': '"macOS"',
        'sec-fetch-dest': 'empty',
        'sec-fetch-mode': 'cors',
        'sec-fetch-site': 'same-origin',
        'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36',
    }

    json_data = {
        'query': '',
        'filters': {},
        'result_fields': {
            'kpmg_image': {
                'raw': {},
            },
            'kpmg_description': {
                'raw': {},
            },
            'kpmg_banner_flag': {
                'raw': {},
            },
            'kpmg_primary_tag': {
                'raw': {},
            },
            'kpmg_article_date': {
                'raw': {},
            },
            'kpmg_contact_job_ttl': {
                'raw': {},
            },
            'kpmg_article_readtime': {
                'raw': {},
            },
            'kpmg_title': {
                'raw': {},
            },
            'kpmg_contact_fn': {
                'raw': {},
            },
            'kpmg_contact_ln': {
                'raw': {},
            },
            'kpmg_event_type': {
                'raw': {},
            },
            'kpmg_contact_city': {
                'raw': {},
            },
            'kpmg_event_start_time': {
                'raw': {},
            },
            'kpmg_article_date_time': {
                'raw': {},
            },
            'kpmg_contact_country': {
                'raw': {},
            },
            'kpmg_tab_type': {
                'raw': {},
            },
            'kpmg_short_desc': {
                'raw': {},
            },
            'kpmg_article_primary_format': {
                'raw': {},
            },
            'kpmg_article_type': {
                'raw': {},
            },
            'kpmg_event_startdate': {
                'raw': {},
            },
            'kpmg_url': {
                'raw': {},
            },
            'kpmg_template_type': {
                'raw': {},
            },
        },
        'page': {
            'size': 40,
            'current': 1,
        },
        'sort': {
            'kpmg_filter_date': 'desc',
        },
    }

    def parse(self, response):
        responses = rq.post('https://home.kpmg/esearch/qa-en', cookies=self.cookies, headers=self.headers,
                            json=self.json_data)
        result = responses.json()['results']
        for i in result:
            yield {"clean_url": self.allowed_domains[0],
                   "base_url": self.start_urls[0],
                   'link': i['kpmg_url']['raw'],
                   "type": 'article'}


class KpmgJordanSpider(scrapy.Spider):
    name = "KpmgJordanSpider"
    allowed_domains = ['home.kpmg']
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    cookies = {
        'gig_canary': 'false',
        'visit_settings': '%7B%22count%22%3A3%2C%22mins%22%3A30%2C%22days%22%3A14%7D',
        'gig_bootstrap_3_dAO2wSinyCBsporhy0kFd0EaIUU-fISB9pq_GaL34bTZzaKzpkZFdHtGcrL0AqKn': 'login_ver4',
        'at_check': 'true',
        'AMCVS_00B621ED542E84FD0A4C98A1%40AdobeOrg': '1',
        '_ga': 'GA1.2.340825921.1655807009',
        '__hstc': '214917896.8cfc5ea1aae695d6eafcaee602ca9a0f.1655882982959.1655882982959.1655882982959.1',
        'hubspotutk': '8cfc5ea1aae695d6eafcaee602ca9a0f',
        '__hssrc': '1',
        's_ips': '821',
        's_cc': 'true',
        'personalization': '%7B%22industry%22%3A%22Education%22%2C%22subindustry%22%3A%22University%22%2C%22zthesIDs%22%3A%22%22%7D',
        'visit_count': '1655882994614-2-1656998052849',
        '_gid': 'GA1.2.1052437806.1656496980',
        'gig_canary_ver': '13196-3-27609435',
        'gig_bootstrap_3_VyMqpWHqLLXp5GxP4hr_mjtF4DLrrHwkesmXXYENo_JX5GzK3mAxG5FLocdrfv-n': 'login_ver4',
        'AMCV_00B621ED542E84FD0A4C98A1%40AdobeOrg': '-408604571%7CMCIDTS%7C19173%7CMCMID%7C47947784634070791523896494648365507125%7CMCAAMLH-1657171291%7C12%7CMCAAMB-1657171291%7CRKhpRz8krg2tLO6pguXWp5olkAcUniQYPHaMWWgdJ3xzPWQmdj0y%7CMCOPTOUT-1656573691s%7CNONE%7CvVersion%7C4.6.0',
        's_vnum': '1656613800038%26vn%3D2',
        's_invisit': 'true',
        's_evar38_per': 'Redefining%20resilience%20',
        'dmdbase_cdc': 'DBSET',
        'sat_track': 'true',
        '_gat': '1',
        'mbox': 'PC#7c087511341343a287189306a920564c.31_0#1719051811|session#96f4cf4bc6214eeb91c4e90305b1d6c5#1656570105',
        's_tp': '4305',
        's_ppv': 'jo%253Aen%253Ahome%253Ainsights%2C19%2C19%2C821%2C1%2C5',
        's_nr30': '1656568244581-Repeat',
        's_pmf': 'jo',
    }

    headers = {
        'authority': 'home.kpmg',
        'accept': '*/*',
        'accept-language': 'en-US,en;q=0.9,hi;q=0.8',
        'origin': 'https://home.kpmg',
        'referer': 'https://home.kpmg/jo/en/home/insights.html',
        'sec-ch-ua': '".Not/A)Brand";v="99", "Google Chrome";v="103", "Chromium";v="103"',
        'sec-ch-ua-mobile': '?0',
        'sec-ch-ua-platform': '"macOS"',
        'sec-fetch-dest': 'empty',
        'sec-fetch-mode': 'cors',
        'sec-fetch-site': 'same-origin',
        'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36',
    }

    json_data = {
        'query': '',
        'filters': {
            'all': [
                {
                    'kpmg_tab_type': [
                        'Insights',
                    ],
                },
            ],
        },
        'result_fields': {
            'kpmg_image': {
                'raw': {},
            },
            'kpmg_description': {
                'raw': {},
            },
            'kpmg_banner_flag': {
                'raw': {},
            },
            'kpmg_primary_tag': {
                'raw': {},
            },
            'kpmg_article_date': {
                'raw': {},
            },
            'kpmg_contact_job_ttl': {
                'raw': {},
            },
            'kpmg_article_readtime': {
                'raw': {},
            },
            'kpmg_title': {
                'raw': {},
            },
            'kpmg_contact_fn': {
                'raw': {},
            },
            'kpmg_contact_ln': {
                'raw': {},
            },
            'kpmg_event_type': {
                'raw': {},
            },
            'kpmg_contact_city': {
                'raw': {},
            },
            'kpmg_event_start_time': {
                'raw': {},
            },
            'kpmg_article_date_time': {
                'raw': {},
            },
            'kpmg_contact_country': {
                'raw': {},
            },
            'kpmg_tab_type': {
                'raw': {},
            },
            'kpmg_short_desc': {
                'raw': {},
            },
            'kpmg_article_primary_format': {
                'raw': {},
            },
            'kpmg_article_type': {
                'raw': {},
            },
            'kpmg_event_startdate': {
                'raw': {},
            },
            'kpmg_url': {
                'raw': {},
            },
            'kpmg_template_type': {
                'raw': {},
            },
        },
        'page': {
            'size': 40,
            'current': 1,
        },
        'sort': {
            'kpmg_filter_date': 'desc',
        },
    }

    def parse(self, response):
        responses = rq.post('https://home.kpmg/esearch/jo-en', cookies=self.cookies, headers=self.headers,
                            json=self.json_data)
        result = responses.json()['results']
        for i in result:
            yield {"clean_url": self.allowed_domains[0],
                   "base_url": self.start_urls[0],
                   'link': i['kpmg_url']['raw'],
                   "type": 'article'}


class KpmgTurkeySpider(scrapy.Spider):
    name = "KpmgTurkeySpider"
    allowed_domains = ['home.kpmg']
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    cookies = {
        'gig_canary': 'false',
        'visit_settings': '%7B%22count%22%3A3%2C%22mins%22%3A30%2C%22days%22%3A14%7D',
        'gig_bootstrap_3_dAO2wSinyCBsporhy0kFd0EaIUU-fISB9pq_GaL34bTZzaKzpkZFdHtGcrL0AqKn': 'login_ver4',
        'at_check': 'true',
        'AMCVS_00B621ED542E84FD0A4C98A1%40AdobeOrg': '1',
        '_ga': 'GA1.2.340825921.1655807009',
        '__hstc': '214917896.8cfc5ea1aae695d6eafcaee602ca9a0f.1655882982959.1655882982959.1655882982959.1',
        'hubspotutk': '8cfc5ea1aae695d6eafcaee602ca9a0f',
        '__hssrc': '1',
        's_ips': '821',
        's_cc': 'true',
        'personalization': '%7B%22industry%22%3A%22Education%22%2C%22subindustry%22%3A%22University%22%2C%22zthesIDs%22%3A%22%22%7D',
        'visit_count': '1655882994614-2-1656998052849',
        '_gid': 'GA1.2.1052437806.1656496980',
        'gig_canary_ver': '13196-3-27609435',
        'gig_bootstrap_3_VyMqpWHqLLXp5GxP4hr_mjtF4DLrrHwkesmXXYENo_JX5GzK3mAxG5FLocdrfv-n': 'login_ver4',
        'AMCV_00B621ED542E84FD0A4C98A1%40AdobeOrg': '-408604571%7CMCIDTS%7C19173%7CMCMID%7C47947784634070791523896494648365507125%7CMCAAMLH-1657171291%7C12%7CMCAAMB-1657171291%7CRKhpRz8krg2tLO6pguXWp5olkAcUniQYPHaMWWgdJ3xzPWQmdj0y%7CMCOPTOUT-1656573691s%7CNONE%7CvVersion%7C4.6.0',
        'sat_track': 'true',
        's_tp': '3568',
        '_gat': '1',
        'dmdbase_cdc': 'DBSET',
        '_gat_UA-150357758-1': '1',
        'mbox': 'PC#7c087511341343a287189306a920564c.31_0#1719051811|session#64d834b8e82641e68beacec9680380a0#1656572671',
        's_evar38_per': 'undefined',
        's_ppv': 'tr%253Aen%253Ahome%253Ainsights%2C23%2C23%2C821%2C1%2C4',
        's_nr30': '1656570810886-Repeat',
        's_pmf': 'tr',
        's_vnum': '1656613800038%26vn%3D3',
        's_invisit': 'true',
    }

    headers = {
        'authority': 'home.kpmg',
        'accept': '*/*',
        'accept-language': 'en-US,en;q=0.9,hi;q=0.8',
        'origin': 'https://home.kpmg',
        'referer': 'https://home.kpmg/tr/en/home/insights.html',
        'sec-ch-ua': '".Not/A)Brand";v="99", "Google Chrome";v="103", "Chromium";v="103"',
        'sec-ch-ua-mobile': '?0',
        'sec-ch-ua-platform': '"macOS"',
        'sec-fetch-dest': 'empty',
        'sec-fetch-mode': 'cors',
        'sec-fetch-site': 'same-origin',
        'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36',
    }

    json_data = {
        'query': '',
        'filters': {
            'all': [
                {
                    'kpmg_tab_type': [
                        'Insights',
                    ],
                },
            ],
        },
        'result_fields': {
            'kpmg_image': {
                'raw': {},
            },
            'kpmg_description': {
                'raw': {},
            },
            'kpmg_banner_flag': {
                'raw': {},
            },
            'kpmg_primary_tag': {
                'raw': {},
            },
            'kpmg_article_date': {
                'raw': {},
            },
            'kpmg_contact_job_ttl': {
                'raw': {},
            },
            'kpmg_article_readtime': {
                'raw': {},
            },
            'kpmg_title': {
                'raw': {},
            },
            'kpmg_contact_fn': {
                'raw': {},
            },
            'kpmg_contact_ln': {
                'raw': {},
            },
            'kpmg_event_type': {
                'raw': {},
            },
            'kpmg_contact_city': {
                'raw': {},
            },
            'kpmg_event_start_time': {
                'raw': {},
            },
            'kpmg_article_date_time': {
                'raw': {},
            },
            'kpmg_contact_country': {
                'raw': {},
            },
            'kpmg_tab_type': {
                'raw': {},
            },
            'kpmg_short_desc': {
                'raw': {},
            },
            'kpmg_article_primary_format': {
                'raw': {},
            },
            'kpmg_article_type': {
                'raw': {},
            },
            'kpmg_event_startdate': {
                'raw': {},
            },
            'kpmg_url': {
                'raw': {},
            },
            'kpmg_template_type': {
                'raw': {},
            },
        },
        'page': {
            'size': 40,
            'current': 1,
        },
        'sort': {
            'kpmg_filter_date': 'desc',
        },
    }

    def parse(self, response):
        responses = rq.post('https://home.kpmg/esearch/tr-en', cookies=self.cookies, headers=self.headers,
                            json=self.json_data)
        result = responses.json()['results']
        for i in result:
            yield {"clean_url": self.allowed_domains[0],
                   "base_url": self.start_urls[0],
                   'link': i['kpmg_url']['raw'],
                   "type": 'article'}


class KpmgSaudiSpider(scrapy.Spider):
    name = "KpmgSaudiSpider"
    allowed_domains = ['home.kpmg']
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    cookies = {
        'gig_canary': 'false',
        'visit_settings': '%7B%22count%22%3A3%2C%22mins%22%3A30%2C%22days%22%3A14%7D',
        'gig_bootstrap_3_dAO2wSinyCBsporhy0kFd0EaIUU-fISB9pq_GaL34bTZzaKzpkZFdHtGcrL0AqKn': 'login_ver4',
        'at_check': 'true',
        'AMCVS_00B621ED542E84FD0A4C98A1%40AdobeOrg': '1',
        '_ga': 'GA1.2.340825921.1655807009',
        '__hstc': '214917896.8cfc5ea1aae695d6eafcaee602ca9a0f.1655882982959.1655882982959.1655882982959.1',
        'hubspotutk': '8cfc5ea1aae695d6eafcaee602ca9a0f',
        '__hssrc': '1',
        's_ips': '821',
        's_cc': 'true',
        'personalization': '%7B%22industry%22%3A%22Education%22%2C%22subindustry%22%3A%22University%22%2C%22zthesIDs%22%3A%22%22%7D',
        'visit_count': '1655882994614-2-1656998052849',
        '_gid': 'GA1.2.1052437806.1656496980',
        'gig_canary_ver': '13196-3-27609435',
        'gig_bootstrap_3_VyMqpWHqLLXp5GxP4hr_mjtF4DLrrHwkesmXXYENo_JX5GzK3mAxG5FLocdrfv-n': 'login_ver4',
        'AMCV_00B621ED542E84FD0A4C98A1%40AdobeOrg': '-408604571%7CMCIDTS%7C19173%7CMCMID%7C47947784634070791523896494648365507125%7CMCAAMLH-1657171291%7C12%7CMCAAMB-1657171291%7CRKhpRz8krg2tLO6pguXWp5olkAcUniQYPHaMWWgdJ3xzPWQmdj0y%7CMCOPTOUT-1656573691s%7CNONE%7CvVersion%7C4.6.0',
        'dmdbase_cdc': 'DBSET',
        's_pmf': 'tr',
        's_vnum': '1656613800038%26vn%3D3',
        's_invisit': 'true',
        's_evar38_per': 'Data%20%26%20Analytics',
        's_tp': '3410',
        's_ppv': 'tr%253Aen%253Ahome%253Ainsights%2C24%2C24%2C821%2C1%2C4',
        's_nr30': '1656570850015-Repeat',
        'sat_track': 'false',
        '_gat': '1',
        'mbox': 'PC#7c087511341343a287189306a920564c.31_0#1719051811|session#64d834b8e82641e68beacec9680380a0#1656572839',
    }

    headers = {
        'authority': 'home.kpmg',
        'accept': '*/*',
        'accept-language': 'en-US,en;q=0.9,hi;q=0.8',
        'origin': 'https://home.kpmg',
        'referer': 'https://home.kpmg/sa/en/home/insights.html',
        'sec-ch-ua': '".Not/A)Brand";v="99", "Google Chrome";v="103", "Chromium";v="103"',
        'sec-ch-ua-mobile': '?0',
        'sec-ch-ua-platform': '"macOS"',
        'sec-fetch-dest': 'empty',
        'sec-fetch-mode': 'cors',
        'sec-fetch-site': 'same-origin',
        'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36',
    }

    json_data = {
        'query': '',
        'filters': {
            'all': [
                {
                    'kpmg_tab_type': [
                        'Insights',
                    ],
                },
            ],
        },
        'result_fields': {
            'kpmg_image': {
                'raw': {},
            },
            'kpmg_description': {
                'raw': {},
            },
            'kpmg_banner_flag': {
                'raw': {},
            },
            'kpmg_primary_tag': {
                'raw': {},
            },
            'kpmg_article_date': {
                'raw': {},
            },
            'kpmg_contact_job_ttl': {
                'raw': {},
            },
            'kpmg_article_readtime': {
                'raw': {},
            },
            'kpmg_title': {
                'raw': {},
            },
            'kpmg_contact_fn': {
                'raw': {},
            },
            'kpmg_contact_ln': {
                'raw': {},
            },
            'kpmg_event_type': {
                'raw': {},
            },
            'kpmg_contact_city': {
                'raw': {},
            },
            'kpmg_event_start_time': {
                'raw': {},
            },
            'kpmg_article_date_time': {
                'raw': {},
            },
            'kpmg_contact_country': {
                'raw': {},
            },
            'kpmg_tab_type': {
                'raw': {},
            },
            'kpmg_short_desc': {
                'raw': {},
            },
            'kpmg_article_primary_format': {
                'raw': {},
            },
            'kpmg_article_type': {
                'raw': {},
            },
            'kpmg_event_startdate': {
                'raw': {},
            },
            'kpmg_url': {
                'raw': {},
            },
            'kpmg_template_type': {
                'raw': {},
            },
        },
        'page': {
            'size': 40,
            'current': 1,
        },
        'sort': {
            'kpmg_filter_date': 'desc',
        },
    }

    def parse(self, response):
        responses = rq.post('https://home.kpmg/esearch/sa-en', cookies=self.cookies, headers=self.headers,
                            json=self.json_data)
        result = responses.json()['results']
        for i in result:
            yield {"clean_url": self.allowed_domains[0],
                   "base_url": self.start_urls[0],
                   'link': i['kpmg_url']['raw'],
                   "type": 'article'}


class KpmgUaeSpider(scrapy.Spider):
    name = "KpmgUaeSpider"
    allowed_domains = ['home.kpmg']
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    cookies = {
        'gig_canary': 'false',
        'visit_settings': '%7B%22count%22%3A3%2C%22mins%22%3A30%2C%22days%22%3A14%7D',
        'gig_bootstrap_3_dAO2wSinyCBsporhy0kFd0EaIUU-fISB9pq_GaL34bTZzaKzpkZFdHtGcrL0AqKn': 'login_ver4',
        'at_check': 'true',
        'AMCVS_00B621ED542E84FD0A4C98A1%40AdobeOrg': '1',
        '_ga': 'GA1.2.340825921.1655807009',
        '__hstc': '214917896.8cfc5ea1aae695d6eafcaee602ca9a0f.1655882982959.1655882982959.1655882982959.1',
        'hubspotutk': '8cfc5ea1aae695d6eafcaee602ca9a0f',
        '__hssrc': '1',
        's_ips': '821',
        's_cc': 'true',
        'personalization': '%7B%22industry%22%3A%22Education%22%2C%22subindustry%22%3A%22University%22%2C%22zthesIDs%22%3A%22%22%7D',
        'visit_count': '1655882994614-2-1656998052849',
        '_gid': 'GA1.2.1052437806.1656496980',
        'gig_bootstrap_3_VyMqpWHqLLXp5GxP4hr_mjtF4DLrrHwkesmXXYENo_JX5GzK3mAxG5FLocdrfv-n': 'login_ver4',
        'AMCV_00B621ED542E84FD0A4C98A1%40AdobeOrg': '-408604571%7CMCIDTS%7C19173%7CMCMID%7C47947784634070791523896494648365507125%7CMCAAMLH-1657171291%7C12%7CMCAAMB-1657171291%7CRKhpRz8krg2tLO6pguXWp5olkAcUniQYPHaMWWgdJ3xzPWQmdj0y%7CMCOPTOUT-1656573691s%7CNONE%7CvVersion%7C4.6.0',
        'dmdbase_cdc': 'DBSET',
        's_vnum': '1656613800038%26vn%3D3',
        's_invisit': 'true',
        's_evar38_per': 'Data%20%26%20Analytics',
        'sat_track': 'true',
        '_gcl_au': '1.1.92387890.1656571263',
        'gig_canary_ver': '13196-3-27609510',
        'gig_bootstrap_3_eYey6Z79si-eeXEPJdZ-nHmhuCW-jna6Vvc90U_rCKgSJBvRulOycPAZkI--y8OB': 'login_ver4',
        'mbox': 'PC#7c087511341343a287189306a920564c.31_0#1719051811|session#64d834b8e82641e68beacec9680380a0#1656573124',
        's_tp': '4956',
        's_ppv': 'ae%253Aen%253Ahome%253Ainsights%2C17%2C17%2C821%2C1%2C6',
        's_nr30': '1656571264085-Repeat',
        's_pmf': 'ae',
        '_gat': '1',
    }

    headers = {
        'authority': 'home.kpmg',
        'accept': '*/*',
        'accept-language': 'en-US,en;q=0.9,hi;q=0.8',
        'origin': 'https://home.kpmg',
        'referer': 'https://home.kpmg/ae/en/home/insights.html',
        'sec-ch-ua': '".Not/A)Brand";v="99", "Google Chrome";v="103", "Chromium";v="103"',
        'sec-ch-ua-mobile': '?0',
        'sec-ch-ua-platform': '"macOS"',
        'sec-fetch-dest': 'empty',
        'sec-fetch-mode': 'cors',
        'sec-fetch-site': 'same-origin',
        'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36',
    }

    json_data = {
        'query': '',
        'filters': {
            'all': [
                {
                    'kpmg_tab_type': [
                        'Insights',
                    ],
                },
            ],
        },
        'result_fields': {
            'kpmg_image': {
                'raw': {},
            },
            'kpmg_description': {
                'raw': {},
            },
            'kpmg_banner_flag': {
                'raw': {},
            },
            'kpmg_primary_tag': {
                'raw': {},
            },
            'kpmg_article_date': {
                'raw': {},
            },
            'kpmg_contact_job_ttl': {
                'raw': {},
            },
            'kpmg_article_readtime': {
                'raw': {},
            },
            'kpmg_title': {
                'raw': {},
            },
            'kpmg_contact_fn': {
                'raw': {},
            },
            'kpmg_contact_ln': {
                'raw': {},
            },
            'kpmg_event_type': {
                'raw': {},
            },
            'kpmg_contact_city': {
                'raw': {},
            },
            'kpmg_event_start_time': {
                'raw': {},
            },
            'kpmg_article_date_time': {
                'raw': {},
            },
            'kpmg_contact_country': {
                'raw': {},
            },
            'kpmg_tab_type': {
                'raw': {},
            },
            'kpmg_short_desc': {
                'raw': {},
            },
            'kpmg_article_primary_format': {
                'raw': {},
            },
            'kpmg_article_type': {
                'raw': {},
            },
            'kpmg_event_startdate': {
                'raw': {},
            },
            'kpmg_url': {
                'raw': {},
            },
            'kpmg_template_type': {
                'raw': {},
            },
        },
        'page': {
            'size': 40,
            'current': 1,
        },
        'sort': {
            'kpmg_filter_date': 'desc',
        },
    }

    def parse(self, response):
        responses = rq.post('https://home.kpmg/esearch/ae-en', cookies=self.cookies, headers=self.headers,
                            json=self.json_data)
        result = responses.json()['results']
        for i in result:
            yield {"clean_url": self.allowed_domains[0],
                   "base_url": self.start_urls[0],
                   'link': i['kpmg_url']['raw'],
                   "type": 'article'}


class KpmgGermanySpider(scrapy.Spider):
    name = "KpmgGermanySpider"
    allowed_domains = ['home.kpmg']
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    cookies = {
        'gig_canary': 'false',
        'visit_settings': '%7B%22count%22%3A3%2C%22mins%22%3A30%2C%22days%22%3A14%7D',
        'gig_bootstrap_3_dAO2wSinyCBsporhy0kFd0EaIUU-fISB9pq_GaL34bTZzaKzpkZFdHtGcrL0AqKn': 'login_ver4',
        'at_check': 'true',
        'AMCVS_00B621ED542E84FD0A4C98A1%40AdobeOrg': '1',
        '_ga': 'GA1.2.340825921.1655807009',
        'hubspotutk': '8cfc5ea1aae695d6eafcaee602ca9a0f',
        '__hssrc': '1',
        's_ips': '821',
        's_cc': 'true',
        'personalization': '%7B%22industry%22%3A%22Education%22%2C%22subindustry%22%3A%22University%22%2C%22zthesIDs%22%3A%22%22%7D',
        '_gid': 'GA1.2.1052437806.1656496980',
        'gig_bootstrap_3_VyMqpWHqLLXp5GxP4hr_mjtF4DLrrHwkesmXXYENo_JX5GzK3mAxG5FLocdrfv-n': 'login_ver4',
        'AMCV_00B621ED542E84FD0A4C98A1%40AdobeOrg': '-408604571%7CMCIDTS%7C19173%7CMCMID%7C47947784634070791523896494648365507125%7CMCAAMLH-1657171291%7C12%7CMCAAMB-1657171291%7CRKhpRz8krg2tLO6pguXWp5olkAcUniQYPHaMWWgdJ3xzPWQmdj0y%7CMCOPTOUT-1656573691s%7CNONE%7CvVersion%7C4.6.0',
        'dmdbase_cdc': 'DBSET',
        's_vnum': '1656613800038%26vn%3D3',
        's_invisit': 'true',
        'sat_track': 'true',
        '_gcl_au': '1.1.92387890.1656571263',
        'gig_bootstrap_3_eYey6Z79si-eeXEPJdZ-nHmhuCW-jna6Vvc90U_rCKgSJBvRulOycPAZkI--y8OB': 'login_ver4',
        's_evar38_per': 'Principles%20for%20Digital%20Transformation%20in%20Cities',
        'promo_shown': 'true',
        'visit_count': '1656571535432-3-1656998052849',
        '_gat': '1',
        '_gat_gtag_UA_19663328_9': '1',
        'gig_canary_ver': '13196-3-27609510',
        '__hstc': '214917896.8cfc5ea1aae695d6eafcaee602ca9a0f.1655882982959.1655882982959.1656571537012.2',
        '__hssc': '214917896.1.1656571537012',
        'mbox': 'PC#7c087511341343a287189306a920564c.31_0#1719051811|session#64d834b8e82641e68beacec9680380a0#1656573398',
        's_tp': '4430',
        's_ppv': 'de%253Aen%253Ahome%253Ainsights%2C19%2C19%2C821%2C1%2C5',
        's_pmf': 'de',
        's_nr30': '1656571537149-Repeat',
    }

    headers = {
        'authority': 'home.kpmg',
        'accept': '*/*',
        'accept-language': 'en-US,en;q=0.9,hi;q=0.8',
        'origin': 'https://home.kpmg',
        'referer': 'https://home.kpmg/de/en/home/insights.html',
        'sec-ch-ua': '".Not/A)Brand";v="99", "Google Chrome";v="103", "Chromium";v="103"',
        'sec-ch-ua-mobile': '?0',
        'sec-ch-ua-platform': '"macOS"',
        'sec-fetch-dest': 'empty',
        'sec-fetch-mode': 'cors',
        'sec-fetch-site': 'same-origin',
        'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36',
    }

    json_data = {
        'query': '',
        'filters': {
            'all': [
                {
                    'kpmg_tab_type': [
                        'Insights',
                    ],
                },
            ],
        },
        'result_fields': {
            'kpmg_image': {
                'raw': {},
            },
            'kpmg_description': {
                'raw': {},
            },
            'kpmg_banner_flag': {
                'raw': {},
            },
            'kpmg_primary_tag': {
                'raw': {},
            },
            'kpmg_article_date': {
                'raw': {},
            },
            'kpmg_contact_job_ttl': {
                'raw': {},
            },
            'kpmg_article_readtime': {
                'raw': {},
            },
            'kpmg_title': {
                'raw': {},
            },
            'kpmg_contact_fn': {
                'raw': {},
            },
            'kpmg_contact_ln': {
                'raw': {},
            },
            'kpmg_event_type': {
                'raw': {},
            },
            'kpmg_contact_city': {
                'raw': {},
            },
            'kpmg_event_start_time': {
                'raw': {},
            },
            'kpmg_article_date_time': {
                'raw': {},
            },
            'kpmg_contact_country': {
                'raw': {},
            },
            'kpmg_tab_type': {
                'raw': {},
            },
            'kpmg_short_desc': {
                'raw': {},
            },
            'kpmg_article_primary_format': {
                'raw': {},
            },
            'kpmg_article_type': {
                'raw': {},
            },
            'kpmg_event_startdate': {
                'raw': {},
            },
            'kpmg_url': {
                'raw': {},
            },
            'kpmg_template_type': {
                'raw': {},
            },
        },
        'page': {
            'size': 40,
            'current': 1,
        },
        'sort': {
            'kpmg_filter_date': 'desc',
        },
    }

    def parse(self, response):
        responses = rq.post('https://home.kpmg/esearch/de-en', cookies=self.cookies, headers=self.headers,
                            json=self.json_data)
        result = responses.json()['results']
        for i in result:
            yield {"clean_url": self.allowed_domains[0],
                   "base_url": self.start_urls[0],
                   'link': i['kpmg_url']['raw'],
                   "type": 'article'}


class TamimiLawSpider(scrapy.Spider):
    name = "TamimiLawSpider"
    allowed_domains = ["tamimi.com"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                categories_links = []
                res = response.css('.lawyer-thumb-img')
                for r in res:
                    try:
                        if "https" in str(r.css("a").attrib["href"]) or "http://" in str(r.css("a").attrib["href"]):
                            categories_links.append(r.css("a").attrib["href"])
                        else:
                            categories_links.append(
                                "https://www.tamimi.com"
                                + r.css("a").attrib["href"]
                            )

                    except:
                        pass

                for i in set(categories_links):
                    try:
                        yield scrapy.Request(
                            i,
                            callback=self.article_links,
                            meta={"dont_retry": True, "download_timeout": cat_timeout,"base_url": response.url},
                            errback=handle_error,
                        )


                    except:
                        pass

    def article_links(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                try:
                    base_url = response.meta.get('base_url')
                    data = response.css('.h-thumb-content')
                except:
                    data = []
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": base_url,
                                       "title": link.css('a::text').get(),
                                       "excerpt": link.css('.para.content::text').get(),
                                       "published_date": 'na',
                                       "link": link.css('a').attrib['href'] if link.css('a').attrib[
                                           'href'] else "False",
                                       }
                            else:
                                if link.css('a').attrib['href']:
                                    yield {"clean_url": self.allowed_domains[0],
                                           "base_url": base_url,
                                           "title": link.css('a::text').get(),
                                           "excerpt": link.css('.para.content::text').get(),
                                           "published_date": 'na',
                                           "link": "https://www.tamimi.com" + link.css('a').attrib['href'] if
                                           link.css('a').attrib['href'] else "False",
                                           }

                    except:
                        pass
        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1


class DmccSpider(scrapy.Spider):
    name = "DmccSpider"
    allowed_domains = ["dmcc.ae"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

        logging.info(
            {
                "proxy": "1",
                "clean_url": self.allowed_domains[0],
                "link": self.start_urls,
            }
        )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                try:
                    data = response.css('.content-list__link')
                except:
                    data = []
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css('a').attrib['href']):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('a').attrib['title'].strip(),
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": link.css('a').attrib['href'],
                                       "topic": "DMCC Compliance and Regulations",
                                       "type": "PDF"}
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('a').attrib['title'].strip(),
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": 'https://www.dmcc.ae' + link.css('a').attrib['href'],
                                       "topic": "DMCC Compliance and Regulations",
                                       "type": "PDF"}
                    except:
                        pass
        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1


class CentralBankOtherNoticeSpider(scrapy.Spider):
    name = "CentralBankOtherNoticeSpider"
    allowed_domains = ["centralbank.ae"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                try:
                    data = response.css('.card-body.text-left')
                except:
                    data = []
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css('a').attrib['href']):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('h2::text').get().strip(),
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": link.css('a').attrib['href'],
                                       "topic": "Central Bank of the UAE - Other Notices",
                                       "type": "PDF"}
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('h2::text').get().strip(),
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": 'https://www.centralbank.ae' + link.css('a').attrib['href'],
                                       "topic": "Central Bank of the UAE - Other Notices",
                                       "type": "PDF"}
                    except:
                        pass
        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1


class CbePaymentArticleSpider(scrapy.Spider):
    name = "CbePaymentArticleSpider"
    allowed_domains = ["cbe.org.eg"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                try:
                    data = response.css('li.clearfix')
                except:
                    data = []
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css('a').attrib['href']):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('span::text').get().strip(),
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "link": link.css('a').attrib['href'].replace(' ', '%20'),
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('span::text').get().strip(),
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "link": 'https://www.cbe.org.eg' + link.css('a').attrib['href'].replace(' ',
                                                                                                                  '%20'),
                                       }
                    except:
                        pass
        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1


class CbePaymentRegSpider(scrapy.Spider):
    name = "CbePaymentRegSpider"
    allowed_domains = ["cbe.org.eg"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0



    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                try:
                    data = response.css('li.clearfix')
                except:
                    data = []
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css('a').attrib['href']):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('span::text').get().strip(),
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "link": link.css('a').attrib['href'].replace(' ', '%20'),
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('span::text').get().strip(),
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "link": 'https://www.cbe.org.eg' + link.css('a').attrib['href'].replace(' ',
                                                                                                                  '%20'),
                                       }
                    except:
                        pass
        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1



class McitRulesAndRegulationsSpider(scrapy.Spider):
    name = "McitRulesAndRegulationsSpider"
    allowed_domains = ["mcit.gov.eg"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                try:
                    data = response.css('#Intro a')
                except:
                    data = []
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css('a').attrib['href']):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('a::text').get().strip(),
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": link.css('a').attrib['href'],
                                       "topic": "MCIT Regulations",
                                       "type": "PDF"}
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('span::text').get().strip(),
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": 'https://mcit.gov.eg' + link.css('a').attrib['href'],
                                       "topic": "MCIT Regulations",
                                       "type": "PDF"}
                    except:
                        pass
        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1


class DlpDubaiLegislationSpider(scrapy.Spider):
    name = "DlpDubaiLegislationSpider"
    allowed_domains = ["dlp.dubai.gov.ae"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                self.start_urls,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

        logging.info(
            {
                "proxy": "1",
                "clean_url": self.allowed_domains[0],
                "link": self.start_urls,
            }
        )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                try:
                    data = response.css('ul li')
                except:
                    data = []
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css('a').attrib['href']):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('div.title::text').get().strip(),
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": link.css('.col-6.viewPdf').css('a').attrib['href'].replace(' ',
                                                                                                             '%20'),
                                       "topic": "DLP Dubai - Legislation Search",
                                       "type": "PDF"}
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('div.title::text').get().strip(),
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": 'https://dlp.dubai.gov.ae/' +
                                                  link.css('.col-6.viewPdf').css('a').attrib['href'].replace(' ',
                                                                                                             '%20'),
                                       "topic": "DLP Dubai - Legislation Search",
                                       "type": "PDF"}
                    except:
                        pass
        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_article_links += 1


class OxfordBusinessGroupSpider(scrapy.Spider):
    name = "OxfordBusinessGroupSpider"
    allowed_domains = ["oxfordbusinessgroup.com"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip
        logging.info(
            {
                "proxy": "1",
                "clean_url": self.allowed_domains[0],
                "link": self.start_urls,
            }
        )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.white')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "https://" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"],
                                       }

                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://oxfordbusinessgroup.com" + link.css("a").attrib["href"],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class TraLawSpider(scrapy.Spider):
    name = "TraLawSpider"
    allowed_domains = ["tra.gov.eg"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                try:
                    data = response.css('.reports-table-2').css('a')
                except:
                    data = []
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css('a').attrib['href']):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('div::text').get().strip() if link.css('div::text') else 'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": link.css('a').attrib['href'].replace(' ', '%20'),
                                       "type": "PDF"}
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('div::text').get().strip() if link.css('div::text') else 'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": 'https://www.tra.gov.eg' + link.css('a').attrib['href'].replace(' ',
                                                                                                                  '%20'),
                                       "type": "PDF"}
                    except:
                        pass
        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1


class TamimiPublicationsSpider(scrapy.Spider):
    name = "TamimiPublicationsSpider"
    allowed_domains = ["tamimi.com"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                try:
                    data = response.css('.lawyer-thumb-content')
                except:
                    data = []
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "https://" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('h4::text').get().strip(),
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "link": link.css('a').attrib['href'],
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('h4::text').get().strip(),
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "link": "https://www.tamimi.com" + link.css('a').attrib['href'],
                                       }
                    except:
                        pass
        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1


class SgbvTitresSpider(scrapy.Spider):
    name = "SgbvTitresSpider"
    allowed_domains = ["sgbv.dz"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                try:
                    data = response.css('p')
                except:
                    data = []
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "https://" in str(link.css("a").attrib["href"]) and link.css("a"):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('a::text').get().strip() if link.css(
                                           'a::text').get() else "na",
                                       "excerpt": link.css('p::text').get().strip() if link.css('p::text') else "na",
                                       "published_date": 'na',
                                       "pdf_url": link.css('a').attrib['href'].replace(' ', '%20').replace('.',
                                                                                                           ''),
                                       "type": "PDF"}
                            elif link.css("a"):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('a::text').get().strip() if link.css(
                                           'a::text').get() else "na",
                                       "excerpt": link.css('p::text').get().strip() if link.css('p::text') else 'na',
                                       "published_date": 'na',
                                       "pdf_url": "https://www.sgbv.dz" + link.css('a').attrib['href'].replace(' ',
                                                                                                               '%20').replace(
                                           '.', ''),
                                       "type": "PDF"}
                            else:
                                pass
                    except:
                        pass
        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1


class MtcitTelecosSpider(scrapy.Spider):
    name = "MtcitTelecosSpider"
    allowed_domains = ["mtcit.gov.om"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                try:
                    data = response.css('.text-section')
                except:
                    data = []
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('div')[1].css('::text').get().strip(),
                                       "excerpt": "\r\n".join(link.css('div::text').getall()).strip(),
                                       "published_date": 'na',
                                       "pdf_url": link.css('a').attrib['href']
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('div')[1].css('::text').get().strip(),
                                       "excerpt": "\r\n".join(link.css('div::text').getall()).strip(),
                                       "published_date": 'na',
                                       "pdf_url": "https://mtcit.gov.om" + link.css('a').attrib['href']
                                       }
                    except:
                        pass
        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1


class MtcitFrameworksSpider(scrapy.Spider):
    name = "MtcitFrameworksSpider"
    allowed_domains = ["mtcit.gov.om"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                try:
                    data = response.css('tr')
                except:
                    data = []
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('td::text')[0].get().strip() if len(
                                           link.css('td::text')[0].get().strip()) > 5 else link.css('td p::text')[
                                           0].get().strip(),
                                       "excerpt": link.css('td::text')[1].get().strip() if len(
                                           link.css('td::text')[1].get().strip()) > 5 else link.css('td p::text')[
                                           1].get().strip(),
                                       "published_date": 'na',
                                       "pdf_url": link.css('a').attrib['href']
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('td::text')[0].get().strip() if len(
                                           link.css('td::text')[0].get().strip()) > 5 else link.css('td p::text')[
                                           0].get().strip(),
                                       "excerpt": link.css('td::text')[1].get().strip() if len(
                                           link.css('td::text')[1].get().strip()) > 5 else link.css('td p::text')[
                                           1].get().strip(),
                                       "published_date": 'na',
                                       "pdf_url": "https://mtcit.gov.om" + link.css('a').attrib['href']
                                       }



                    except:
                        pass
        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1


class PmaCircularsSpider(scrapy.Spider):
    name = "PmaCircularsSpider"
    allowed_domains = ["pma.ps"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                try:
                    data = response.css('#dnn_ctr2934_ModuleContent').css('li')
                except:
                    data = []
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "https://" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('::text').get(),
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": link.css('a').attrib['href'].replace(' ', '%20'),
                                       "topic": "Palestine Monetary Authority",
                                       "type": "PDF"}
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('::text').get(),
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": "https://www.pma.ps" + link.css('a').attrib['href'].replace(' ',
                                                                                                              '%20'),
                                       "topic": "Palestine Monetary Authority",
                                       "type": "PDF"}

                    except:
                        pass
        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1


class NaPkSpider(scrapy.Spider):
    name = "NaPkSpider"
    allowed_domains = ["na.gov.pk"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                try:
                    data = response.css('#listacts tr')
                except:
                    data = []
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "https://" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('a::text').get(),
                                       "excerpt": 'na',
                                       "published_date": link.css('.odd,.even').css(' td::text')[1].get() if link.css(
                                           '.odd,.even') else 'na',
                                       "pdf_url": link.css('a').attrib['href'].replace('..', ''),
                                       "topic": "National Assembly of Pakistan",
                                       "type": "PDF"}
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('a::text').get(),
                                       "excerpt": 'na',
                                       "published_date": link.css('.odd,.even').css(' td::text')[1].get() if link.css(
                                           '.odd,.even') else 'na',
                                       "pdf_url": "https://na.gov.pk" + link.css('a').attrib['href'].replace('..', ''),
                                       "topic": "National Assembly of Pakistan",
                                       "type": "PDF"}

                    except:
                        pass
        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1


class NaPkMAJLISSpider(scrapy.Spider):
    name = "NaPkMAJLISSpider"
    allowed_domains = ["na.gov.pk"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                try:
                    data = response.css('#pagecontent tr')
                except:
                    data = []
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "https://" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('a::text').get(),
                                       "excerpt": 'na',
                                       "published_date": link.css('.odd,.even').css(' td::text')[1].get() if link.css(
                                           '.odd,.even') else 'na',
                                       "pdf_url": link.css('a').attrib['href'].replace('..', ''),
                                       "topic": "National Assembly of Pakistan",
                                       "type": "PDF"}
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('a::text').get(),
                                       "excerpt": 'na',
                                       "published_date": link.css('.odd,.even').css(' td::text')[1].get() if link.css(
                                           '.odd,.even') else 'na',
                                       "pdf_url": "https://na.gov.pk" + link.css('a').attrib['href'].replace('..', ''),
                                       "topic": "National Assembly of Pakistan",
                                       "type": "PDF"}

                    except:
                        pass
        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1


class NaPkBillsSpider(scrapy.Spider):
    name = "NaPkBillsSpider"
    allowed_domains = ["na.gov.pk"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                try:
                    data = response.css('#pagecontent tr')
                except:
                    data = []
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "https://" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('a::text').get(),
                                       "excerpt": 'na',
                                       "published_date": link.css('tr td').css(' td::text')[1].get() if link.css(
                                           'tr td') else 'na',
                                       "pdf_url": link.css('a').attrib['href'].replace('..', ''),
                                       "topic": "National Assembly of Pakistan",
                                       "type": "PDF"}
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('a::text').get(),
                                       "excerpt": 'na',
                                       "published_date": link.css('tr td').css(' td::text')[1].get() if link.css(
                                           'tr td') else 'na',
                                       "pdf_url": "https://na.gov.pk" + link.css('a').attrib['href'].replace('..', ''),
                                       "topic": "National Assembly of Pakistan",
                                       "type": "PDF"}

                    except:
                        pass
        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1


class NaPkPassBillsSpider(scrapy.Spider):
    name = "NaPkPassBillsSpider"
    allowed_domains = ["na.gov.pk"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                try:
                    data = response.css('#pagecontent tr')
                except:
                    data = []
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "https://" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('a::text').get(),
                                       "excerpt": 'na',
                                       "published_date": link.css('tr td').css(' td::text')[1].get() if link.css(
                                           'tr td') else 'na',
                                       "pdf_url": link.css('a').attrib['href'].replace('..', ''),
                                       "topic": "National Assembly of Pakistan",
                                       "type": "PDF"}
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('a::text').get(),
                                       "excerpt": 'na',
                                       "published_date": link.css('tr td').css(' td::text')[1].get() if link.css(
                                           'tr td') else 'na',
                                       "pdf_url": "https://na.gov.pk" + link.css('a').attrib['href'].replace('..', ''),
                                       "topic": "National Assembly of Pakistan",
                                       "type": "PDF"}

                    except:
                        pass
        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1


class MoroccoTicSpider(scrapy.Spider):
    name = "MoroccoTicSpider"
    allowed_domains = ["portail.tax.gov.ma"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                try:
                    data = response.css('.file').css('a')
                except:
                    data = []
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "https://" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": response.css('.titre-conventions::text').get(),
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": link.css('a').attrib['href'].replace('..', ''),
                                       "topic": "Morocco Tax Authority - Tax on Insurance Contracts",
                                       "type": "PDF"}
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": response.css('.titre-conventions::text').get(),
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": "https://portail.tax.gov.ma" + link.css('a').attrib['href'].replace(
                                           '..', ''),
                                       "topic": "Morocco Tax Authority - Tax on Insurance Contracts",
                                       "type": "PDF"}

                    except:
                        pass
        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1


class MoroccoLocalTaxesSpider(scrapy.Spider):
    name = "MoroccoLocalTaxesSpider"
    allowed_domains = ["portail.tax.gov.ma"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                try:
                    data = response.css('.content.taxes').css('.description,.file')
                except:
                    data = []
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "https://" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('p::text').get() if link.css('p').get() else 'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": link.css('.file').css('a').attrib['href'],
                                       "topic": "Morocco Tax Authority - Local Taxes",
                                       "type": "PDF"}
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('p::text').get() if link.css('p').get() else 'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": "https://portail.tax.gov.ma" + link.css('.file').css('a').attrib[
                                           'href'],
                                       "topic": "Morocco Tax Authority - Local Taxes",
                                       "type": "PDF"}

                    except:
                        pass
        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1


class MoroccoGeneralTaxSpider(scrapy.Spider):
    name = "MoroccoGeneralTaxSpider"
    allowed_domains = ["portail.tax.gov.ma"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                try:
                    data = response.css('.content-allocution').css('.allocution')
                except:
                    data = []
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "https://" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('p::text').get() if link.css('p').get() else 'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": link.css('.file').css('a').attrib['href'],
                                       "topic": "Morocco Tax Authority - General Tax Code",
                                       "type": "PDF"}
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('p::text').get() if link.css('p').get() else 'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": "https://portail.tax.gov.ma" + link.css('.file').css('a').attrib[
                                           'href'],
                                       "topic": "Morocco Tax Authority - General Tax Code",
                                       "type": "PDF"}

                    except:
                        pass
        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1


class MtcitLandTransportRegulationsSpider(scrapy.Spider):
    name = "MtcitLandTransportRegulationsSpider"
    allowed_domains = ["mtcit.gov.om"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                try:
                    data = response.css('tr')
                except:
                    data = []
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "https://" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('.xl65::text,tbody td::text').get().strip(),
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": link.css('a').attrib['href'].replace('..', '')
                                       }

                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('.xl65::text,tbody td::text').get().strip(),
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": "https://mtcit.gov.om/ITAPortal_AR" + link.css('a').attrib[
                                           'href'].replace('..', '')
                                       }
                    except:
                        pass
        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1


class MtcitLandTransportAgreementsSpider(scrapy.Spider):
    name = "MtcitLandTransportAgreementsSpider"
    allowed_domains = ["mtcit.gov.om"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                try:
                    data = response.css('tr')
                except:
                    data = []
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "https://" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css(
                                           '.xl65::text,tbody td::text, tr td::text').get().strip() if link.css(
                                           '.xl65::text,tbody td::text, tr td::text').get() else 'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": link.css('a').attrib['href'].replace('..', ''),
                                       "topic": "Ministry of Transport, Communications, and Information Technology (Oman) - Land Transport - Agreements",
                                       "type": "PDF"}
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css(
                                           '.xl65::text,tbody td::text,tr td::text').get().strip() if link.css(
                                           '.xl65::text,tbody td::text, tr td::text').get() else 'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": "https://mtcit.gov.om/ITAPortal_AR" + link.css('a').attrib[
                                           'href'].replace('..', ''),
                                       "topic": "Ministry of Transport, Communications, and Information Technology (Oman) - Land Transport - Agreements",
                                       "type": "PDF"}

                    except:
                        pass
        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1


class UabTurkeySpider(scrapy.Spider):
    name = "UabTurkeySpider"
    allowed_domains = ["uab.gov.tr"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                self.start_urls,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

        logging.info(
            {
                "proxy": "1",
                "clean_url": self.allowed_domains[0],
                "link": self.start_urls,
            }
        )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                try:
                    data = response.css('.cPjQhf,.ksFEfE')
                except:
                    data = []
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "https://" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('span::text').get().strip(),
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": link.css('a').attrib['href'].replace('..', ''),
                                       "topic": "Ministry of Transport and Infrastructure (Turkey)",
                                       "type": "PDF"}
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('span::text').get().strip(),
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": "https://mtcit.gov.om/ITAPortal_AR" + link.css('a').attrib[
                                           'href'].replace('..', ''),
                                       "topic": "Ministry of Transport and Infrastructure (Turkey)",
                                       "type": "PDF"}

                    except:
                        pass

        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1


class MtcitICTGovernanceSpider(scrapy.Spider):
    name = "MtcitICTGovernanceSpider"
    allowed_domains = ["mtcit.gov.om"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                try:
                    data = response.css('tr')
                except:
                    data = []
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "https://" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('tr td::text').get().strip() if len(
                                           str(link.css('tr td::text').get().strip())) > 5
                                       else link.css('td')[1].css('::text').get().strip(),
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": link.css('a').attrib['href'].replace('..', '').replace(' ', '%20')
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('tr td::text').get().strip() if len(
                                           str(link.css('tr td::text').get().strip())) > 5
                                       else link.css('td')[1].css('::text').get().strip(),
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": "https://mtcit.gov.om/ITAPortal_AR" + link.css('a').attrib[
                                           'href'].replace('..', '').replace(' ', '%20')
                                       }
                    except:
                        pass
        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1


class RtaLegislationSpider(scrapy.Spider):
    name = "RtaLegislationSpider"
    allowed_domains = ["rta.ae"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                try:
                    data = response.css('.vAlignMiddle')
                except:
                    data = []
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "https://" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('td::text')[1].get().strip(),
                                       "excerpt": 'na',
                                       "published_date": link.css('td::text')[3].get() if link.css('td')[
                                           3].get() else 'na',
                                       "pdf_url": link.css('a').attrib['href'].replace('..', '').replace(' ', '%20'),
                                       "topic": "Dubai RTA - Agencies Legislation",
                                       "type": "PDF"}
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('td::text')[1].get().strip(),
                                       "excerpt": 'na',
                                       "published_date": link.css('td::text')[3].get() if link.css('td')[
                                           3].get() else 'na',
                                       "pdf_url": "https://www.rta.ae" + link.css('a').attrib['href'].replace('..',
                                                                                                              '').replace(
                                           ' ', '%20'),
                                       "topic": "Dubai RTA - Agencies Legislation",
                                       "type": "PDF"}

                    except:
                        pass
        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1


class RtaLTransportSpider(scrapy.Spider):
    name = "RtaLTransportSpider"
    allowed_domains = ["rta.ae"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                try:
                    data = response.css('.vAlignMiddle')
                except:
                    data = []
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "https://" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('td::text')[1].get().strip(),
                                       "excerpt": 'na',
                                       "published_date": link.css('td::text')[3].get() if link.css('td')[
                                           3].get() else 'na',
                                       "pdf_url": link.css('a').attrib['href'].replace('..', '').replace(' ', '%20'),
                                       "topic": "Dubai RTA - Public Transport Legislation",
                                       "type": "PDF"}
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('td::text')[1].get().strip(),
                                       "excerpt": 'na',
                                       "published_date": link.css('td::text')[3].get() if link.css('td')[
                                           3].get() else 'na',
                                       "pdf_url": "https://www.rta.ae" + link.css('a').attrib['href'].replace('..',
                                                                                                              '').replace(
                                           ' ', '%20'),
                                       "topic": "Dubai RTA - Public Transport Legislation",
                                       "type": "PDF"}

                    except:
                        pass
        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1


class RtaRoadsSpider(scrapy.Spider):
    name = "RtaRoadsSpider"
    allowed_domains = ["rta.ae"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                try:
                    data = response.css('.vAlignMiddle')
                except:
                    data = []
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "https://" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('td::text')[1].get().strip(),
                                       "excerpt": 'na',
                                       "published_date": link.css('td::text')[3].get() if link.css('td')[
                                           3].get() else 'na',
                                       "pdf_url": link.css('a').attrib['href'].replace('..', '').replace(' ', '%20'),
                                       "topic": "Dubai RTA - Traffic and Roads Legislation",
                                       "type": "PDF"}
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('td::text')[1].get().strip(),
                                       "excerpt": 'na',
                                       "published_date": link.css('td::text')[3].get() if link.css('td')[
                                           3].get() else 'na',
                                       "pdf_url": "https://www.rta.ae" + link.css('a').attrib['href'].replace('..',
                                                                                                              '').replace(
                                           ' ', '%20'),
                                       "topic": "Dubai RTA - Traffic and Roads Legislation",
                                       "type": "PDF"}

                    except:
                        pass
        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1


class PmaAntiMoneySpider(scrapy.Spider):
    name = "PmaAntiMoneySpider"
    allowed_domains = ["pma.ps"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                try:
                    data = response.css('.dnnClear').css('li')
                except:
                    data = []
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "https://" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('::text').get(),
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": link.css('a').attrib['href'].replace(' ', '%20'),
                                       "topic": "Palestine Monetary Authority",
                                       "type": "PDF"}
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('::text').get(),
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": "https://www.pma.ps" + link.css('a').attrib['href'].replace(' ',
                                                                                                              '%20'),
                                       "topic": "Palestine Monetary Authority",
                                       "type": "PDF"}

                    except:
                        pass
        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1


class PmaLawSpider(scrapy.Spider):
    name = "PmaLawSpider"
    allowed_domains = ["pma.ps"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                try:
                    data = response.css('#dnn_ctr1067_HtmlModule_lblContent').css('li')
                except:
                    data = []
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "https://" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('::text').get(),
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": link.css('a').attrib['href'].replace(' ', '%20'),
                                       "topic": "Palestine Monetary Authority",
                                       "type": "PDF"}
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('::text').get(),
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": "https://www.pma.ps" + link.css('a').attrib['href'].replace(' ',
                                                                                                              '%20'),
                                       "topic": "Palestine Monetary Authority",
                                       "type": "PDF"}

                    except:
                        pass
        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1


class PmaLegislationSpider(scrapy.Spider):
    name = "PmaLegislationSpider"
    allowed_domains = ["pma.ps"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                try:
                    data = response.css('#dnn_ctr1883_HtmlModule_lblContent').css('li')
                except:
                    data = []
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "https://" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('::text').get(),
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": link.css('a').attrib['href'].replace(' ', '%20'),
                                       "topic": "Palestine Monetary Authority",
                                       "type": "PDF"}
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('::text').get(),
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": "https://www.pma.ps" + link.css('a').attrib['href'].replace(' ',
                                                                                                              '%20'),
                                       "topic": "Palestine Monetary Authority",
                                       "type": "PDF"}

                    except:
                        pass
        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1


class PmaRegulationsSpider(scrapy.Spider):
    name = "PmaRegulationsSpider"
    allowed_domains = ["pma.ps"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                try:
                    data = response.css('#dnn_ctr1068_HtmlModule_lblContent').css('li')
                except:
                    data = []
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "https://" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('::text').get(),
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": link.css('a').attrib['href'].replace(' ', '%20'),
                                       "topic": "Palestine Monetary Authority",
                                       "type": "PDF"}
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('::text').get(),
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": "https://www.pma.ps" + link.css('a').attrib['href'].replace(' ',
                                                                                                              '%20'),
                                       "topic": "Palestine Monetary Authority",
                                       "type": "PDF"}

                    except:
                        pass
        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1


class SbpPakSpider(scrapy.Spider):
    name = "SbpPakSpider"
    allowed_domains = ["sbp.org.pk"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

        logging.info(
            {
                "proxy": "1",
                "clean_url": self.allowed_domains[0],
                "link": self.start_urls,
            }
        )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                try:
                    data = response.css('tr~ tr+ tr td+ td')
                except:
                    data = []
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('a::text,font::text').get().strip(),
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": link.css('a').attrib['href'].replace('..', ''),
                                       "topic": "State Bank of Pakistan - Laws & Regulations",
                                       "type": "PDF"}
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('a::text,font::text').get().strip(),
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": "https://www.sbp.org.pk" + link.css('a').attrib['href'].replace('..',
                                                                                                                  ''),
                                       "topic": "State Bank of Pakistan - Laws & Regulations",
                                       "type": "PDF"}

                    except:
                        pass
        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1


class BaghdadEregulationsSpider(scrapy.Spider):
    name = "BaghdadEregulationsSpider"
    allowed_domains = ["baghdad.eregulations.org"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

        logging.info(
            {
                "proxy": "1",
                "clean_url": self.allowed_domains[0],
                "link": self.start_urls,
            }
        )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                try:
                    data = response.css('td')
                except:
                    data = []
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('.text-center+ td::text').get() if link.css('a').attrib['href'] else 'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "link": link.css('a').attrib['href'].replace(' ', '%20'),
                                       "topic": "E-Regulations Baghdad",
                                       "type": "PDF"}
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('.text-center+ td::text').get() if link.css('a').attrib['href'] else 'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "link": "https://baghdad.eregulations.org" + link.css('a').attrib[
                                           'href'].replace(' ', '%20'),
                                       "topic": "E-Regulations Baghdad",
                                       "type": "PDF"}

                    except:
                        pass
        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1


class MohSpider(scrapy.Spider):
    name = "MohSpider"
    allowed_domains = ["moh.gov.bh"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

        logging.info(
            {
                "proxy": "1",
                "clean_url": self.allowed_domains[0],
                "link": self.start_urls,
            }
        )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                try:
                    data = response.css('tr+ tr td~ td+ td')
                except:
                    data = []
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('a::text').get().strip(),
                                       "excerpt": 'na',
                                       "published_date": link.css('td::text').get() if link.css(
                                           'td::text').get() else 'na',
                                       "pdf_url": link.css('a').attrib['href'].replace(' ', '%20'),
                                       "topic": "Ministry of Health - Health Law & Legislation",
                                       "type": "PDF"}
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('a::text').get().strip(),
                                       "excerpt": 'na',
                                       "published_date": link.css('td::text').get() if link.css(
                                           'td::text').get() else 'na',
                                       "pdf_url": "https://www.moh.gov.bh" + link.css('a').attrib['href'].replace(' ',
                                                                                                                  '%20'),
                                       "topic": "Ministry of Health - Health Law & Legislation",
                                       "type": "PDF"}

                    except:
                        pass
        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1


class MtcitCybercrimeSpider(scrapy.Spider):
    name = "MtcitCybercrimeSpider"
    allowed_domains = ["ita.gov.om"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                categories_links = []
                res = response.css('.text-section')
                for r in res:
                    try:
                        if "http" in str(r.css("a").attrib["href"]):
                            categories_links.append(r.css("a").attrib["href"])
                        else:
                            categories_links.append(
                                r.css("a").attrib["href"]
                            )

                    except:
                        pass

                for i in set(categories_links):
                    try:
                        yield scrapy.Request(
                            i,
                            callback=self.article_links,
                            meta={"dont_retry": True, "download_timeout": cat_timeout,"base_url": response.url},
                            errback=handle_error,
                        )


                    except:
                        pass

    def article_links(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                base_url = response.meta.get('base_url')
                data = response.css('.text-center')

                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]) and link.css("a").attrib["href"]:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": base_url,
                                       "title": response.css('#MainForm h3::text').get().strip(),
                                       "excerpt": response.css('p::text').get().strip(),
                                       "published_date": link.css('td.text-center::text').get() if link.css(
                                           'td.text-center::text').get() else 'na',
                                       "pdf_url": link.css('a').attrib['href'].replace(' ', '%20').replace('.',
                                                                                                           '').replace(
                                           'pdf', '.pdf'),
                                       "topic": "Ministry of Transport, Communications, and Information Technology (Oman) - ICT - Cybercrime",
                                       "type": "PDF"}
                            elif link.css("a").attrib["href"]:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": base_url,
                                       "title": response.css('#MainForm h3::text').get().strip(),
                                       "excerpt": response.css('p::text').get().strip(),
                                       "published_date": link.css('td.text-center::text').get() if link.css(
                                           'td.text-center::text').get() else 'na',
                                       "pdf_url": "https://www.ita.gov.om/ITAPortal_AR" + link.css('a').attrib[
                                           'href'].replace(' ', '%20').replace('.', '').replace('pdf', '.pdf'),
                                       "topic": "Ministry of Transport, Communications, and Information Technology (Oman) - ICT - Cybercrime",
                                       "type": "PDF"}
                            else:
                                pass
                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class BOASpider(scrapy.Spider):
    name = "BOASpider"
    allowed_domains = ["bank-of-algeria.dz"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

        logging.info(
            {
                "proxy": "1",
                "clean_url": self.allowed_domains[0],
                "link": self.start_urls,
            }
        )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                try:
                    data = response.css('.style57')
                except:
                    data = []
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('a strong span::text').get().strip(),
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": link.css('a').attrib['href'].replace('..', ''),
                                       "topic": "Bank of Algeria",
                                       "type": "PDF"}
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('a strong span::text').get().strip(),
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": "https://www.bank-of-algeria.dz" + link.css('a').attrib[
                                           'href'].replace('..', ''),
                                       "topic": "Bank of Algeria",
                                       "type": "PDF"}

                    except:
                        pass
        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1


class MtcitFinalSpider(scrapy.Spider):
    name = "MtcitFinalSpider"
    allowed_domains = ["ita.gov.om"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                try:
                    data = response.css('tr')
                except:
                    data = []
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if '.pdf' in str(link.css("a").attrib["href"]):
                                if "https://" in str(link.css("a").attrib["href"]):
                                    yield {"clean_url": self.allowed_domains[0],
                                           "base_url": response.url,
                                           "title": link.css('tr td::text').get().strip() if len(
                                               str(link.css('tr td::text').get().strip())) > 5
                                           else link.css('td')[1].css('::text').get().strip(),
                                           "excerpt": 'na',
                                           "published_date": 'na',
                                           "pdf_url": link.css('a').attrib['href'].replace('..', '').replace(' ',
                                                                                                             '%20'),
                                           "topic": "Ministry of Transport, Communications, and Information Technology (Oman) - ICT Governance - Instructions",
                                           "type": "PDF"}
                                else:
                                    yield {"clean_url": self.allowed_domains[0],
                                           "base_url": response.url,
                                           "title": link.css('tr td::text').get().strip() if len(
                                               str(link.css('tr td::text').get().strip())) > 5
                                           else link.css('td')[1].css('::text').get().strip(),
                                           "excerpt": 'na',
                                           "published_date": 'na',
                                           "pdf_url": "https://mtcit.gov.om/ITAPortal_AR" + link.css('a').attrib[
                                               'href'].replace('..', '').replace(' ', '%20'),
                                           "topic": "Ministry of Transport, Communications, and Information Technology (Oman) - ICT Governance - Instructions",
                                           "type": "PDF"}
                            else:
                                if "https://" in str(link.css("a").attrib["href"]):
                                    yield scrapy.Request(
                                        link.css('a').attrib['href'].replace('..', '').replace(' ', '%20'),
                                        callback=self.article_links,
                                        meta={"dont_retry": True, "download_timeout": cat_timeout},
                                        errback=handle_error,
                                    )
                                else:
                                    a = "https://mtcit.gov.om/ITAPortal_AR" + link.css('a').attrib['href'].replace('..',
                                                                                                                   '').replace(
                                        ' ', '%20')
                                    yield scrapy.Request(
                                        "https://mtcit.gov.om/ITAPortal_AR" + link.css('a').attrib['href'].replace('..',
                                                                                                                   '').replace(
                                            ' ', '%20'),
                                        callback=self.article_links,
                                        meta={"dont_retry": True, "download_timeout": cat_timeout},
                                        errback=handle_error,
                                    )



                    except:
                        pass
        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1

    def article_links(self, response):
        if str(response.status) == "200":
            data = response.css('.info .text-center:nth-child(4)')
            for link in data:
                try:
                    if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                        pass
                    else:

                        if "https://" in str(link.css("a").attrib["href"]):
                            yield {"clean_url": self.allowed_domains[0],
                                   "base_url": response.url,
                                   "title": 'na',
                                   "excerpt": 'na',
                                   "published_date": 'na',
                                   "pdf_url": link.css('a').attrib['href'].replace('..', '').replace(' ', '%20'),
                                   "topic": "Ministry of Transport, Communications, and Information Technology (Oman) - ICT Governance - Instructions",
                                   "type": "PDF"}
                        else:
                            yield {"clean_url": self.allowed_domains[0],
                                   "base_url": response.url,
                                   "title": 'na',
                                   "excerpt": 'na',
                                   "published_date": 'na',
                                   "pdf_url": "https://mtcit.gov.om/ITAPortal_AR" + link.css('a').attrib[
                                       'href'].replace('..', '').replace(' ', '%20'),
                                   "topic": "Ministry of Transport, Communications, and Information Technology (Oman) - ICT Governance - Instructions",
                                   "type": "PDF"}

                except:
                    pass
        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1


class ArabianBusinessSpider(scrapy.Spider):
    name = "ArabianBusinessSpider"
    allowed_domains = ['arabianbusiness.com']
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        yield scrapy.Request(url="https://books.toscrape.com/", callback=self.parse, dont_filter=True)

    def parse(self, response):
        for i in self.start_urls:
            responses = rq.get(
                i,
                proxies={
                    "http": "http://cdfcd4e233464959ac5f1f8d45a9c05f:@proxy.crawlera.com:8011/",
                    "https": "http://cdfcd4e233464959ac5f1f8d45a9c05f:@proxy.crawlera.com:8011/",
                },
                verify=verify_certi,
                headers={"X-Crawlera-Region": "IN"}
            )

            soup = BeautifulSoup(responses.text, 'lxml')
            result = soup.find_all('h2', attrs={'class': 'entry-title'})
            for j in result:
                yield {"clean_url": self.allowed_domains[0],
                       "base_url": i,
                       'link': j.find('a').get('href'),
                       "type": 'article'}


class CatererMiddleeastSpider(scrapy.Spider):
    name = "CatererMiddleeastSpider"
    allowed_domains = ['caterermiddleeast.com']
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        yield scrapy.Request(url="https://books.toscrape.com/", callback=self.parse, dont_filter=True)

    def parse(self, response):
        responses = rq.get(
            self.start_urls[0],
            proxies={
                "http": "http://cdfcd4e233464959ac5f1f8d45a9c05f:@proxy.crawlera.com:8011/",
                "https": "http://cdfcd4e233464959ac5f1f8d45a9c05f:@proxy.crawlera.com:8011/",
            },
            verify=verify_certi,
            headers={"X-Crawlera-Region": "IN"}
        )

        soup = BeautifulSoup(responses.text, 'lxml')
        result = soup.find_all('h2', attrs={'class': 'entry-title'})
        for i in result:
            yield {"clean_url": self.allowed_domains[0],
                   "base_url": self.start_urls[0],
                   'link': i.find('a').get('href'),
                   "type": 'article'}


class FtSpider(scrapy.Spider):
    name = "FtSpider"
    allowed_domains = ["ft.com"]
    not_allowed_keyword = ["/newsletters", "/twitter", "/digitalassets"]
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

        logging.info(
            {
                "proxy": "1",
                "clean_url": self.allowed_domains[0],
                "link": self.start_urls,
            }
        )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css(".o-teaser__heading")
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www.ft.com" + link.css("a").attrib["href"]
                                       }
                    except:
                        pass

        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1


class AlbawabaSpider(scrapy.Spider):
    name = "AlbawabaSpider"
    allowed_domains = ["albawaba.com"]
    not_allowed_keyword = ["/newsletters", "/twitter", "/digitalassets"]
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

        logging.info(
            {
                "proxy": "1",
                "clean_url": self.allowed_domains[0],
                "link": self.start_urls,
            }
        )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css(".field--name-node-title, .press_release")
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www.albawaba.com" + link.css("a").attrib["href"]
                                       }
                    except:
                        pass

        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1


class IndiaTimesSpider(scrapy.Spider):
    name = "IndiaTimesSpider"
    allowed_domains = ["indiatimes.com"]
    not_allowed_keyword = ["/newsletters", "/twitter"]
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

        logging.info(
            {
                "proxy": "1",
                "clean_url": self.allowed_domains[0],
                "link": self.start_urls,
            }
        )

    def parse(self, response):

        if str(response.status) == "200":
            if response.css("body"):
                data = response.css(".descBx h3,.contentD")
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://" + '.'.join(list(tldextract.extract(response.url))) +
                                               link.css("a").attrib["href"]
                                       }
                    except:
                        pass

        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1


class EuromoneySpider(scrapy.Spider):
    name = "EuromoneySpider"
    allowed_domains = ["euromoney.com"]
    not_allowed_keyword = ["/newsletters", "/twitter"]
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

        logging.info(
            {
                "proxy": "1",
                "clean_url": self.allowed_domains[0],
                "link": self.start_urls,
            }
        )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css(".PromoA-title,.PromoB-title")
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www.euromoney.com" + link.css("a").attrib["href"]
                                       }

                    except:
                        pass


        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1


class ArabNewsSpider(scrapy.Spider):
    name = "ArabNewsSpider"
    allowed_domains = ["arabnews.com"]
    not_allowed_keyword = ["/newsletters", "/twitter", "/digitalassets"]
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

        logging.info(
            {
                "proxy": "1",
                "clean_url": self.allowed_domains[0],
                "link": self.start_urls,
            }
        )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css(".media-object-section,.emperor-font-color,.article-item-title a")
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www.arabnews.com" + link.css("a").attrib["href"]
                                       }
                    except:
                        pass

        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1


class ReutersSpider(scrapy.Spider):
    name = "ReutersSpider"
    allowed_domains = ["reuters.com"]
    not_allowed_keyword = ["/newsletters", "/twitter", "/digitalassets"]
    check_ip_category = 0
    check_ip_article_links = 0
    check_ip_sub_category = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

        logging.info(
            {
                "proxy": "1",
                "clean_url": self.allowed_domains[0],
                "link": self.start_urls,
            }
        )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css(".media-story-card__heading__eqhp9")

                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www.reuters.com" + link.css("a").attrib["href"]
                                       }

                    except:
                        pass

        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1


class OkazSpider(scrapy.Spider):
    name = "OkazSpider"
    allowed_domains = ["okaz.com.sa"]
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

        logging.info(
            {
                "proxy": "1",
                "clean_url": self.allowed_domains[0],
                "link": self.start_urls,
            }
        )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('h6 , #dailyTab a , .avisited , #content .mobile-hidden a')
                for link in data:
                    try:
                        if "http" in str(link.css("a").attrib["href"]):
                            yield {"clean_url": self.allowed_domains[0],
                                   "base_url": response.url,
                                   "link": link.css("a").attrib["href"]
                                   }
                        else:
                            yield {"clean_url": self.allowed_domains[0],
                                   "base_url": response.url,
                                   "link": "https://www.okaz.com.sa" + link.css("a").attrib["href"]
                                   }

                    except:
                        pass

        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1


class AleqtSpider(scrapy.Spider):
    name = "AleqtSpider"
    allowed_domains = ["aleqt.com"]
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

        logging.info(
            {
                "proxy": "1",
                "clean_url": self.allowed_domains[0],
                "link": self.start_urls,
            }
        )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css(".field-content a , h6 , .slide-caption")
                for link in data:
                    try:
                        if "http" in str(link.css("a").attrib["href"]):
                            yield {"clean_url": self.allowed_domains[0],
                                   "base_url": response.url,
                                   "link": link.css("a").attrib["href"]
                                   }
                        else:
                            yield {"clean_url": self.allowed_domains[0],
                                   "base_url": response.url,
                                   "link": "https://www.aleqt.com" + link.css("a").attrib["href"]
                                   }

                    except:
                        pass

        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1


class AlittihadSpider(scrapy.Spider):
    name = "AlittihadSpider"
    allowed_domains = ["alittihad.ae"]
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

        logging.info(
            {
                "proxy": "1",
                "clean_url": self.allowed_domains[0],
                "link": self.start_urls,
            }
        )

    def parse(self, response):

        if str(response.status) == "200":
            if response.css("body"):
                data = response.css(".most-read-text,.innerdesc , .mb0")

                for link in data:
                    try:
                        if "http" in str(link.css("a").attrib["href"]):
                            yield {"clean_url": self.allowed_domains[0],
                                   "base_url": response.url,
                                   "link": link.css("a").attrib["href"]
                                   }
                        else:
                            yield {"clean_url": self.allowed_domains[0],
                                   "base_url": response.url,
                                   "link": "https://www.alittihad.ae" + link.css("a").attrib["href"]
                                   }

                    except:
                        pass

        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1


class EmaratalyoumSpider(scrapy.Spider):
    name = "EmaratalyoumSpider"
    allowed_domains = ["emaratalyoum.com"]
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

        logging.info(
            {
                "proxy": "1",
                "clean_url": self.allowed_domains[0],
                "link": self.start_urls,
            }
        )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css(".text , #mainslider h2")

                for link in data:
                    try:
                        if "http" in str(link.css("a").attrib["href"]):
                            yield {"clean_url": self.allowed_domains[0],
                                   "base_url": response.url,
                                   "link": link.css("a").attrib["href"]
                                   }
                        else:
                            yield {"clean_url": self.allowed_domains[0],
                                   "base_url": response.url,
                                   "link": "https://www.emaratalyoum.com" + link.css("a").attrib["href"]
                                   }

                    except:
                        pass

        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1


class ZawyaSpider(scrapy.Spider):
    name = "ZawyaSpider"
    allowed_domains = ["zawya.com"]
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

        logging.info(
            {
                "proxy": "1",
                "clean_url": self.allowed_domains[0],
                "link": self.start_urls,
            }
        )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css(".teaser-title a")
                for link in data:
                    try:
                        if "http" in str(link.css("a").attrib["href"]):
                            yield {"clean_url": self.allowed_domains[0],
                                   "base_url": response.url,
                                   "link": link.css("a").attrib["href"]
                                   }
                        else:
                            yield {"clean_url": self.allowed_domains[0],
                                   "base_url": response.url,
                                   "link": "https://www.zawya.com" + link.css("a").attrib["href"]
                                   }
                    except:
                        pass
        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1


class BnnBloombergSpider(scrapy.Spider):
    name = "BnnBloombergSpider"
    allowed_domains = ["bnnbloomberg.ca"]
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

        logging.info(
            {
                "proxy": "1",
                "clean_url": self.allowed_domains[0],
                "link": self.start_urls,
            }
        )

    def parse(self, response):

        if str(response.status) == "200":
            if response.css("body"):
                data = response.css(".article-content")
                for link in data:
                    try:
                        if "http" in str(link.css("a").attrib["href"]):
                            yield {"clean_url": self.allowed_domains[0],
                                   "base_url": response.url,
                                   "link": link.css("a").attrib["href"]
                                   }
                        else:
                            yield {"clean_url": self.allowed_domains[0],
                                   "base_url": response.url,
                                   "link": "https://www.bnnbloomberg.ca" + link.css("a").attrib["href"]
                                   }
                    except:
                        pass

        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1


class CNBCSpider(scrapy.Spider):
    name = "CNBCSpider"
    allowed_domains = ["cnbc.com"]
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css(".Card-titleContainer")
                for link in data:
                    try:
                        if "http" in str(link.css("a").attrib["href"]):
                            yield {"clean_url": self.allowed_domains[0],
                                   "base_url": response.url,
                                   "link": link.css("a").attrib["href"]
                                   }
                        else:
                            yield {"clean_url": self.allowed_domains[0],
                                   "base_url": response.url,
                                   "link": "https://www.cnbc.com" + link.css("a").attrib["href"]
                                   }

                    except:
                        pass

        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1


class BreCorderSpider(scrapy.Spider):
    name = "BreCorderSpider"
    allowed_domains = ["brecorder.com"]
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

        logging.info(
            {
                "proxy": "1",
                "clean_url": self.allowed_domains[0],
                "link": self.start_urls,
            }
        )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css(".hover\:text-gray-800")
                for link in data:
                    try:
                        if "http" in str(link.css("a").attrib["href"]):
                            yield {"clean_url": self.allowed_domains[0],
                                   "base_url": response.url,
                                   "link": link.css("a").attrib["href"]
                                   }
                        else:
                            yield {"clean_url": self.allowed_domains[0],
                                   "base_url": response.url,
                                   "link": "https://www.brecorder.com" + link.css("a").attrib["href"]
                                   }

                    except:
                        pass

        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1


class GulfNewsSpider(scrapy.Spider):
    name = "GulfNewsSpider"
    allowed_domains = ["gulfnews.com"]
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

        logging.info(
            {
                "proxy": "1",
                "clean_url": self.allowed_domains[0],
                "link": self.start_urls,
            }
        )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css(".card-title")
                for link in data:
                    try:
                        if "http" in str(link.css("a").attrib["href"]):
                            yield {"clean_url": self.allowed_domains[0],
                                   "base_url": response.url,
                                   "link": link.css("a").attrib["href"]
                                   }
                        else:
                            yield {"clean_url": self.allowed_domains[0],
                                   "base_url": response.url,
                                   "link": "https://gulfnews.com" + link.css("a").attrib["href"]
                                   }

                    except:
                        pass

        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1


class MercuryNewsSpider(scrapy.Spider):
    name = "MercuryNewsSpider"
    allowed_domains = ["mercurynews.com"]
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

        logging.info(
            {
                "proxy": "1",
                "clean_url": self.allowed_domains[0],
                "link": self.start_urls,
            }
        )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                try:
                    data = response.css(".article-info .entry-title .article-title")
                except:
                    data = []
                for link in data:
                    try:
                        if "http" in str(link.css("a").attrib["href"]):
                            yield {"clean_url": self.allowed_domains[0],
                                   "base_url": response.url,
                                   "link": link.css("a").attrib["href"]
                                   }
                        else:
                            yield {"clean_url": self.allowed_domains[0],
                                   "base_url": response.url,
                                   "link": "https://www.mercurynews.com" + link.css("a").attrib["href"]
                                   }
                    except:
                        pass
        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1


class EntrepreneurSpider(scrapy.Spider):
    name = "EntrepreneurSpider"
    allowed_domains = ["entrepreneur.com"]
    not_allowed_keyword = ['/video']
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

        logging.info(
            {
                "proxy": "1",
                "clean_url": self.allowed_domains[0],
                "link": self.start_urls,
            }
        )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                try:
                    data = response.css('h3 , .pl-jumbotron .ga-click')
                except:
                    data = []
                for link in data:
                    try:
                        if "http" in str(link.css("a").attrib["href"]):
                            yield {"clean_url": self.allowed_domains[0],
                                   "base_url": response.url,
                                   "link": link.css("a").attrib["href"]
                                   }
                        else:
                            yield {"clean_url": self.allowed_domains[0],
                                   "base_url": response.url,
                                   "link": "https://www.entrepreneur.com" + link.css("a").attrib["href"]
                                   }
                    except:
                        pass
        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1


class JfdaJoSpider(scrapy.Spider):
    name = "JfdaJoSpider"
    allowed_domains = ["jfda.jo"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.ContentNews')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "https://" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]}
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "http://www.jfda.jo" + link.css("a").attrib["href"].replace('..', '')}

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class AryNewsSpider(scrapy.Spider):
    name = "AryNewsSpider"
    allowed_domains = ["arynews.tv"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

        logging.info(
            {
                "proxy": "1",
                "clean_url": self.allowed_domains[0],
                "link": self.start_urls,
            }
        )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                try:
                    data = response.css('.td-module-title')
                except:
                    data = []
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "https://" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"].strip()}
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://arynews.tv" + link.css("a").attrib["href"].strip()}
                    except:
                        pass
        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1


class trapressreleasesSpider(scrapy.Spider):
    name = "trapressreleasesSpider"
    allowed_domains = ["tra.gov.eg"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.display-posts-listing .title')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www.tra.gov.eg" + link.css("a").attrib["href"],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class kurdsatenglishCategoryIraqSpider(scrapy.Spider):
    name = "kurdsatenglishCategoryIraqSpider"
    allowed_domains = ["kurdsatenglish.com"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.rangdanboxBitmy')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://kurdsatenglish.com/" + link.css("a").attrib["href"],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class zagrosnewsSpider(scrapy.Spider):
    name = "zagrosnewsSpider"
    allowed_domains = ["zagrosnews.net"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.news-item')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://zagrosnews.net" + link.css("a").attrib["href"],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class thestartupsceneSpider(scrapy.Spider):
    name = "thestartupsceneSpider"
    allowed_domains = ["thestartupscene.me"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.block-title a')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://thestartupscene.me" + link.css("a").attrib["href"],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class ptvNewsSpider(scrapy.Spider):
    name = "ptvNewsSpider"
    allowed_domains = ["ptv.com.pk"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.Newscategory a')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://ptv.com.pk" + link.css("a").attrib["href"],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class palpressSpider(scrapy.Spider):
    name = "palpressSpider"
    allowed_domains = ["palpress.net"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.post-info-2 .title')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www.palpress.net" + link.css("a").attrib["href"],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class omanobserverScitechSpider(scrapy.Spider):
    name = "omanobserverScitechSpider"
    allowed_domains = ["omanobserver.om"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.article-title-big , .colorGrey')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www.omanobserver.om" + link.css("a").attrib["href"],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class omanobserverBankingSpider(scrapy.Spider):
    name = "omanobserverBankingSpider"
    allowed_domains = ["omanobserver.om"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.article')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www.omanobserver.om" + link.css("a").attrib["href"],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class ninanewsNews4Spider(scrapy.Spider):
    name = "ninanewsNews4Spider"
    allowed_domains = ["ninanews.com"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

        logging.info(
            {
                "proxy": "1",
                "clean_url": self.allowed_domains[0],
                "link": self.start_urls,
            }
        )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.post-content')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://ninanews.com" + link.css("a").attrib["href"],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class marocgazetteTECHNOLOGYSpider(scrapy.Spider):
    name = "marocgazetteTECHNOLOGYSpider"
    allowed_domains = ["marocgazette.com"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.auto-new-read a')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "http://marocgazette.com/" + link.css("a").attrib["href"],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class inaEconomySpider(scrapy.Spider):
    name = "inaEconomySpider"
    allowed_domains = ["ina.iq"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.para a')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://ina.iq" + link.css("a").attrib["href"],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class aweneSpider(scrapy.Spider):
    name = "aweneSpider"
    allowed_domains = ["awene.com"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.newstopsumbtitle a')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www.awene.com/" + link.css("a").attrib["href"],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class dfsaAlertsSpider(scrapy.Spider):
    name = "dfsaAlertsSpider"
    allowed_domains = ["dfsa.ae"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

        logging.info(
            {
                "proxy": "1",
                "clean_url": self.allowed_domains[0],
                "link": self.start_urls,
            }
        )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.item')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www.dfsa.ae" + link.css("a").attrib["href"],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class almadinaSpider(scrapy.Spider):
    name = "almadinaSpider"
    allowed_domains = ["al-madina.com"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

        logging.info(
            {
                "proxy": "1",
                "clean_url": self.allowed_domains[0],
                "link": self.start_urls,
            }
        )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.article-title a')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www.al-madina.com" + link.css("a").attrib["href"],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class mediaGovSaNewsSpider(scrapy.Spider):
    name = "mediaGovSaNewsSpider"
    allowed_domains = ["media.gov.sa"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

        logging.info(
            {
                "proxy": "1",
                "clean_url": self.allowed_domains[0],
                "link": self.start_urls,
            }
        )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('#rs_read_this .justify-content-between a')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www.media.gov.sa" + link.css("a").attrib["href"],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class cmaMarketNewsPagesSpider(scrapy.Spider):
    name = "cmaMarketNewsPagesSpider"
    allowed_domains = ["cma.org.sa"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.anno_arch')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://cma.org.sa" + link.css("a").attrib["href"],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class nccMediaCenterNewsPagesSpider(scrapy.Spider):
    name = "nccMediaCenterNewsPagesSpider"
    allowed_domains = ["ncc.gov.sa"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

        logging.info(
            {
                "proxy": "1",
                "clean_url": self.allowed_domains[0],
                "link": self.start_urls,
            }
        )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('a.item_news')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www.ncc.gov.sa" + link.css("a").attrib["href"],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class mewaMediaCenterNewsPagesHomeSpider(scrapy.Spider):
    name = "mewaMediaCenterNewsPagesHomeSpider"
    allowed_domains = ["mewa.gov.sa"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('a.card-NewsInternal')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www.mewa.gov.sa/ar/MediaCenter/News/Pages/" +
                                               link.css("a").attrib["href"],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class techxploreSpider(scrapy.Spider):
    name = "techxploreSpider"
    allowed_domains = ["mewa.gov.sa"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('a.news-link')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://techxplore.com" + link.css("a").attrib["href"],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class aljazirahSpider(scrapy.Spider):
    name = "aljazirahSpider"
    allowed_domains = ["al-jazirah.com"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

        logging.info(
            {
                "proxy": "1",
                "clean_url": self.allowed_domains[0],
                "link": self.start_urls,
            }
        )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.news :nth-child(1) div :nth-child(2) a')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www.al-jazirah.com" + link.css("a").attrib["href"],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class menabytesSpider(scrapy.Spider):
    name = "menabytesSpider"
    allowed_domains = ["menabytes.com"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

        logging.info(
            {
                "proxy": "1",
                "clean_url": self.allowed_domains[0],
                "link": self.start_urls,
            }
        )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('li.infinite-post')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www.menabytes.com" + link.css("a").attrib["href"],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class sfdaSpider(scrapy.Spider):
    name = "sfdaSpider"
    allowed_domains = ["sfda.gov.sa"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                headers={"X-Crawlera-Region": "IN"},
                errback=handle_error,
            )
            yield res
            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.m-c-title+ a')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://sfda.gov.sa" + link.css("a").attrib["href"],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class alriyadhSpider(scrapy.Spider):
    name = "alriyadhSpider"
    allowed_domains = ["alriyadh.com"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

        logging.info(
            {
                "proxy": "1",
                "clean_url": self.allowed_domains[0],
                "link": self.start_urls,
            }
        )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.entry-title a')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www.alriyadh.com" + link.css("a").attrib["href"],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class telecomreviewSpider(scrapy.Spider):
    name = "telecomreviewSpider"
    allowed_domains = ["telecomreview.com"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

        logging.info(
            {
                "proxy": "1",
                "clean_url": self.allowed_domains[0],
                "link": self.start_urls,
            }
        )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.article-title a')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www.telecomreview.com" + link.css("a").attrib["href"],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class pharmaboardroomLegalRegulatorySpider(scrapy.Spider):
    name = "pharmaboardroomLegalRegulatorySpider"
    allowed_domains = ["pharmaboardroom.com"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": self.start_urls,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('a.article-info__header')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"].strip()
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://pharmaboardroom.com" + link.css("a").attrib["href"].strip(),
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class englishAlarabiyaBusinessMarketsSpider(scrapy.Spider):
    name = "englishAlarabiyaBusinessMarketsSpider"
    allowed_domains = ["english.alarabiya.net"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('a.sectionHero_element, .content .title, a.latest_link  ')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://english.alarabiya.net" + link.css("a").attrib["href"],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class fintechgalaxySpider(scrapy.Spider):
    name = "fintechgalaxySpider"
    allowed_domains = ["fintech-galaxy.com"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

        logging.info(
            {
                "proxy": "1",
                "clean_url": self.allowed_domains[0],
                "link": self.start_urls,
            }
        )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.listingTitle a')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www.fintech-galaxy.com" + link.css("a").attrib["href"],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class gfmagSpider(scrapy.Spider):
    name = "gfmagSpider"
    allowed_domains = ["gfmag.com"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

        logging.info(
            {
                "proxy": "1",
                "clean_url": self.allowed_domains[0],
                "link": self.start_urls,
            }
        )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('h3 a')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www.gfmag.com" + link.css("a").attrib["href"],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class gpssaSpider(scrapy.Spider):
    name = "gpssaSpider"
    allowed_domains = ["gpssa.gov.ae"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

        logging.info(
            {
                "proxy": "1",
                "clean_url": self.allowed_domains[0],
                "link": self.start_urls,
            }
        )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.mediaTitle')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://gpssa.gov.ae/ar/Pages/" + link.css("a").attrib["href"],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class ncemaSpider(scrapy.Spider):
    name = "ncemaSpider"
    allowed_domains = ["ncema.gov.ae"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                headers={"X-Crawlera-Region": "IN"},
                errback=handle_error,
            )
            yield res

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.news-list-title')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www.ncema.gov.ae/" + link.css("a").attrib["href"],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class fcaSpider(scrapy.Spider):
    name = "fcaSpider"
    allowed_domains = ["fca.gov.ae"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.newsListingTitle')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://fca.gov.ae" + link.css("a").attrib["href"],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class enecSpider(scrapy.Spider):
    name = "enecSpider"
    allowed_domains = ["enec.gov.ae"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

        logging.info(
            {
                "proxy": "1",
                "clean_url": self.allowed_domains[0],
                "link": self.start_urls,
            }
        )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.w__item')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www.enec.gov.ae" + link.css("a").attrib["href"],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class difcSpider(scrapy.Spider):
    name = "difcSpider"
    allowed_domains = ["difc.ae"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

        logging.info(
            {
                "proxy": "1",
                "clean_url": self.allowed_domains[0],
                "link": self.start_urls,
            }
        )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('a.press-releases-item-container')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www.difc.ae" + link.css("a").attrib["href"],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class fahrSpider(scrapy.Spider):
    name = "fahrSpider"
    allowed_domains = ["fahr.gov.ae"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

        logging.info(
            {
                "proxy": "1",
                "clean_url": self.allowed_domains[0],
                "link": self.start_urls,
            }
        )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('li.list-entry ').css('h4')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www.fahr.gov.ae/Portal/" + link.css("a").attrib["href"],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class dubaichamberWhatsHappeningNewsSpider(scrapy.Spider):
    name = "dubaichamberWhatsHappeningNewsSpider"
    allowed_domains = ["dubaichamber.com"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('#news_highlite_text_title a')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www.dubaichamber.com" + link.css("a").attrib["href"],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class uaecabinetSpider(scrapy.Spider):
    name = "uaecabinetSpider"
    allowed_domains = ["uaecabinet.ae"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

        logging.info(
            {
                "proxy": "1",
                "clean_url": self.allowed_domains[0],
                "link": self.start_urls,
            }
        )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.read')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://uaecabinet.ae" + link.css("a").attrib["href"],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class moecSpider(scrapy.Spider):
    name = "moecSpider"
    allowed_domains = ["moec.gov.ae"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                headers={"X-Crawlera-Region": "IN"},
                errback=handle_error,
            )
            yield res
            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": self.start_urls,
                }
            )

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.btn-secondary')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www.moec.gov.ae" + link.css("a").attrib["href"],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class mofaicSpider(scrapy.Spider):
    name = "mofaicSpider"
    allowed_domains = ["mofaic.gov.ae"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

        logging.info(
            {
                "proxy": "1",
                "clean_url": self.allowed_domains[0],
                "link": self.start_urls,
            }
        )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.item-info')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www.mofaic.gov.ae" + link.css("a").attrib["href"],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class argaamSpider(scrapy.Spider):
    name = "argaamSpider"
    allowed_domains = ["argaam.com"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.res-art-temp .headline a')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www.argaam.com" + link.css("a").attrib["href"],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class tiecSpider(scrapy.Spider):
    name = "tiecSpider"
    allowed_domains = ["tiec.gov.eg"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

        logging.info(
            {
                "proxy": "1",
                "clean_url": self.allowed_domains[0],
                "link": self.start_urls,
            }
        )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.listingHeader a')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://tiec.gov.eg/English/MediaCenter/PressReleases" + str(
                                           link.css("a").attrib["href"])[2:],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class mompSpider(scrapy.Spider):
    name = "mompSpider"
    allowed_domains = ["momp.gov.eg"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                headers={"X-Crawlera-Region": "IN"},
                errback=handle_error,
            )
            yield res

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.funny-boxes a')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www.momp.gov.eg/Ar/" + link.css("a").attrib["href"],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class itidaSpider(scrapy.Spider):
    name = "itidaSpider"
    allowed_domains = ["itida.gov.eg"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.listingHeader a')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://itida.gov.eg" + link.css("a").attrib["href"],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class eipicoSpider(scrapy.Spider):
    name = "eipicoSpider"
    allowed_domains = ["eipico.com.eg"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.treat')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www.eipico.com.eg/" + link.css("a").attrib["href"],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class mossSpider(scrapy.Spider):
    name = "mossSpider"
    allowed_domains = ["moss.gov.eg"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.news-details a')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www.moss.gov.eg" + link.css("a").attrib["href"],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class mcitSpider(scrapy.Spider):
    name = "mcitSpider"
    allowed_domains = ["mcit.gov.eg"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.title')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://mcit.gov.eg" + link.css("a").attrib["href"],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class petroleumSpider(scrapy.Spider):
    name = "petroleumSpider"
    allowed_domains = ["petroleum.gov.eg"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.col-xs-7')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www.petroleum.gov.eg" + link.css("a").attrib["href"],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class businesstodayegyptSpider(scrapy.Spider):
    name = "businesstodayegyptSpider"
    allowed_domains = ["businesstodayegypt.com"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.LeftWhiteBlock a')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www.businesstodayegypt.com" + link.css("a").attrib["href"],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class almadapaperSpider(scrapy.Spider):
    name = "almadapaperSpider"
    allowed_domains = ["almadapaper.net"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.col-sm-8.last-col a, .item-post a')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://almadapaper.net/" + link.css("a").attrib["href"],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class alhayaCategory38Spider(scrapy.Spider):
    name = "alhayaCategory38Spider"
    allowed_domains = ["alhaya.ps"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.btitle1')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www.alhaya.ps" + link.css("a").attrib["href"],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class alforatnewsNewsSpider(scrapy.Spider):
    name = "alforatnewsNewsSpider"
    allowed_domains = ["alforatnews.iq"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('#pills-62eea5cdcbf07-0 .p-name a , .col-9 .p-name a')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://alforatnews.iq" + link.css("a").attrib["href"],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class alayyamSpider(scrapy.Spider):
    name = "alayyamSpider"
    allowed_domains = ["al-ayyam.ps"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.stitle')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www.al-ayyam.ps" + link.css("a").attrib["href"],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class bitetechghostSpider(scrapy.Spider):
    name = "bitetechghostSpider"
    allowed_domains = ["bitetech.ghost.io"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.h4 a')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://bitetech.ghost.io" + link.css("a").attrib["href"],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class fiveonelabsSpider(scrapy.Spider):
    name = "fiveonelabsSpider"
    allowed_domains = ["fiveonelabs.org"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

        logging.info(
            {
                "proxy": "1",
                "clean_url": self.allowed_domains[0],
                "link": self.start_urls,
            }
        )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.eventlist-title-link')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://fiveonelabs.org" + link.css("a").attrib["href"],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class alsharqiyaSpider(scrapy.Spider):
    name = "alsharqiyaSpider"
    allowed_domains = ["alsharqiya.com"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                headers={"X-Crawlera-Region": "IN"},
                errback=handle_error,
            )
            yield res

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.title-left').css('h3')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www.alsharqiya.com" + link.css("a").attrib["href"],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class hawlatiSpider(scrapy.Spider):
    name = "hawlatiSpider"
    allowed_domains = ["hawlati.co"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.text-dark')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://hawlati.co/" + link.css("a").attrib["href"],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class pecdarSpider(scrapy.Spider):
    name = "pecdarSpider"
    allowed_domains = ["pecdar.ps"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.stitle')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "http://www.pecdar.ps" + link.css("a").attrib["href"],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class itaSpider(scrapy.Spider):
    name = "itaSpider"
    allowed_domains = ["ita.gov.om"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.btn-more')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www.ita.gov.om/itaportal/MediaCenter/" + link.css("a").attrib[
                                           "href"],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class fsdmenaSpider(scrapy.Spider):
    name = "fsdmenaSpider"
    allowed_domains = ["fsd-mena.org"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.title a')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "http://fsd-mena.org" + link.css("a").attrib["href"],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class motSpider(scrapy.Spider):
    name = "motSpider"
    allowed_domains = ["mot.gov.ps"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

        logging.info(
            {
                "proxy": "1",
                "clean_url": self.allowed_domains[0],
                "link": self.start_urls,
            }
        )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.post-box-title a')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "http://www.mot.gov.ps" + link.css("a").attrib["href"],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class fintechnewsSpider(scrapy.Spider):
    name = "fintechnewsSpider"
    allowed_domains = ["fintechnews.africa"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.entry-title a')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://fintechnews.africa" + link.css("a").attrib["href"],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class tccbSpider(scrapy.Spider):
    name = "tccbSpider"
    allowed_domains = ["tccb.gov.tr"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('#divContentList a')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www.tccb.gov.tr" + link.css("a").attrib["href"],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class btkSpider(scrapy.Spider):
    name = "btkSpider"
    allowed_domains = ["btk.gov.tr"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.card__Outer-dxc4uf-0 a')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www.btk.gov.tr" + link.css("a").attrib["href"],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class qatarchronicleSpider(scrapy.Spider):
    name = "qatarchronicleSpider"
    allowed_domains = ["qatarchronicle.com"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):

        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('h2 a')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www.qatarchronicle.com" + link.css("a").attrib["href"],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class tabukpharmaceuticalsSpider(scrapy.Spider):
    name = "tabukpharmaceuticalsSpider"
    allowed_domains = ["tabukpharmaceuticals.com"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):

        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('#News a')
                for link in data:
                    for i in range(1, 13):
                        try:
                            if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                                pass
                            else:
                                if "http" in str(link.css("a").attrib["href"]):
                                    yield {"clean_url": self.allowed_domains[0],
                                           "base_url": response.url,
                                           "link": "https://www.tabukpharmaceuticals.com/en/read-more/" + str(i)
                                           }


                                else:
                                    yield {"clean_url": self.allowed_domains[0],
                                           "base_url": response.url,
                                           "link": "https://www.tabukpharmaceuticals.com/en/read-more/" + str(i),
                                           }

                        except:
                            pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class enterpriseSpider(scrapy.Spider):
    name = "enterpriseSpider"
    allowed_domains = ["enterprise.press"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):

        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.infinite-item a')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css('a').attrib['href']
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://enterprise.press" + link.css('a').attrib['href'],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class balSpider(scrapy.Spider):
    name = "balSpider"
    allowed_domains = ["bal.com"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):

        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('h4 a')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css('a').attrib['href']
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www.bal.com" + link.css('a').attrib['href'],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class ncmSpider(scrapy.Spider):
    name = "ncmSpider"
    allowed_domains = ["ncm.gov.sa"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"proxy": proxy, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('a.inner_small_news_subject')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://ncm.gov.sa" + link.css("a").attrib["href"],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class magnittSpider(scrapy.Spider):
    name = "magnittSpider"
    allowed_domains = ["magnitt.com"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"proxy": proxy, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.content-item-row-title a')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://magnitt.com" + link.css("a").attrib["href"],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class mohreSpider(scrapy.Spider):
    name = "mohreSpider"
    allowed_domains = ["mohre.gov.ae"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"proxy": proxy, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.article .title')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www.mohre.gov.ae/" + link.css("a").attrib["href"],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class modSpider(scrapy.Spider):
    name = "modSpider"
    allowed_domains = ["mod.gov.eg"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"proxy": proxy, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.read-more')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www.mod.gov.eg/ModWebSite/" + link.css("a").attrib["href"],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class sgbvSpider(scrapy.Spider):
    name = "sgbvSpider"
    allowed_domains = ["sgbv.dz"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"proxy": proxy, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('#liste .fr')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www.sgbv.dz" + str(link.css("a").attrib["href"])[1:],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1



class CitcSpider(scrapy.Spider):
    name = "CitcSpider"
    allowed_domains = ["citc.gov.sa"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res


    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.listingItemDetails.col-md-8.col-xs-12')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www.citc.gov.sa/" + str(link.css("a").attrib["href"])[1:],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1





class CommerceSpider(scrapy.Spider):
    name = "CommerceSpider"
    allowed_domains = ["commerce.gov.dz"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res


    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.entry-title')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www.commerce.gov.dz" + str(link.css("a").attrib["href"])[1:],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class DmGovSpider(scrapy.Spider):
    name = "DmGovSpider"
    allowed_domains = ["dm.gov.ae"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.title')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www.dm.gov.ae" + str(link.css("a").attrib["href"])[1:],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class HealthAlertSpider(scrapy.Spider):
    name = "HealthAlertSpider"
    allowed_domains = ["healthalert.gov.bh"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.col-lg-4.col-md-6.col-sm-6.col-xs-12')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://healthalert.gov.bh" + str(link.css("a").attrib["href"])[1:],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class InteriorSpider(scrapy.Spider):
    name = "InteriorSpider"
    allowed_domains = ["interior.gov.bh"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.openpop')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www.interior.gov.bh" + str(link.css("a").attrib["href"])[1:],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class MafSpider(scrapy.Spider):
    name = "MafSpider"
    allowed_domains = ["maf.gov.om"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.caption')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www.maf.gov.om/" + str(link.css("a").attrib["href"])[1:],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class McinetSpider(scrapy.Spider):
    name = "McinetSpider"
    allowed_domains = ["mcinet.gov.ma"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.main-post,.odd,.even')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www.mcinet.gov.ma/" + str(link.css("a").attrib["href"])[1:],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class MiepeecSpider(scrapy.Spider):
    name = "MiepeecSpider"
    allowed_domains = ["miepeec.gov.ma"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.post-title')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://miepeec.gov.ma" + str(link.css("a").attrib["href"])[1:],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class MlsdSpider(scrapy.Spider):
    name = "MlsdSpider"
    allowed_domains = ["mlsd.gov.bh"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.read-more')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www.mlsd.gov.bh" + str(link.css("a").attrib["href"]),
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class ModAeSpider(scrapy.Spider):
    name = "ModAeSpider"
    allowed_domains = ["mod.gov.ae"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.entry-title')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://mod.gov.ae" + str(link.css("a").attrib["href"]),
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class ModEgSpider(scrapy.Spider):
    name = "ModEgSpider"
    allowed_domains = ["mod.gov.eg"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.news-items')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www.mod.gov.eg/ModWebSite/" + str(link.css("a").attrib["href"]),
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1




class MofaBhSpider(scrapy.Spider):
    name = "MofaBhSpider"
    allowed_domains = ["mofa.gov.bh"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.desc')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www.mofa.gov.bh" + str(link.css("a").attrib["href"]),
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1




class MohBhSpider(scrapy.Spider):
    name = "MohBhSpider"
    allowed_domains = ["moh.gov.bh"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('#myTable td')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www.moh.gov.bh" + str(link.css("a").attrib["href"]),
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1





class MohSaSpider(scrapy.Spider):
    name = "MohSaSpider"
    allowed_domains = ["moh.gov.sa"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.news_arch li')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www.moh.gov.sa" + str(link.css("a").attrib["href"]),
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1






class MohapSpider(scrapy.Spider):
    name = "MohapSpider"
    allowed_domains = ["mohap.gov.ae"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.news-title,.news-box')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://mohap.gov.ae/" + str(link.css("a").attrib["href"]),
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1





class MoiOmSpider(scrapy.Spider):
    name = "MoiOmSpider"
    allowed_domains = ["moi.gov.om"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.categories-left')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www.moi.gov.om" + str(link.css("a").attrib["href"]),
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1




class SaudigazetteSpider(scrapy.Spider):
    name = "SaudigazetteSpider"
    allowed_domains = ["saudigazette.com.sa"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.col-1 .row-col-1 div:nth-child(2) a,.content .title-widget-1')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "https://" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"],
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://saudigazette.com.sa" + link.css("a").attrib["href"],
                                       }
                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1




class EnglishAhramSpider(scrapy.Spider):
    name = "EnglishAhramSpider"
    allowed_domains = ["ahram.org.eg"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"proxy": proxy,"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res
            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.title a, .mar-top-outer a')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "https://" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"],
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://english.ahram.org.eg" + link.css("a").attrib["href"],
                                       }
                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1





class EkonomiHaberSpider(scrapy.Spider):
    name = "EkonomiHaberSpider"
    allowed_domains = ["haber7.com"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res


    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy,"dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )


    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.clearfix a,.other-news-section a,.top-headline.row a')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "https://" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"],
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://ekonomi.haber7.com" + link.css("a").attrib["href"],
                                       }
                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class NewsabahSpider(scrapy.Spider):
    name = "NewsabahSpider"
    allowed_domains = ["newsabah.com"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip
            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.post-title a')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"],
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "http://newsabah.com" + link.css("a").attrib["href"],
                                       }
                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1




class AajSpider(scrapy.Spider):
    name = "AajSpider"
    allowed_domains = ["aaj.tv"]
    check_ip_article_links = 0
    not_allowed_keyword = []

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):

                llinks = []
                result = response.css('a.story__link')
                for r in result:
                    if (r.css("a").attrib["href"] not in llinks):
                        llinks.append(r.css("a").attrib["href"])

                        try:
                            if "http" not in r.css("a").attrib["href"]:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "" + r.css("a").attrib["href"],
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": r.css("a").attrib["href"],
                                       }
                        except:
                            pass
                del llinks
                gc.collect()


        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_article_links += 1


class AhvalnewsSpider(scrapy.Spider):
    name = "AhvalnewsSpider"
    allowed_domains = ["ahvalnews.com"]
    check_ip_article_links = 0
    not_allowed_keyword = []

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res
            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )
    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):

                llinks = []
                result = response.css('.field-title')
                for r in result:
                    if (r.css("a").attrib["href"] not in llinks):
                        llinks.append(r.css("a").attrib["href"])

                        try:
                            if "http" not in r.css("a").attrib["href"]:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://ahvalnews.com" + r.css("a").attrib["href"],
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": r.css("a").attrib["href"],
                                       }

                        except:
                            pass
                del llinks
                gc.collect()


        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_article_links += 1


# /3/
class BahrainmirrorSpider(scrapy.Spider):
    name = "BahrainmirrorSpider"
    allowed_domains = ["bahrainmirror.com"]
    check_ip_article_links = 0
    not_allowed_keyword = []

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):

                llinks = []
                result = response.css('.horz a')
                for r in result:
                    if (r.css("a").attrib["href"] not in llinks):
                        llinks.append(r.css("a").attrib["href"])
                        try:
                            if "http" not in r.css("a").attrib["href"]:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "http://bahrainmirror.com/en/" + r.css("a").attrib["href"],
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": r.css("a").attrib["href"],
                                       }
                        except:
                            pass
                del llinks
                gc.collect()


        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_article_links += 1


# /4/
class ConsultancySpider(scrapy.Spider):
    name = "ConsultancySpider"
    allowed_domains = ["consultancy-me.com"]
    check_ip_article_links = 0
    not_allowed_keyword = []

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                llinks = []
                result = response.css('.news-item-info')
                for r in result:
                    if (r.css("a").attrib["href"] not in llinks):
                        llinks.append(r.css("a").attrib["href"])
                        try:
                            if "http" not in r.css("a").attrib["href"]:

                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www.consultancy-me.com" + r.css("a").attrib["href"],
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": r.css("a").attrib["href"],
                                       }
                        except:
                            pass
                del llinks
                gc.collect()


        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_article_links += 1


# /5/
class DailynewsegyptSpider(scrapy.Spider):
    name = "DailynewsegyptSpider"
    allowed_domains = ["dailynewsegypt.com"]
    check_ip_article_links = 0
    not_allowed_keyword = []

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"proxy": proxy,"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res
            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )


    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                llinks = []
                result = response.css('.entry-title a')
                result += response.css('.media-heading .text-transparent')
                for r in result:
                    if (r.css("a").attrib["href"] not in llinks):
                        llinks.append(r.css("a").attrib["href"])
                        try:
                            if "http" not in r.css("a").attrib["href"]:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://dailynewsegypt.com" + r.css("a").attrib["href"],
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": r.css("a").attrib["href"],
                                       }

                        except:
                            pass
                del llinks
                gc.collect()


        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_article_links += 1


# /6/
class DiplomatieSpider(scrapy.Spider):
    name = "DiplomatieSpider"
    allowed_domains = ["diplomatie.ma"]
    check_ip_article_links = 0
    not_allowed_keyword = []

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                llinks = []
                result = response.css('.p-md-2 a')
                for r in result:
                    if (r.css("a").attrib["href"] not in llinks):
                        llinks.append(r.css("a").attrib["href"])
                        try:
                            if "http" not in r.css("a").attrib["href"]:

                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www.diplomatie.ma" + r.css("a").attrib["href"],
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": r.css("a").attrib["href"],
                                       }
                        except:
                            pass
                del llinks
                gc.collect()

        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_article_links += 1


class HealthcareitnewsSpider(scrapy.Spider):
    name = "HealthcareitnewsSpider"
    allowed_domains = ["healthcareitnews.com"]
    check_ip_article_links = 0
    not_allowed_keyword = []

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                llinks = []
                result = response.css('.views-field-title .field-content a')
                for r in result:
                    if (r.css("a").attrib["href"] not in llinks and "http" in r.css("a").attrib["href"] and "emea" in
                            r.css("a").attrib["href"]):
                        llinks.append(r.css("a").attrib["href"])
                        try:
                            if "http" not in r.css("a").attrib["href"]:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www.healthcareitnews.com" + r.css("a").attrib["href"],
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": r.css("a").attrib["href"],
                                       }
                        except:
                            pass
                del llinks
                gc.collect()

        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_article_links += 1


# /11/
class IraqbritainbusinessSpider(scrapy.Spider):
    name = "IraqbritainbusinessSpider"
    allowed_domains = ["iraqbritainbusiness.org"]
    check_ip_article_links = 0
    not_allowed_keyword = []

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                llinks = []
                result = response.css('h5 a')
                for r in result:
                    if (r.css("a").attrib["href"] not in llinks):
                        llinks.append(r.css("a").attrib["href"])
                        try:
                            if "http" not in r.css("a").attrib["href"]:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://iraqbritainbusiness.org" + r.css("a").attrib["href"],
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": r.css("a").attrib["href"],
                                       }
                        except:
                            pass
                del llinks
                gc.collect()

        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_article_links += 1


# /12/
class SabqSpider(scrapy.Spider):
    name = "SabqSpider"
    allowed_domains = ["sabq.org"]
    check_ip_article_links = 0
    not_allowed_keyword = []

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                llinks = []
                result = response.css('.arr--headline').css("a")
                for r in result:
                    if (r.css("a").attrib["href"] not in llinks):
                        llinks.append(r.css("a").attrib["href"])
                        try:
                            if "http" not in r.css("a").attrib["href"]:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://sabq.org" + r.css("a").attrib["href"],
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": r.css("a").attrib["href"],
                                       }
                        except:
                            pass
                del llinks
                gc.collect()

        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_article_links += 1


# /14/
class JordannewsSpider(scrapy.Spider):
    name = "JordannewsSpider"
    allowed_domains = ["jordannews.jo"]
    check_ip_article_links = 0
    not_allowed_keyword = []

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                llinks = []
                result = response.css('.line-height-3')
                for r in result:
                    if (r.css("a").attrib["href"] not in llinks):
                        llinks.append(r.css("a").attrib["href"])
                        try:
                            if "http" not in r.css("a").attrib["href"]:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www.jordannews.jo" + r.css("a").attrib["href"],
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": r.css("a").attrib["href"],
                                       }
                        except:
                            pass
                del llinks
                gc.collect()

        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_article_links += 1


# /16/
class JordantimesSpider(scrapy.Spider):
    name = "JordantimesSpider"
    allowed_domains = ["jordantimes.com"]
    check_ip_article_links = 0
    not_allowed_keyword = []

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                llinks = []
                result = response.css('#left-col .row a')
                for r in result:
                    if (r.css("a").attrib["href"] not in llinks):
                        llinks.append(r.css("a").attrib["href"])
                        try:
                            if "http" not in r.css("a").attrib["href"]:

                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www.jordantimes.com" + r.css("a").attrib["href"],
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": r.css("a").attrib["href"],
                                       }
                        except:
                            pass
                del llinks
                gc.collect()

        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_article_links += 1


# /17/
class KunaSpider(scrapy.Spider):
    name = "KunaSpider"
    allowed_domains = ["kuna.net.kw"]
    check_ip_article_links = 0
    not_allowed_keyword = []

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                llinks = []
                result = response.css('.col-sm-12.category-news a')
                for r in result:
                    if (r.css("a").attrib["href"] not in llinks):
                        llinks.append(r.css("a").attrib["href"])
                        try:
                            if "http" not in r.css("a").attrib["href"]:

                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www.kuna.net.kw/" + r.css("a").attrib["href"],
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": r.css("a").attrib["href"],
                                       }
                        except:
                            pass
                del llinks
                gc.collect()

        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_article_links += 1


# /18/
class MtcitSpider(scrapy.Spider):
    name = "MtcitSpider"
    allowed_domains = ["mtcit.gov.om"]
    check_ip_article_links = 0
    not_allowed_keyword = []

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                llinks = []
                result = response.css('h3 a')
                for r in result:
                    if (r.css("a").attrib["href"] not in llinks):
                        llinks.append(r.css("a").attrib["href"])
                        try:
                            if "http" not in r.css("a").attrib["href"]:

                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www.mtcit.gov.om/ITAPortal/MediaCenter/" + r.css("a").attrib[
                                           "href"],
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": r.css("a").attrib["href"],
                                       }
                        except:
                            pass
                del llinks
                gc.collect()

        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_article_links += 1


# /19/
class MttSpider(scrapy.Spider):
    name = "MttSpider"
    allowed_domains = ["mtt.gov.bh"]
    check_ip_article_links = 0
    not_allowed_keyword = []

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                llinks = []
                result = response.css('.read-more')
                for r in result:
                    if (r.css("a").attrib["href"] not in llinks):
                        llinks.append(r.css("a").attrib["href"])
                        try:
                            if "http" not in r.css("a").attrib["href"]:

                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://mtt.gov.bh" + r.css("a").attrib["href"],
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": r.css("a").attrib["href"],
                                       }
                        except:
                            pass
                del llinks
                gc.collect()

        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_article_links += 1


# /20/
class SozcuSpider(scrapy.Spider):
    name = "SozcuSpider"
    allowed_domains = ["sozcu.com.tr"]
    check_ip_article_links = 0
    not_allowed_keyword = []

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                llinks = []
                result = response.css('.news-item-title')
                for r in result:
                    if (r.css("a").attrib["href"] not in llinks):
                        llinks.append(r.css("a").attrib["href"])
                        try:
                            if "http" not in r.css("a").attrib["href"]:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": r.css("a").attrib["href"],
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": r.css("a").attrib["href"],
                                       }
                        except:
                            pass
                del llinks
                gc.collect()

        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_article_links += 1


# /21/
class StartuppakistanSpider(scrapy.Spider):
    name = "StartuppakistanSpider"
    allowed_domains = ["startuppakistan.com.pk"]
    check_ip_article_links = 0
    not_allowed_keyword = []

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                llinks = []
                result = response.css('.td-module-title a')
                for r in result:
                    if (r.css("a").attrib["href"] not in llinks):
                        llinks.append(r.css("a").attrib["href"])
                        try:
                            if "http" not in r.css("a").attrib["href"]:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": r.css("a").attrib["href"],
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": r.css("a").attrib["href"],
                                       }
                        except:
                            pass
                del llinks
                gc.collect()

        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_article_links += 1


# /22/
class TdraSpider(scrapy.Spider):
    name = "TdraSpider"
    allowed_domains = ["tdra.gov.ae"]
    check_ip_article_links = 0
    not_allowed_keyword = []

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                llinks = []
                result = response.css(".hide-2lines")
                for r in result:
                    if (r.css("a").attrib["href"] not in llinks):
                        llinks.append(r.css("a").attrib["href"])
                        try:
                            if "http" not in r.css("a").attrib["href"]:

                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://tdra.gov.ae" + r.css("a").attrib["href"],
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": r.css("a").attrib["href"],
                                       }
                        except:
                            pass
                del llinks
                gc.collect()

        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_article_links += 1


# /23/
class TechjuiceSpider(scrapy.Spider):
    name = "TechjuiceSpider"
    allowed_domains = ["techjuice.pk"]
    check_ip_article_links = 0
    not_allowed_keyword = []

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": self.start_urls,
                }
            )
            yield res_ip

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                llinks = []
                result = response.css(".text-dark")
                for r in result:
                    if (r.css("a").attrib["href"] not in llinks):
                        llinks.append(r.css("a").attrib["href"])
                        try:
                            if "http" not in r.css("a").attrib["href"]:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www.techjuice.pk/" + r.css("a").attrib["href"]
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": r.css("a").attrib["href"]
                                       }
                        except:
                            pass
                del llinks
                gc.collect()

            else:
                while self.check_ip_article_links < 2:
                    yield response.follow(self.start_urls, callback=self.start_requests_ip)
                    self.check_ip_article_links += 1


# /24/
class TelcotitansSpider(scrapy.Spider):
    name = "TelcotitansSpider"
    allowed_domains = ["telcotitans.com"]
    check_ip_article_links = 0
    not_allowed_keyword = []

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                llinks = []
                result = response.css("h2 a")
                for r in result:
                    if (r.css("a").attrib["href"] not in llinks):
                        llinks.append(r.css("a").attrib["href"])
                        try:
                            if "http" not in r.css("a").attrib["href"]:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": r.css("a").attrib["href"],
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": r.css("a").attrib["href"],
                                       }
                        except:
                            pass
                del llinks
                gc.collect()

        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_article_links += 1


# /25/
class ThebankerSpider(scrapy.Spider):
    name = "ThebankerSpider"
    allowed_domains = ["thebanker.com"]
    check_ip_article_links = 0
    not_allowed_keyword = []

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                llinks = []
                result = response.css("h4 a")
                for r in result:
                    if (r.css("a").attrib["href"] not in llinks):
                        llinks.append(r.css("a").attrib["href"])
                        try:
                            if "http" not in r.css("a").attrib["href"]:

                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www.thebanker.com" + r.css("a").attrib["href"],
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": r.css("a").attrib["href"],
                                       }
                        except:
                            pass
                del llinks
                gc.collect()

        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_article_links += 1


# /26/
class SeddSpider(scrapy.Spider):
    name = "SeddSpider"
    allowed_domains = ["sedd.ae"]
    check_ip_article_links = 0
    not_allowed_keyword = []

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                llinks = []
                result = response.css(".news-block").css('a')
                for r in result:
                    if (r.css("a").attrib["href"] not in llinks):
                        llinks.append(r.css("a").attrib["href"])
                        try:
                            if "http" not in r.css("a").attrib["href"]:

                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://sedd.ae/web/sedd/" + r.css("a").attrib["href"],
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": r.css("a").attrib["href"],
                                       }
                        except:
                            pass
                del llinks
                gc.collect()

        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_article_links += 1


# /27/


# /28/
class EntrepreneuralarabiyaSpider(scrapy.Spider):
    name = "EntrepreneuralarabiyaSpider"
    allowed_domains = ["entrepreneuralarabiya.com"]
    check_ip_article_links = 0
    not_allowed_keyword = []

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                llinks = []
                result = response.css("h3 a")
                for r in result:
                    if (r.css("a").attrib["href"] not in llinks):
                        llinks.append(r.css("a").attrib["href"])
                        try:
                            if "http" not in r.css("a").attrib["href"]:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": r.css("a").attrib["href"],
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": r.css("a").attrib["href"],
                                       }
                        except:
                            pass
                del llinks
                gc.collect()

        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_article_links += 1


# /29/
class EgyptianstreetsSpider(scrapy.Spider):
    name = "EgyptianstreetsSpider"
    allowed_domains = ["egyptianstreets.com"]
    check_ip_article_links = 0
    not_allowed_keyword = []

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                llinks = []
                result = response.css(".widget-full-list-text").css("a")
                for r in result:
                    if (r.css("a").attrib["href"] not in llinks):
                        llinks.append(r.css("a").attrib["href"])
                        try:
                            if "http" not in r.css("a").attrib["href"]:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": r.css("a").attrib["href"],
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": r.css("a").attrib["href"],
                                       }
                        except:
                            pass
                del llinks
                gc.collect()

        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_article_links += 1


# /30/
class SeaSpider(scrapy.Spider):
    name = "SeaSpider"
    allowed_domains = ["sea.gov.bh"]
    check_ip_article_links = 0
    not_allowed_keyword = []

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                llinks = []
                result = response.css(".fusion-read-more")
                for r in result:
                    if (r.css("a").attrib["href"] not in llinks):
                        llinks.append(r.css("a").attrib["href"])
                        try:
                            if "http" not in r.css("a").attrib["href"]:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": r.css("a").attrib["href"],
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": r.css("a").attrib["href"],
                                       }
                        except:
                            pass
                del llinks
                gc.collect()

        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_article_links += 1


# /31/
class Fintechnews2Spider(scrapy.Spider):
    name = "Fintechnews2Spider"
    allowed_domains = ["fintechnews.ae"]
    check_ip_article_links = 0
    not_allowed_keyword = []

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": self.start_urls,
                }
            )
            yield res_ip

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                llinks = []
                result = response.css(".entry-title a")
                for r in result:
                    if (r.css("a").attrib["href"] not in llinks):
                        llinks.append(r.css("a").attrib["href"])
                        try:
                            if "http" not in r.css("a").attrib["href"]:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "" + r.css("a").attrib["href"]
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": r.css("a").attrib["href"]
                                       }
                        except:
                            pass
                del llinks
                gc.collect()

            else:
                while self.check_ip_article_links < 2:
                    yield response.follow(self.start_urls, callback=self.start_requests_ip)
                    self.check_ip_article_links += 1


# /33/
class FinancialafrikSpider(scrapy.Spider):
    name = "FinancialafrikSpider"
    allowed_domains = ["financialafrik.com"]
    check_ip_article_links = 0
    not_allowed_keyword = []

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": self.start_urls,
                }
            )
            yield res_ip

    def parse(self, response, llinks=[]):
        if str(response.status) == "200":
            if response.css("body"):
                llinks = []
                result = response.css(".post-title a")
                for r in result:
                    if (r.css("a").attrib["href"] not in llinks):
                        llinks.append(r.css("a").attrib["href"])
                        try:
                            if "http" not in r.css("a").attrib["href"]:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "" + r.css("a").attrib["href"]
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": r.css("a").attrib["href"]
                                       }
                        except:
                            pass
                del llinks
                gc.collect()

            else:
                while self.check_ip_article_links < 2:
                    yield response.follow(self.start_urls, callback=self.start_requests_ip)
                    self.check_ip_article_links += 1


# /34/
class TimesofomanSpider(scrapy.Spider):
    name = "TimesofomanSpider"
    allowed_domains = ["timesofoman.com"]
    check_ip_article_links = 0
    not_allowed_keyword = []

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": self.start_urls,
                }
            )
            yield res_ip

    def parse(self, response, llinks=[]):
        if str(response.status) == "200":
            if response.css("body"):
                llinks = []
                result = response.css(".col-lg-8 .post-title a")
                for r in result:
                    if (r.css("a").attrib["href"] not in llinks):
                        llinks.append(r.css("a").attrib["href"])
                        try:
                            if "http" not in r.css("a").attrib["href"]:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://timesofoman.com" + r.css("a").attrib["href"]
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": r.css("a").attrib["href"]
                                       }
                        except:
                            pass
                del llinks
                gc.collect()

            else:
                while self.check_ip_article_links < 2:
                    yield response.follow(self.start_urls, callback=self.start_requests_ip)
                    self.check_ip_article_links += 1


# /35/
class TheuaenewsSpider(scrapy.Spider):
    name = "TheuaenewsSpider"
    allowed_domains = ["theuaenews.com"]
    check_ip_article_links = 0
    not_allowed_keyword = []

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": self.start_urls,
                }
            )
            yield res_ip

    def parse(self, response, llinks=[]):
        if str(response.status) == "200":
            if response.css("body"):
                llinks = []
                result = response.css("h3 a")
                for r in result:
                    if (r.css("a").attrib["href"] not in llinks):
                        llinks.append(r.css("a").attrib["href"])
                        try:
                            if "http" not in r.css("a").attrib["href"]:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "" + r.css("a").attrib["href"]
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": r.css("a").attrib["href"]
                                       }
                        except:
                            pass
                del llinks
                gc.collect()


            else:
                while self.check_ip_article_links < 2:
                    yield response.follow(self.start_urls, callback=self.start_requests_ip)
                    self.check_ip_article_links += 1


# /36/


# /37/
class TheconversationSpider(scrapy.Spider):
    name = "TheconversationSpider"
    allowed_domains = ["theconversation.com"]
    check_ip_article_links = 0
    not_allowed_keyword = []

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                llinks = []
                result = response.css("h2 a")
                for r in result:
                    if (r.css("a").attrib["href"] not in llinks):
                        llinks.append(r.css("a").attrib["href"])
                        try:
                            if "http" not in r.css("a").attrib["href"]:

                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://theconversation.com" + r.css("a").attrib["href"],
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": r.css("a").attrib["href"],
                                       }
                        except:
                            pass
                del llinks
                gc.collect()


        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_article_links += 1


# /38/
class ThefintechtimesSpider(scrapy.Spider):
    name = "ThefintechtimesSpider"
    allowed_domains = ["thefintechtimes.com"]
    check_ip_article_links = 0
    not_allowed_keyword = []

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": self.start_urls,
                }
            )
            yield res_ip

    def parse(self, response, llinks=[]):
        if str(response.status) == "200":
            if response.css("body"):
                llinks = []
                result = response.css(".entry-title a")
                for r in result:
                    if (r.css("a").attrib["href"] not in llinks):
                        llinks.append(r.css("a").attrib["href"])
                        try:
                            if "http" not in r.css("a").attrib["href"]:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://thefintechtimes.com" + r.css("a").attrib["href"]
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": r.css("a").attrib["href"]
                                       }
                        except:
                            pass
                del llinks
                gc.collect()


            else:
                while self.check_ip_article_links < 2:
                    yield response.follow(self.start_urls, callback=self.start_requests_ip)
                    self.check_ip_article_links += 1


# /40/
class DuvarenglishSpider(scrapy.Spider):
    name = "DuvarenglishSpider"
    allowed_domains = ["duvarenglish.com"]
    check_ip_article_links = 0
    not_allowed_keyword = []

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": self.start_urls,
                }
            )
            yield res_ip

    def parse(self, response, llinks=[]):
        if str(response.status) == "200":
            if response.css("body"):
                llinks = []
                result = response.css("a.box")
                for r in result:
                    if (r.css("a").attrib["href"] not in llinks):
                        llinks.append(r.css("a").attrib["href"])
                        try:
                            if "http" not in r.css("a").attrib["href"]:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "" + r.css("a").attrib["href"]
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": r.css("a").attrib["href"]
                                       }
                        except:
                            pass
                del llinks
                gc.collect()


            else:
                while self.check_ip_article_links < 2:
                    yield response.follow(self.start_urls, callback=self.start_requests_ip)
                    self.check_ip_article_links += 1


# /41/
class AlmonitorSpider(scrapy.Spider):
    name = "AlmonitorSpider"
    allowed_domains = ["al-monitor.com"]
    check_ip_article_links = 0
    not_allowed_keyword = []

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": self.start_urls,
                }
            )
            yield res_ip

    def parse(self, response, llinks=[]):
        if str(response.status) == "200":
            if response.css("body"):
                llinks = []
                result = response.css(".b-inner")
                for r in result:
                    if (r.css("a").attrib["href"] not in llinks):
                        llinks.append(r.css("a").attrib["href"])
                        try:
                            if "http" not in r.css("a").attrib["href"]:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www.al-monitor.com" + r.css("a").attrib["href"]
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": r.css("a").attrib["href"]
                                       }
                        except:
                            pass
                del llinks
                gc.collect()


            else:
                while self.check_ip_article_links < 2:
                    yield response.follow(self.start_urls, callback=self.start_requests_ip)
                    self.check_ip_article_links += 1


# /42/
class AlborsaanewsSpider(scrapy.Spider):
    name = "AlborsaanewsSpider"
    allowed_domains = ["alborsaanews.com"]
    check_ip_article_links = 0
    not_allowed_keyword = []

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": self.start_urls,
                }
            )
            yield res_ip

    def parse(self, response, llinks=[]):
        if str(response.status) == "200":
            if response.css("body"):
                llinks = []
                result = response.css(".entry-title a , .text-shadow-black")
                for r in result:
                    if (r.css("a").attrib["href"] not in llinks):
                        llinks.append(r.css("a").attrib["href"])
                        try:
                            if "http" not in r.css("a").attrib["href"]:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "" + r.css("a").attrib["href"]
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": r.css("a").attrib["href"]
                                       }
                        except:
                            pass
                del llinks
                gc.collect()


            else:
                while self.check_ip_article_links < 2:
                    yield response.follow(self.start_urls, callback=self.start_requests_ip)
                    self.check_ip_article_links += 1


# /43/
class AiSpider(scrapy.Spider):
    name = "AiSpider"
    allowed_domains = ["ai.gov.ae"]
    check_ip_article_links = 0
    not_allowed_keyword = []

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                llinks = []
                result = response.css(".x-text").css("a")
                for r in result:
                    if (r.css("a").attrib["href"] not in llinks and "details" in r.css("a").attrib["href"]):
                        llinks.append(r.css("a").attrib["href"])
                        try:
                            if "http" not in r.css("a").attrib["href"]:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": r.css("a").attrib["href"],
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": r.css("a").attrib["href"],
                                       }
                        except:
                            pass
                del llinks
                gc.collect()


        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_article_links += 1


# /44/
class DawnSpider(scrapy.Spider):
    name = "DawnSpider"
    allowed_domains = ["dawn.com"]
    check_ip_article_links = 0
    not_allowed_keyword = []

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": self.start_urls,
                }
            )
            yield res_ip

    def parse(self, response, llinks=[]):
        if str(response.status) == "200":
            if response.css("body"):
                llinks = []
                result = response.css(".story__link")
                for r in result:
                    if (r.css("a").attrib["href"] not in llinks):
                        llinks.append(r.css("a").attrib["href"])
                        try:
                            if "http" not in r.css("a").attrib["href"]:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "" + r.css("a").attrib["href"]
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": r.css("a").attrib["href"]
                                       }
                        except:
                            pass
                del llinks
                gc.collect()


            else:
                while self.check_ip_article_links < 2:
                    yield response.follow(self.start_urls, callback=self.start_requests_ip)
                    self.check_ip_article_links += 1


# /45/


# /46/
class DohanewsSpider(scrapy.Spider):
    name = "DohanewsSpider"
    allowed_domains = ["dohanews.co"]
    check_ip_article_links = 0
    not_allowed_keyword = []

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": self.start_urls,
                }
            )
            yield res_ip

    def parse(self, response, llinks=[]):
        if str(response.status) == "200":
            if response.css("body"):
                llinks = []
                result = response.css(".entry-title a")
                for r in result:
                    if (r.css("a").attrib["href"] not in llinks):
                        llinks.append(r.css("a").attrib["href"])
                        try:
                            if "http" not in r.css("a").attrib["href"]:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "" + r.css("a").attrib["href"]
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": r.css("a").attrib["href"]
                                       }
                        except:
                            pass
                del llinks
                gc.collect()


            else:
                while self.check_ip_article_links < 2:
                    yield response.follow(self.start_urls, callback=self.start_requests_ip)
                    self.check_ip_article_links += 1


# /47/
class GulftimesSpider(scrapy.Spider):
    name = "GulftimesSpider"
    allowed_domains = ["gulf-times.com"]
    check_ip_article_links = 0
    not_allowed_keyword = []

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": self.start_urls,
                }
            )
            yield res_ip

    def parse(self, response, llinks=[]):
        if str(response.status) == "200":
            if response.css("body"):
                llinks = []
                result = response.css(".businessBrief").css("a")
                for r in result:
                    if (r.css("a").attrib["href"] not in llinks):
                        llinks.append(r.css("a").attrib["href"])
                        try:
                            if "http" not in r.css("a").attrib["href"]:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www.gulf-times.com" + r.css("a").attrib["href"]
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": r.css("a").attrib["href"]
                                       }
                        except:
                            pass
                del llinks
                gc.collect()


            else:
                while self.check_ip_article_links < 2:
                    yield response.follow(self.start_urls, callback=self.start_requests_ip)
                    self.check_ip_article_links += 1


# /48/
class Gulfnews2Spider(scrapy.Spider):
    name = "Gulfnews2Spider"
    allowed_domains = ["gulfnews.com"]
    check_ip_article_links = 0
    not_allowed_keyword = []

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": self.start_urls,
                }
            )
            yield res_ip

    def parse(self, response, llinks=[]):
        if str(response.status) == "200":
            if response.css("body"):
                llinks = []
                result = response.css(".card-title a")
                for r in result:
                    if (r.css("a").attrib["href"] not in llinks):
                        llinks.append(r.css("a").attrib["href"])
                        try:
                            if "http" not in r.css("a").attrib["href"]:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://gulfnews.com" + r.css("a").attrib["href"]
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": r.css("a").attrib["href"]
                                       }
                        except:
                            pass
                del llinks
                gc.collect()


            else:
                while self.check_ip_article_links < 2:
                    yield response.follow(self.start_urls, callback=self.start_requests_ip)
                    self.check_ip_article_links += 1


# /49/
class GulfbusinessSpider(scrapy.Spider):
    name = "GulfbusinessSpider"
    allowed_domains = ["gulfbusiness.com"]
    check_ip_article_links = 0
    not_allowed_keyword = []

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": self.start_urls,
                }
            )
            yield res_ip

    def parse(self, response, llinks=[]):
        if str(response.status) == "200":
            if response.css("body"):
                llinks = []
                result = response.css("h4 a")
                for r in result:
                    if (r.css("a").attrib["href"] not in llinks):
                        llinks.append(r.css("a").attrib["href"])
                        try:
                            if "http" not in r.css("a").attrib["href"]:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "" + r.css("a").attrib["href"]
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": r.css("a").attrib["href"]
                                       }
                        except:
                            pass
                del llinks
                gc.collect()


            else:
                while self.check_ip_article_links < 2:
                    yield response.follow(self.start_urls, callback=self.start_requests_ip)
                    self.check_ip_article_links += 1


# /50/
class GulfinsiderSpider(scrapy.Spider):
    name = "GulfinsiderSpider"
    allowed_domains = ["gulf-insider.com"]
    check_ip_article_links = 0
    not_allowed_keyword = []

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                llinks = []
                result = response.css(".post-title a")
                for r in result:
                    if (r.css("a").attrib["href"] not in llinks):
                        llinks.append(r.css("a").attrib["href"])
                        try:
                            if "http" not in r.css("a").attrib["href"]:

                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www.gulf-insider.com" + r.css("a").attrib["href"],
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": r.css("a").attrib["href"],
                                       }
                        except:
                            pass
                del llinks
                gc.collect()


        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_article_links += 1


# /51/
class KuwaittimesSpider(scrapy.Spider):
    name = "KuwaittimesSpider"
    allowed_domains = ["kuwaittimes.com"]
    check_ip_article_links = 0
    not_allowed_keyword = []

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": self.start_urls,
                }
            )
            yield res_ip

    def parse(self, response, llinks=[]):
        if str(response.status) == "200":
            if response.css("body"):
                llinks = []
                result = response.css(".post-title a")
                for r in result:
                    if (r.css("a").attrib["href"] not in llinks):
                        llinks.append(r.css("a").attrib["href"])
                        try:
                            if "http" not in r.css("a").attrib["href"]:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "" + r.css("a").attrib["href"]
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": r.css("a").attrib["href"]
                                       }
                        except:
                            pass
                del llinks
                gc.collect()


            else:
                while self.check_ip_article_links < 2:
                    yield response.follow(self.start_urls, callback=self.start_requests_ip)
                    self.check_ip_article_links += 1


# /52/
class KhaleejtimesSpider(scrapy.Spider):
    name = "KhaleejtimesSpider"
    allowed_domains = ["khaleejtimes.com"]
    not_allowed_keyword = ["weather"]
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": self.start_urls,
                }
            )
            yield res_ip

    def parse(self, response, llinks=[]):
        if str(response.status) == "200":
            if response.css("body"):
                llinks = []
                result = response.css(".post-title a")
                for r in result:
                    if any(n in str(r.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                        pass
                    else:
                        if (r.css("a").attrib["href"] not in llinks):
                            llinks.append(r.css("a").attrib["href"])
                            try:
                                if "http" not in r.css("a").attrib["href"]:
                                    yield {"clean_url": self.allowed_domains[0],
                                           "base_url": response.url,
                                           "link": "" + r.css("a").attrib["href"]
                                           }
                                else:
                                    yield {"clean_url": self.allowed_domains[0],
                                           "base_url": response.url,
                                           "link": r.css("a").attrib["href"]
                                           }
                            except:
                                pass
                del llinks
                gc.collect()


            else:
                while self.check_ip_article_links < 2:
                    yield response.follow(self.start_urls, callback=self.start_requests_ip)
                    self.check_ip_article_links += 1


# /53/
class DhaSpider(scrapy.Spider):
    name = "DhaSpider"
    allowed_domains = ["dha.com.tr"]
    check_ip_article_links = 0
    not_allowed_keyword = []

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):

                llinks = []
                result = response.css(".category-card")
                result += response.css(".cat-list-card__link")
                for r in result:
                    if (r.css("a").attrib["href"] not in llinks):
                        llinks.append(r.css("a").attrib["href"])
                        try:
                            if "http" not in r.css("a").attrib["href"]:

                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www.dha.com.tr" + r.css("a").attrib["href"],
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": r.css("a").attrib["href"],
                                       }
                        except:
                            pass
                del llinks
                gc.collect()


        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_article_links += 1


# /54/
class MofSpider(scrapy.Spider):
    name = "MofSpider"
    allowed_domains = ["mof.gov.ae"]
    check_ip_article_links = 0
    not_allowed_keyword = ["twitter", "#"]

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": self.start_urls,
                }
            )
            yield res_ip

    def parse(self, response, llinks=[]):
        if str(response.status) == "200":
            if response.css("body"):
                llinks = []
                result = response.css(".read-more-link")
                result += response.css(".elementor-button-link")
                for r in result:
                    if any(n in str(r.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                        pass
                    else:
                        if (r.css("a").attrib["href"] not in llinks):
                            llinks.append(r.css("a").attrib["href"])
                            try:
                                if "http" not in r.css("a").attrib["href"]:
                                    yield {"clean_url": self.allowed_domains[0],
                                           "base_url": response.url,
                                           "link": "" + r.css("a").attrib["href"]
                                           }
                                else:
                                    yield {"clean_url": self.allowed_domains[0],
                                           "base_url": response.url,
                                           "link": r.css("a").attrib["href"]
                                           }
                            except:
                                pass
                del llinks
                gc.collect()

            else:
                while self.check_ip_article_links < 2:
                    yield response.follow(self.start_urls, callback=self.start_requests_ip)
                    self.check_ip_article_links += 1


# /55/
class AaTrSpider(scrapy.Spider):
    name = "AaTrSpider"
    allowed_domains = ["aa.com.tr"]
    check_ip_article_links = 0
    not_allowed_keyword = []

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                llinks = []
                result = response.css(".col-sm-12").css("a")
                for r in result:
                    if (r.css("a").attrib["href"] not in llinks):
                        llinks.append(r.css("a").attrib["href"])

                        try:
                            if "http" not in r.css("a").attrib["href"]:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": r.css("a").attrib["href"],
                                       }

                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": r.css("a").attrib["href"],
                                       }

                        except:
                            pass

                del llinks
                gc.collect()



        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_article_links += 1


# /56/
class AaSpider(scrapy.Spider):
    name = "AaSpider"
    allowed_domains = ["aa.com.tr"]
    check_ip_article_links = 0
    not_allowed_keyword = ["default", "facebook", "youtube", "twitter", "instagram", "apple", "microsoft", "play"]

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": self.start_urls,
                }
            )
            yield res_ip

    def parse(self, response, llinks=[]):
        if str(response.status) == "200":
            if response.css("body"):
                llinks = []
                result = response.css(".col-sm-12").css("a")
                result += response.css("h3 a")
                for r in result:
                    if any(n in str(r.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                        pass
                    else:
                        if (r.css("a").attrib["href"] not in llinks):
                            llinks.append(r.css("a").attrib["href"])
                            try:

                                if "http" not in r.css("a").attrib["href"]:
                                    yield {"clean_url": self.allowed_domains[0],
                                           "base_url": response.url,
                                           "link": "" + r.css("a").attrib["href"]
                                           }
                                else:
                                    yield {"clean_url": self.allowed_domains[0],
                                           "base_url": response.url,
                                           "link": r.css("a").attrib["href"]
                                           }
                            except:
                                pass
                del llinks
                gc.collect()

            else:
                while self.check_ip_article_links < 2:
                    yield response.follow(self.start_urls, callback=self.start_requests_ip)
                    self.check_ip_article_links += 1


# /57/
class DhccSpider(scrapy.Spider):
    name = "DhccSpider"
    allowed_domains = ["dhcc.ae"]
    check_ip_article_links = 0
    not_allowed_keyword = []

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):

                llinks = []
                result = response.css(".card").css("a")
                for r in result:
                    if (r.css("a").attrib["href"] not in llinks):
                        llinks.append(r.css("a").attrib["href"])

                        try:
                            if "http" not in r.css("a").attrib["href"]:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": r.css("a").attrib["href"],
                                       }

                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": r.css("a").attrib["href"],
                                       }

                        except:
                            pass
                del llinks
                gc.collect()



        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_article_links += 1



class IamMaSpider(scrapy.Spider):
    name = "IamMaSpider"
    allowed_domains = ["iam.ma"]
    not_allowed_keyword = []
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"proxy": proxy,"dont_retry": True, "download_timeout": timeout},
                headers={"X-Crawlera-Region": "IN"},
                errback=handle_error,
            )
            yield res
            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                try:
                    data = response.css('.pressePdf-fiche')
                except:
                    data = []
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('.pressePdf-intro::text').get().strip() if link.css('.pressePdf-intro::text') else 'na',
                                       "excerpt": 'na',
                                       "published_date": link.css('.pressePdf-date::text').get() if link.css('.pressePdf-date::text').get() else 'na',
                                       "link": link.css("a").attrib["href"].replace(' ','%20'),
                                      }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('.pressePdf-intro::text').get().strip() if link.css('.pressePdf-intro::text') else 'na',
                                       "excerpt": 'na',
                                       "published_date": link.css('.pressePdf-date::text').get() if link.css('.pressePdf-date::text').get() else 'na',
                                       "link": "https://www.iam.ma" + link.css("a").attrib["href"].replace(' ','%20'),
                                      }
                    except:
                        pass
        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1



class MoroccoNotesCirculairesSpider(scrapy.Spider):
    name = "MoroccoNotesCirculairesSpider"
    allowed_domains = ["portail.tax.gov.ma"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                try:
                    data = response.css('.decrets')

                except:
                    data = []
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('.description::text').get().strip(),
                                       "excerpt": 'na',
                                       "published_date": link.css('.date::text').get() if link.css('.date::text') else 'na',
                                       "link": link.css('a').attrib['href'].replace('..', ''),
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('.description::text').get().strip(),
                                       "excerpt": 'na',
                                       "published_date": link.css('.date::text').get() if link.css('.date::text') else 'na',
                                       "link": "https://portail.tax.gov.ma" + link.css('a').attrib['href'].replace(
                                           '..', ''),
                                        }

                    except:
                        pass
        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1




class ProPakistaniSpider(scrapy.Spider):
    name = "ProPakistaniSpider"
    allowed_domains = ["propakistani.pk"]
    not_allowed_keyword = ['#videoModal']
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"proxy": proxy,"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res
            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.entry-title a')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "https://" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]}
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://propakistani.pk" + link.css("a").attrib["href"].replace('..', '')}

                    except:
                        pass


        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1




class SceGovCirculairesSpider(scrapy.Spider):
    name = "SceGovCirculairesSpider"
    allowed_domains = ["sce.gov.bh"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0
    start_urls = ['http://www.sce.gov.bh/en/Overview?cms=iQRpheuphYtJ6pyXUGiNqj0IIkIp0ipd']

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                try:
                    data = response.css('.report-row-wrap')

                except:
                    data = []
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('a::text').get().strip() if link.css('a::text') else 'na',
                                       "excerpt": 'na',
                                       "published_date": link.css('.report-row1::text').get() if link.css('.report-row1::text') else 'na',
                                       "pdf_url": link.css('a').attrib['href'].replace('..', '').replace(' ',''),
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('a::text').get().strip() if link.css('a::text') else 'na',
                                       "excerpt": 'na',
                                       "published_date": link.css('.report-row1::text').get() if link.css('.report-row1::text') else 'na',
                                       "pdf_url": "http://www.sce.gov.bh/en/" + link.css('a').attrib['href'].replace(
                                           '..', '').replace(' ',''),
                                        }

                    except:
                        pass
        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1




class PalestinecabinetSpider(scrapy.Spider):
    name = "PalestinecabinetSpider"
    allowed_domains = ["palestinecabinet.gov.ps"]
    check_ip_article_links = 0
    not_allowed_keyword = []

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                llinks = []
                result = response.css(".table-hover td")
                for r in result:
                        try:
                            if "http" not in r.css("a").attrib["href"]:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": r.css('a::text').get() if r.css('a::text') else 'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": "http://www.palestinecabinet.gov.ps" + r.css("a").attrib["href"],
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": r.css('a::text').get() if r.css('a::text') else 'na',
                                       "excerpt": 'na',
                                        "published_date": 'na',
                                       "pdf_url": r.css("a").attrib["href"],
                                       }
                        except:
                            pass

        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_article_links += 1



class AseSpider(scrapy.Spider):
    name = "AseSpider"
    allowed_domains = ["ase.com.jo"]
    not_allowed_keyword = []
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                try:
                    data = response.css('tbody tr')
                except:
                    data = []
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('.views-field.views-field-document-name a::text').get().strip() if link.css('.views-field.views-field-document-name a::text') else 'na',
                                       "excerpt": 'na',
                                       "published_date": link.css('.views-field.views-field-published::text').get().strip() if link.css('.views-field.views-field-published::text') else 'na',
                                       "link": link.css("td a").attrib["href"].replace(' ','%20'),
                                      }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('.views-field.views-field-document-name a::text').get().strip() if link.css('.views-field.views-field-document-name a::text') else 'na',
                                       "excerpt": 'na',
                                       "published_date": link.css('.views-field.views-field-published::text').get().strip() if link.css('.views-field.views-field-published::text') else 'na',
                                       "link": "https://www.ase.com.jo" + link.css("td a").attrib["href"].replace(' ','%20'),
                                      }
                    except:
                        pass
        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1


class ClydecoSpider(scrapy.Spider):
    name = "ClydecoSpider"
    allowed_domains = ["clydeco.com"]
    check_ip_article_links = 0
    not_allowed_keyword = []

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                llinks = []
                result = response.css(".card__cta")
                for r in result:
                    if (r.css("a").attrib["href"] not in llinks) and ("passle-post-url" not in r.css('a').attrib['href']):
                        llinks.append(r.css("a").attrib["href"])
                        try:
                            if "http" not in r.css("a").attrib["href"]:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "" + r.css("a").attrib["href"],
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": r.css("a").attrib["href"],
                                       }
                        except:
                            pass

        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_article_links += 1



class MealcSpider(scrapy.Spider):
    name = "MealcSpider"
    allowed_domains = ["mealc.org"]
    check_ip_article_links = 0
    not_allowed_keyword = []

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                llinks = []
                result = response.css(".O16KGI")
                for r in result:
                    if (r.css("a").attrib["href"] not in llinks):
                        llinks.append(r.css("a").attrib["href"])
                        try:
                            if "http" not in r.css("a").attrib["href"]:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "" + r.css("a").attrib["href"],
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": r.css("a").attrib["href"],
                                       }
                        except:
                            pass

        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_article_links += 1


class CoursupremeSpider(scrapy.Spider):
    name = "CoursupremeSpider"
    allowed_domains = ["coursupreme.dz"]
    not_allowed_keyword = []
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                headers={"X-Crawlera-Region": "FR"},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                llinks = []
                result = response.css("span.field-content").css("a")
                for r in result:
                    if (r.css("a").attrib["href"] not in llinks):
                        llinks.append(r.css("a").attrib["href"])
                        try:
                            if "http" not in r.css("a").attrib["href"]:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www.coursupreme.dz" + r.css("a").attrib["href"],
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": r.css("a").attrib["href"],
                                       }
                        except:
                            pass

        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_article_links += 1




class CbiSpider(scrapy.Spider):
    name = "CbiSpider"
    allowed_domains = ["cbi.iq"]
    not_allowed_keyword = []
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                try:
                    data = response.css('.page-data a')
                except:
                    data = []
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('a::text').get().strip() if link.css('a::text') else 'na',
                                       "excerpt": 'na',
                                       "published_date":  'na',
                                       "pdf_url": link.css('a').attrib['href'] if link.css('a') else 'na'
                                        }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('.page-data a::text').get().strip() if link.css('.page-data a::text') else 'na',
                                       "excerpt": 'na',
                                       "published_date":  'na',
                                       "pdf_url": ""+link.css('.page-data a').attrib['href'] if link.css('.page-data').css('a') else 'na'
                                        }
                    except:
                        pass
        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1




class CbiRegSpider(scrapy.Spider):
    name = "CbiRegSpider"
    allowed_domains = ["cbi.iq"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.news-list-item')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('h2 a::text').get().strip() if link.css('h2 a') else 'na',
                                       "excerpt": link.css('p::text').get() if link.css('p::text') else 'na',
                                       "published_date": link.css('.date::text').get() if link.css('.date') else 'na',
                                       "pdf_url": link.css("h2 a").attrib["href"]
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('h2 a::text').get().strip() if link.css('h2 a') else 'na',
                                       "excerpt": link.css('p::text').get() if link.css('p::text') else 'na',
                                       "published_date": link.css('.date::text').get() if link.css('.date') else 'na',
                                       "pdf_url": "https://cbi.iq" + link.css("h2 a").attrib["href"],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1



class HrsdSpider(scrapy.Spider):
    name = "HrsdSpider"
    allowed_domains = ["hrsd.gov.sa"]
    not_allowed_keyword = []
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                try:
                    data = response.css('.item')
                except:
                    data = []
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('.title::text').get().strip() if link.css('.title::text') else 'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": link.css('.download-item').attrib['href'] if link.css('.download-item') else 'na',
                                        }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('.d-desc title::text').get().strip() if link.css('.d-desc title::text') else 'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": "" + link.css('.download-item').attrib['href'] if link.css('.download-item') else 'na',
                                        }
                    except:
                        pass
        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1




class Miepeec2Spider(scrapy.Spider):
    name = "Miepeec2Spider"
    allowed_domains = ["miepeec.gov.ma"]
    not_allowed_keyword = []
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

            yield res_ip

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                try:
                    data = response.css("tr")
                except:
                    data = []
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('td.column-1::text').get().strip() if ('td.column-1::text') else 'na',
                                       "excerpt": 'na',
                                       "published_date": link.css("td.column-2::text").get() if link.css("td.column-2::text").get() else 'na',
                                       "pdf_url": link.css('td.column-3 a').attrib['href'] if link.css('td.column-3 a') else 'na',
                                        }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('td.column-1::text').get().strip() if ('td.column-1::text') else 'na',
                                       "excerpt": 'na',
                                       "published_date": link.css("td.column-2::text").get() if link.css("td.column-2::text").get() else 'na',
                                       "pdf_url": link.css('td.column-3 a').attrib['href'] if link.css('td.column-3 a') else 'na',
                                        }
                    except:
                        pass
        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1


class Almeezan2Spider(scrapy.Spider):
    name = "Almeezan2Spider"
    allowed_domains = ["almeezan.qa"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):

                data = response.css('.bulleted-list a')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://almeezan.qa/" + link.css("a").attrib["href"],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1




class TejarahSpider(scrapy.Spider):
    name = "TejarahSpider"
    allowed_domains = ["tejarah.gov.om"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0


    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.explore-projects-content')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('.item.d-flex.align-items-center span::text').get().strip() if link.css('.item.d-flex.align-items-center span') else 'na',
                                       "excerpt": link.css('h3::text').get().strip() if link.css('h3') else 'na',
                                       "published_date": 'na',
                                       "pdf_url": link.css("a").attrib["href"].replace('..','')
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('.item.d-flex.align-items-center span::text').get().strip() if link.css('.item.d-flex.align-items-center span') else 'na',
                                       "excerpt": link.css('h3::text').get().strip() if link.css('h3') else 'na',
                                       "published_date": 'na',
                                       "pdf_url": "https://tejarah.gov.om" + link.css("a").attrib["href"].replace('..','')
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1

class dailypostNGSpider(scrapy.Spider):
    name = "dailypostNGSpider"
    allowed_domains = ["dailypost.ng"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

            yield res_ip

    # def parse(self, response):
    #     if str(response.status) == "200":
    #         if response.css("body"):
    #             categories_links = []
    #             res = response.css('#menu-menu1 a')
    #             for r in res:
    #                 try:
    #                     if "https://" in str(r.css("a").attrib["href"]) or "http://" in str(r.css("a").attrib["href"]):
    #                         categories_links.append(r.css("a").attrib["href"])
    #                     else:
    #                         categories_links.append(
    #                             "https://dailypost.ng" + r.css("a").attrib["href"]
    #                         )
    #
    #                 except:
    #                     pass
    #
    #             for i in set(categories_links):
    #                 print(i)
    #                 try:
    #                     yield scrapy.Request(
    #                         i,
    #                         callback=self.article_links,
    #                         meta={"dont_retry": True, "download_timeout": cat_timeout},
    #                         errback=handle_error,
    #                     )
    #
    #
    #                 except:
    #                     pass
    #
    #             data = response.css('div.mvp-widget-feat1-cont.left.relative').css('a')
    #             for link in data:
    #                 try:
    #                     if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
    #                         pass
    #                     else:
    #                         if "https://" in str(link.css("a").attrib["href"]):
    #                             yield {"clean_url": self.allowed_domains[0],
    #                                    "base_url": response.url,
    #                                    "link": link.css("a").attrib["href"],
    #                                    "type": 'article'}
    #                         else:
    #                             yield {"clean_url": self.allowed_domains[0],
    #                                    "base_url": response.url,
    #                                    "link": "https://dailypost.ng" + link.css("a").attrib["href"],
    #                                    "type": 'article'}
    #                 except:
    #                     pass

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.infinite-post')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "https://" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"],
                                       "type": 'article'}
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://dailypost.ng" + link.css("a").attrib["href"],
                                       "type": 'article'}
                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_category += 1

class brazilianReportSpider(scrapy.Spider):
    name = "brazilianReportSpider"
    allowed_domains = ["brazilian.report"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

            yield res_ip

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                categories_links = []
                res = response.css('.menu-item-59658 a , .menu-item-61947 a , .menu-item-59649 a , .menu-item-59656 a , .menu-item-59655 a , .menu-item-59654 a , .menu-item-59653 a , .menu-item-59652 a , .menu-item-59651 a , .menu-item-59650 a')
                for r in res:
                    try:
                        if "https://" in str(r.css("a").attrib["href"]) or "http://" in str(r.css("a").attrib["href"]):
                            categories_links.append(r.css("a").attrib["href"])
                        else:
                            categories_links.append(
                                "https://brazilian.report" + r.css("a").attrib["href"]
                            )

                    except:
                        pass

                for i in set(categories_links):
                    try:
                        yield scrapy.Request(
                            i,
                            callback=self.article_links,
                            meta={"dont_retry": True, "download_timeout": cat_timeout},
                            errback=handle_error,
                        )


                    except:
                        pass

                data = response.css('.news-card')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "https://" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"],
                                       "type": 'article'}
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://brazilian.report" + link.css("a").attrib["href"],
                                       "type": 'article'}
                    except:
                        pass

    def article_links(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.news-card')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "https://" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": self.start_urls[0],
                                       "link": link.css("a").attrib["href"],
                                       "type": 'article'}
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": self.start_urls[0],
                                       "link": "https://brazilian.report" + link.css("a").attrib["href"],
                                       "type": 'article'}
                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_category += 1

class dailytimesngSpider(scrapy.Spider):
    name = "dailytimesngSpider"
    allowed_domains = ["dailytimesng.com"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

            yield res_ip

    # def parse(self, response):
    #     if str(response.status) == "200":
    #         if response.css("body"):
    #             categories_links = []
    #             res = response.css('.vce-cat-116 a , .vce-cat-33 > a , .vce-cat-7 > a , .vce-cat-90 > a , .vce-cat-1 > a , .vce-cat-2 > a , .vce-cat-53456 a , .vce-cat-52341 a , .vce-cat-36430 a , .vce-cat-50 a , .vce-cat-32 > a , .vce-cat-31 > a')
    #             for r in res:
    #                 try:
    #                     if "https://" in str(r.css("a").attrib["href"]) or "http://" in str(r.css("a").attrib["href"]):
    #                         categories_links.append(r.css("a").attrib["href"])
    #                     else:
    #                         categories_links.append(
    #                             "https://dailytimesng.com" + r.css("a").attrib["href"]
    #                         )
    #
    #                 except:
    #                     pass
    #
    #             for i in set(categories_links):
    #                 print(i)
    #                 try:
    #                     yield scrapy.Request(
    #                         i,
    #                         callback=self.article_links,
    #                         meta={"dont_retry": True, "download_timeout": cat_timeout},
    #                         errback=handle_error,
    #                     )
    #
    #
    #                 except:
    #                     pass
    #
    #             data = response.css('.entry-title a')
    #             for link in data:
    #                 try:
    #                     if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
    #                         pass
    #                     else:
    #                         if "https://" in str(link.css("a").attrib["href"]):
    #                             yield {"clean_url": self.allowed_domains[0],
    #                                    "base_url": response.url,
    #                                    "link": link.css("a").attrib["href"],
    #                                    "type": 'article'}
    #                         else:
    #                             yield {"clean_url": self.allowed_domains[0],
    #                                    "base_url": response.url,
    #                                    "link": "https://dailytimesng.com" + link.css("a").attrib["href"],
    #                                    "type": 'article'}
    #                 except:
    #                     pass

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.entry-title a')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "https://" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"],
                                       "type": 'article'}
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://dailytimesng.com" + link.css("a").attrib["href"],
                                       "type": 'article'}
                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_category += 1

class elpaisSpider(scrapy.Spider):
    name = "elpaisSpider"
    allowed_domains = ["elpais.com"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

            yield res_ip

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                categories_links = []
                res = response.css("#csw ._df a")
                for r in res:
                    try:
                        if "https://" in str(r.css("a").attrib["href"]) or "http://" in str(r.css("a").attrib["href"]):
                            categories_links.append(r.css("a").attrib["href"])
                        else:
                            categories_links.append(
                                "https://elpais.com" + r.css("a").attrib["href"]
                            )

                    except:
                        pass

                for i in set(categories_links):
                    try:
                        yield scrapy.Request(
                            i,
                            callback=self.article_links,
                            meta={"dont_retry": True, "download_timeout": cat_timeout},
                            errback=handle_error,
                        )


                    except:
                        pass

                data = response.css('.c_t a')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "https://" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"],
                                       "type": 'article'}
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://elpais.com" + link.css("a").attrib["href"],
                                       "type": 'article'}
                    except:
                        pass

    def article_links(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.c_t a')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "https://" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": self.start_urls[0],
                                       "link": link.css("a").attrib["href"],
                                       "type": 'article'}
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": self.start_urls[0],
                                       "link": "https://elpais.com" + link.css("a").attrib["href"],
                                       "type": 'article'}
                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_category += 1

class mercopressSpider(scrapy.Spider):
    name = "mercopressSpider"
    allowed_domains = ["en.mercopress.com"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

            yield res_ip

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.title a')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://en.mercopress.com" + link.css("a").attrib["href"],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_category += 1

class nationAfricaSpider(scrapy.Spider):
    name = "nationAfricaSpider"
    allowed_domains = ["nation.africa"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

            yield res_ip

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                categories_links = []
                res = response.css('.sub-nav_link-list-item:nth-child(7) a , .sub-nav_link-list-item:nth-child(6) a , .sub-nav_link-list-item:nth-child(5) a , .sub-nav_link-list-item:nth-child(4) a , .sub-nav_link-list-item:nth-child(3) a , .sub-nav_link-list-item:nth-child(2) a , .sub-nav_link-list-item:nth-child(1) a')
                for r in res:
                    try:
                        if "https://" in str(r.css("a").attrib["href"]) or "http://" in str(r.css("a").attrib["href"]):
                            categories_links.append(r.css("a").attrib["href"])
                        else:
                            categories_links.append(
                                "https://nation.africa" + r.css("a").attrib["href"]
                            )

                    except:
                        pass

                for i in set(categories_links):
                    try:
                        yield scrapy.Request(
                            i,
                            callback=self.article_links,
                            meta={"dont_retry": True, "download_timeout": cat_timeout},
                            errback=handle_error,
                        )


                    except:
                        pass

                data = response.css('a.article-collection-teaser')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "https://" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"],
                                       "type": 'article'}
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://nation.africa" + link.css("a").attrib["href"],
                                       "type": 'article'}
                    except:
                        pass

    def article_links(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('a.article-collection-teaser, a.teaser-image-large')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "https://" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"],
                                       "type": 'article'}
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://nation.africa" + link.css("a").attrib["href"],
                                       "type": 'article'}
                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_category += 1

class ntvkenyaSpider(scrapy.Spider):
    name = "ntvkenyaSpider"
    allowed_domains = ["ntvkenya.co.ke"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

            yield res_ip

    # def parse(self, response):
    #     if str(response.status) == "200":
    #         if response.css("body"):
    #             categories_links = []
    #             res = response.css('a.content_nav__list-item')
    #             for r in res:
    #                 try:
    #                     if "https://" in str(r.css("a").attrib["href"]) or "http://" in str(r.css("a").attrib["href"]):
    #                         categories_links.append(r.css("a").attrib["href"])
    #                     else:
    #                         categories_links.append(
    #                             "https://ntvkenya.co.ke" + r.css("a").attrib["href"]
    #                         )
    #
    #                 except:
    #                     pass
    #
    #             for i in set(categories_links):
    #                 print(i)
    #                 try:
    #                     yield scrapy.Request(
    #                         i,
    #                         callback=self.article_links,
    #                         meta={"dont_retry": True, "download_timeout": cat_timeout},
    #                         errback=handle_error,
    #                     )
    #
    #
    #                 except:
    #                     pass
    #
    #             data = response.css('a.video-card_title')
    #             for link in data:
    #                 try:
    #                     if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
    #                         pass
    #                     else:
    #                         if "https://" in str(link.css("a").attrib["href"]):
    #                             yield {"clean_url": self.allowed_domains[0],
    #                                    "base_url": response.url,
    #                                    "link": link.css("a").attrib["href"],
    #                                    "type": 'article'}
    #                         else:
    #                             yield {"clean_url": self.allowed_domains[0],
    #                                    "base_url": response.url,
    #                                    "link": "https://ntvkenya.co.ke" + link.css("a").attrib["href"],
    #                                    "type": 'article'}
    #                 except:
    #                     pass

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('a.video-card_title')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "https://" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": self.start_urls[0],
                                       "link": link.css("a").attrib["href"],
                                       "type": 'article'}
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": self.start_urls[0],
                                       "link": "https://ntvkenya.co.ke" + link.css("a").attrib["href"],
                                       "type": 'article'}
                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_category += 1

class bbcNigeriaSpider(scrapy.Spider):
    name = "bbcNigeriaSpider"
    allowed_domains = ["bbc.co.uk"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

            yield res_ip

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.e1f5wbog0')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www.bbc.co.uk" + link.css("a").attrib["href"],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_category += 1

class bbcBrazilSpider(scrapy.Spider):
    name = "bbcBrazilSpider"
    allowed_domains = ["bbc.com"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

            yield res_ip

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.e1f5wbog0')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www.bbc.com" + link.css("a").attrib["href"],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_category += 1

class brazzilSpider(scrapy.Spider):
    name = "brazzilSpider"
    allowed_domains = ["brazzil.com"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

            yield res_ip

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                categories_links = []
                res = response.css('.menu-item-39417-0 a')
                for r in res:
                    try:
                        if "https://" in str(r.css("a").attrib["href"]) or "http://" in str(r.css("a").attrib["href"]):
                            categories_links.append(r.css("a").attrib["href"])
                        else:
                            categories_links.append(
                                "https://www.brazzil.com" + r.css("a").attrib["href"]
                            )

                    except:
                        pass

                for i in set(categories_links):
                    try:
                        yield scrapy.Request(
                            i,
                            callback=self.article_links,
                            meta={"dont_retry": True, "download_timeout": cat_timeout},
                            errback=handle_error,
                        )


                    except:
                        pass

                data = response.css('.article-title')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "https://" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"],
                                       "type": 'article'}
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www.brazzil.com" + link.css("a").attrib["href"],
                                       "type": 'article'}
                    except:
                        pass

    def article_links(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.article-title')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "https://" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": self.start_urls[0],
                                       "link": link.css("a").attrib["href"],
                                       "type": 'article'}
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": self.start_urls[0],
                                       "link": "https://www.brazzil.com" + link.css("a").attrib["href"],
                                       "type": 'article'}
                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_category += 1

class capitalfmSpider(scrapy.Spider):
    name = "capitalfmSpider"
    allowed_domains = ["capitalfm.co.ke"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

            yield res_ip

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.zox-art-title')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www.capitalfm.co.ke" + link.css("a").attrib["href"],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_category += 1

class huffingtonpostSpider(scrapy.Spider):
    name = "huffingtonpostSpider"
    allowed_domains = ["huffingtonpost.es"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

            yield res_ip

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('a.card__headline')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www.huffingtonpost.es" + link.css("a").attrib["href"],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_category += 1

class independentNGSpider(scrapy.Spider):
    name = "independentNGSpider"
    allowed_domains = ["independent.ng"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

            yield res_ip

    # def parse(self, response):
    #     if str(response.status) == "200":
    #         if response.css("body"):
    #             categories_links = []
    #             res = response.css('#main-navigation .menu-item-1383617 a')
    #             for r in res:
    #                 try:
    #                     if "https://" in str(r.css("a").attrib["href"]) or "http://" in str(r.css("a").attrib["href"]):
    #                         categories_links.append(r.css("a").attrib["href"])
    #                     else:
    #                         categories_links.append(
    #                             "https://independent.ng" + r.css("a").attrib["href"]
    #                         )
    #
    #                 except:
    #                     pass
    #
    #             for i in set(categories_links):
    #                 print(i)
    #                 try:
    #                     yield scrapy.Request(
    #                         i,
    #                         callback=self.article_links,
    #                         meta={"dont_retry": True, "download_timeout": cat_timeout},
    #                         errback=handle_error,
    #                     )
    #
    #
    #                 except:
    #                     pass
    #
    #             data = response.css('.post-title')
    #             for link in data:
    #                 try:
    #                     if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
    #                         pass
    #                     else:
    #                         if "https://" in str(link.css("a").attrib["href"]):
    #                             yield {"clean_url": self.allowed_domains[0],
    #                                    "base_url": response.url,
    #                                    "link": link.css("a").attrib["href"],
    #                                    "type": 'article'}
    #                         else:
    #                             yield {"clean_url": self.allowed_domains[0],
    #                                    "base_url": response.url,
    #                                    "link": "https://independent.ng" + link.css("a").attrib["href"],
    #                                    "type": 'article'}
    #                 except:
    #                     pass

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.post-title')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "https://" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"],
                                       "type": 'article'}
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://independent.ng" + link.css("a").attrib["href"],
                                       "type": 'article'}
                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_category += 1

class kbcSpider(scrapy.Spider):
    name = "kbcSpider"
    allowed_domains = ["kbc.co.ke"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

            yield res_ip

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                categories_links = []
                res = response.css('.ptop--20 a')
                for r in res:
                    try:
                        if "https://" in str(r.css("a").attrib["href"]) or "http://" in str(r.css("a").attrib["href"]):
                            categories_links.append(r.css("a").attrib["href"])
                        else:
                            categories_links.append(
                                "https://www.kbc.co.ke" + r.css("a").attrib["href"]
                            )

                    except:
                        pass

                for i in set(categories_links):
                    try:
                        yield scrapy.Request(
                            i,
                            callback=self.article_links,
                            meta={"dont_retry": True, "download_timeout": cat_timeout},
                            errback=handle_error,
                        )


                    except:
                        pass

                data = response.css('.title .btn-link')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "https://" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"],
                                       "type": 'article'}
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www.kbc.co.ke" + link.css("a").attrib["href"],
                                       "type": 'article'}
                    except:
                        pass

    def article_links(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.title .btn-link')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "https://" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"],
                                       "type": 'article'}
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www.kbc.co.ke" + link.css("a").attrib["href"],
                                       "type": 'article'}
                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_category += 1

class premiumtimesngSpider(scrapy.Spider):
    name = "premiumtimesngSpider"
    allowed_domains = ["premiumtimesng.com"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

            yield res_ip

    # def parse(self, response):
    #     if str(response.status) == "200":
    #         if response.css("body"):
    #             categories_links = []
    #             res = response.css('.menu-item-object-category a')
    #             for r in range(0,21):
    #                 try:
    #                     if "https://" in str(res[r].css("a").attrib["href"]) or "http://" in str(res[r].css("a").attrib["href"]):
    #                         categories_links.append(res[r].css("a").attrib["href"])
    #                     else:
    #                         categories_links.append(
    #                             "https://www.premiumtimesng.com" + res[r].css("a").attrib["href"]
    #                         )
    #
    #                 except:
    #                     pass
    #
    #             for i in set(categories_links):
    #                 print(i)
    #                 try:
    #                     yield scrapy.Request(
    #                         i,
    #                         callback=self.article_links,
    #                         meta={"dont_retry": True, "download_timeout": cat_timeout},
    #                         errback=handle_error,
    #                     )
    #
    #
    #                 except:
    #                     pass
    #
    #             data = response.css('.jeg_post_title a')
    #             for link in data:
    #                 try:
    #                     if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
    #                         pass
    #                     else:
    #                         if "https://" in str(link.css("a").attrib["href"]):
    #                             yield {"clean_url": self.allowed_domains[0],
    #                                    "base_url": response.url,
    #                                    "link": link.css("a").attrib["href"],
    #                                    "type": 'article'}
    #                         else:
    #                             yield {"clean_url": self.allowed_domains[0],
    #                                    "base_url": response.url,
    #                                    "link": "https://www.premiumtimesng.com" + link.css("a").attrib["href"],
    #                                    "type": 'article'}
    #                 except:
    #                     pass

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.jeg_post_title a')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "https://" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"],
                                       "type": 'article'}
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www.premiumtimesng.com" + link.css("a").attrib["href"],
                                       "type": 'article'}
                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_category += 1

class riotimesonlineSpider(scrapy.Spider):
    name = "riotimesonlineSpider"
    allowed_domains = ["riotimesonline.com"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

            yield res_ip

    # def parse(self, response):
    #     if str(response.status) == "200":
    #         if response.css("body"):
    #             categories_links = []
    #             res = response.css('#menu-header-main-menu-1 a')
    #             for r in res:
    #                 try:
    #                     if "https://" in str(r.css("a").attrib["href"]) or "http://" in str(r.css("a").attrib["href"]):
    #                         categories_links.append(r.css("a").attrib["href"])
    #                     else:
    #                         categories_links.append(
    #                             "https://www.riotimesonline.com" + r.css("a").attrib["href"]
    #                         )
    #
    #                 except:
    #                     pass
    #
    #             for i in set(categories_links):
    #                 print(i)
    #                 try:
    #                     yield scrapy.Request(
    #                         i,
    #                         callback=self.article_links,
    #                         meta={"dont_retry": True, "download_timeout": cat_timeout},
    #                         errback=handle_error,
    #                     )
    #
    #
    #                 except:
    #                     pass
    #
    #             data = response.css('.td-module-title a')
    #             for link in data:
    #                 try:
    #                     if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
    #                         pass
    #                     else:
    #                         if "https://" in str(link.css("a").attrib["href"]):
    #                             yield {"clean_url": self.allowed_domains[0],
    #                                    "base_url": response.url,
    #                                    "link": link.css("a").attrib["href"],
    #                                    "type": 'article'}
    #                         else:
    #                             yield {"clean_url": self.allowed_domains[0],
    #                                    "base_url": response.url,
    #                                    "link": "https://www.riotimesonline.com" + link.css("a").attrib["href"],
    #                                    "type": 'article'}
    #                 except:
    #                     pass

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.td-module-title a')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "https://" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"],
                                       "type": 'article'}
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www.riotimesonline.com" + link.css("a").attrib["href"],
                                       "type": 'article'}
                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_category += 1

class thecableSpider(scrapy.Spider):
    name = "thecableSpider"
    allowed_domains = ["thecable.ng"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

            yield res_ip
    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.main-split-left .article-header,.main-split-right .article-header')
                for link in data:
                    try:
                        if "http" in str(link.css("a").attrib["href"]):
                            yield {"clean_url": self.allowed_domains[0],
                                   "base_url": response.url,
                                   "link": link.css("a").attrib["href"]
                                   }
                        else:
                            yield {"clean_url": self.allowed_domains[0],
                                   "base_url": response.url,
                                   "link": "https://www.thecable.ng"
                                + link.css("a").attrib["href"]
                                   }
                    except:
                        pass
        else:
            while self.check_ip_article_links < 2:
                self.start_urls = response.url
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_article_links += 1



class theStarSpider(scrapy.Spider):
    name = "theStarSpider"
    allowed_domains = ["the-star.co.ke"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

            yield res_ip

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                categories_links = []
                res = response.css('.dropdown-news a')
                for r in res:
                    try:
                        if "https://" in str(r.css("a").attrib["href"]) or "http://" in str(r.css("a").attrib["href"]):
                            categories_links.append(r.css("a").attrib["href"])
                        else:
                            categories_links.append(
                                "https://www.the-star.co.ke" + r.css("a").attrib["href"]
                            )

                    except:
                        pass

                for i in set(categories_links):
                    try:
                        yield scrapy.Request(
                            i,
                            callback=self.article_links,
                            meta={"dont_retry": True, "download_timeout": cat_timeout},
                            errback=handle_error,
                        )


                    except:
                        pass

                data = response.css('.article-body')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "https://" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"],
                                       "type": 'article'}
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www.the-star.co.ke" + link.css("a").attrib["href"],
                                       "type": 'article'}
                    except:
                        pass

    def article_links(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.article-body')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "https://" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"],
                                       "type": 'article'}
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www.the-star.co.ke" + link.css("a").attrib["href"],
                                       "type": 'article'}
                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_category += 1

class folhaSpider(scrapy.Spider):
    name = "folhaSpider"
    allowed_domains = ["folha.uol.com.br"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

            yield res_ip

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.c-headline__content')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www1.folha.uol.com.br" + link.css("a").attrib["href"],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_category += 1

class ElmundoSpider(scrapy.Spider):
    name = "ElmundoSpider"
    allowed_domains = ["elmundo.es"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

            yield res_ip

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css(".ue-c-cover-content__link")[:10]
                for link in data:
                    try:

                        if "http" in str(link.css("a").attrib["href"]):
                            yield {"clean_url": self.allowed_domains[0],
                                   "base_url": response.url,
                                   "link": link.css("a").attrib["href"]
                                   }
                        else:
                            yield {"clean_url": self.allowed_domains[0],
                                   "base_url": response.url,
                                   "link": "https://www.elmundo.es"
                                        + link.css("a").attrib["href"]
                                   }

                    except:
                        pass

        else:
            while self.check_ip_article_links < 2:
                self.start_urls = response.url
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_article_links += 1

class FoxNewsSpider(scrapy.Spider):
    name = "FoxNewsSpider"
    allowed_domains = ["foxnews.com"]
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

            yield res_ip
    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css(".title")
                for link in data:
                    try:
                        if "http" in str(link.css("a").attrib["href"]):
                            yield {"clean_url": self.allowed_domains[0],
                                   "base_url": response.url,
                                   "link": link.css("a").attrib["href"]
                                   }
                        else:
                            yield {"clean_url": self.allowed_domains[0],
                                   "base_url": response.url,
                                   "link": "https://www.foxnews.com"
                                + link.css("a").attrib["href"]
                                   }
                    except:
                        pass
        else:
            while self.check_ip_article_links < 2:
                self.start_urls = response.url
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_article_links += 1


class StandardMediaSpider(scrapy.Spider):
    name = "StandardMediaSpider"
    allowed_domains = ["standardmedia.co.ke"]
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

            yield res_ip

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css(".d-md-block , .text-dark , .card-title").css("a")
                for link in data:
                    try:
                        yield {"clean_url": self.allowed_domains[0],
                               "base_url": response.url,
                               "link": link.attrib["href"]
                               }
                    except:
                        pass

        else:
            while self.check_ip_article_links < 2:
                self.start_urls = response.url
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_article_links += 1

class TheGuardianSpider(scrapy.Spider):
    name = "TheGuardianSpider"
    allowed_domains = ["theguardian.com"]
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

            yield res_ip

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css("div.fc-item__content")
                for link in data:
                    try:
                        yield {"clean_url": self.allowed_domains[0],
                               "base_url": response.url,
                               "link": link.css("a").attrib["href"]}
                    except:
                        pass
        else:
            while self.check_ip_article_links < 2:
                self.start_urls = response.url
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_article_links += 1


class KenyaLawSpider(scrapy.Spider):
    name = "KenyaLawSpider"
    allowed_domains = ["kenyalaw.org"]
    start_urls_local = ['http://kenyalaw.org/kenya_gazette/gazette/year/2022/']
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls_local:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls_local:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                categories_links = []
                res = response.css('td')
                for r in res:
                    if r.css("a"):
                        try:
                            if "https://" in str(r.css("a").attrib["href"]) or "http://" in str(r.css("a").attrib["href"]):
                                categories_links.append(r.css("a").attrib["href"])
                            else:
                                categories_links.append(
                                    "http://kenyalaw.org"
                                    + r.css("a").attrib["href"]
                                )

                        except:
                            pass

                for i in set(categories_links):
                    try:
                        yield scrapy.Request(
                            i,
                            callback=self.article_links,
                            meta={"dont_retry": True, "download_timeout": cat_timeout, "base_url": response.url},
                            errback=handle_error,
                        )


                    except:
                        pass

    def article_links(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                base_url = self.start_urls[0]
                data = response.css('.sd')
                for link in data.css("a"):
                    try:
                        if "http://" in str(link.attrib["href"]):
                            yield {"clean_url": self.allowed_domains[0],
                                   "base_url": base_url,
                                   "title": response.url.split("/")[len(response.url.split("/"))-2],
                                   "excerpt": 'na',
                                   "published_date": response.css("strong")[3].get().split("\t")[len(response.css("strong")[3].get().split("\t"))-1].replace("</strong>","")
                                   if response.css("strong")[3] else 'na',
                                   "pdf_url": link.attrib["href"],
                                   }
                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1

class abcEspanaSpider(scrapy.Spider):
    name = 'abcEspanaSpider'
    allowed_domains = ['abc.es']
    not_allowed_keyword = ['/autor']
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"proxy": proxy,"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res
            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )


    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                try:
                    data = response.css('article')
                except:
                    data = []
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css('a').attrib['href']):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css('a').attrib['href'].replace(' ', '%20'),
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": 'https://www.abc.es' + link.css('a').attrib['href'].replace(' ',
                                                                                                                  '%20'),
                                       }
                    except:
                        pass
        else:
            while self.check_ip_article_links < 5:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1

class vanguardngrPoliticsSpider(scrapy.Spider):
    name = 'vanguardngrPoliticsSpider'
    allowed_domains = ['www.vanguardngr.com']
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"proxy": proxy,"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                try:
                    data = response.css('article, .latest-news-title')
                except:
                    data = []
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css('a').attrib['href']):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css('a').attrib['href'].replace(' ', '%20'),
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": 'https://www.vanguardngr.com' + link.css('a').attrib['href'].replace(' ',
                                                                                                                  '%20'),
                                       }
                    except:
                        pass
        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1

class citizenDigital(scrapy.Spider):
    name = 'citizenDigital'
    allowed_domains = ['citizen.digital']
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"proxy": proxy,"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                try:
                    data = BeautifulSoup(response.text).find_all('a', href=True)
                except:
                    data = []
                for link in data:
                    try:
                        if any(n in str(link.attrs["href"]) for n in self.not_allowed_keyword) or not link.attrs["href"].startswith('/news'):
                            pass
                        else:
                            if "http" in str(link.attrs["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.attrs["href"].replace(' ', '%20'),
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": 'https://www.citizen.digital' + link.attrs["href"].replace(' ',
                                                                                                                  '%20'),
                                       }
                    except:
                        pass
        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1





class MwiPdfSpider(scrapy.Spider):
    name = "MwiPdfSpider"
    allowed_domains = ["mwi.gov.jo"]
    not_allowed_keyword = ["#"]
    check_ip_article_links = 0


    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                llinks = []

                try:
                    data = response.css('.col-2.col-sm-2.text-center.text-black a')
                except:
                    data = []
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        elif link.css("a").attrib["href"] not in llinks:
                            llinks.append(link.css("a").attrib["href"])
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title":'na',
                                       "excerpt": 'na',
                                       "published_date":  'na',
                                       "pdf_url":link.css('a').attrib['href'] if link.css('a') else  'na'
                                        }
                            else:
                                yield  {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title": 'na',
                                       "excerpt": 'na',
                                       "published_date":  'na',
                                       "pdf_url":"https://mwi.gov.jo" +  link.css('a').attrib['href'] if link.css('a') else 'na'
                                        }
                    except:
                        pass
        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1


class JscSpider(scrapy.Spider):
    name = "JscSpider"
    allowed_domains = ["www.jsc.gov.jo"]
    not_allowed_keyword = []
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                llinks = []

                try:
                    data = response.css('tr')
                except:
                    data = []
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        elif link.css("a").attrib["href"] not in llinks:
                            llinks.append(link.css("a").attrib["href"])
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title": 'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": link.css('td a').attrib['href'] if link.css('td a') else 'na'
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title": link.css('td')[1].css("::text").get().strip(),
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": "https://www.jsc.gov.jo" + link.css('td a').attrib[
                                           'href'] if link.css('td a') else 'na'
                                       }
                    except:
                        pass
        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1



class CarcSpider(scrapy.Spider):
    name = "CarcSpider"
    allowed_domains = ["carc.gov.jo"]
    not_allowed_keyword = []
    check_ip_article_links = 0


    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                llinks = []
                try:
                    data = response.css('#block-carcjo-content .views-row')
                except:
                    data = []
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        elif link.css("a").attrib["href"] not in llinks:
                            llinks.append(link.css("a").attrib["href"])
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title":  link.css('a::text').get().strip() if link.css('a::text') else 'na',
                                       "excerpt": 'na',
                                       "published_date":  'na',
                                       "pdf_url": link.css('a').attrib['href'] if link.css('a') else 'na'
                                        }
                            else:
                                yield  {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title": link.css('a::text').get().strip() if link.css('a::text') else 'na',
                                       "excerpt": 'na',
                                       "published_date":  'na',
                                       "pdf_url":"https://carc.gov.jo/" + link.css('a').attrib['href'] if link.css('a') else 'na'
                                        }
                    except:
                        pass
        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1


class JmaSpider(scrapy.Spider):
    name = "JmaSpider"
    allowed_domains = ["jma.gov.jo"]
    not_allowed_keyword = []
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                llinks = []
                try:
                    data = response.css('td')
                except:
                    data = []
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        elif link.css("a").attrib["href"] not in llinks:
                            llinks.append(link.css("a").attrib["href"])
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title":  link.css('a span::text').get().strip() if link.css('a span::text') else 'na',
                                       "excerpt": 'na',
                                       "published_date":  'na',
                                       "pdf_url": link.css('a').attrib['href'] if link.css('a') else 'na'
                                        }
                            else:
                                yield  {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title": link.css('a span::text').get().strip() if link.css('a span::text') else 'na',
                                       "excerpt": 'na',
                                       "published_date":  'na',
                                       "pdf_url":"" + link.css('a').attrib['href'] if link.css('a') else 'na'
                                        }
                    except:
                        pass
        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1


class PcSpider(scrapy.Spider):
    name = "PcSpider"
    allowed_domains = ["pc.gov.pk"]
    not_allowed_keyword = []
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                llinks = []

                try:
                    data = response.css('.col-md-9')
                    data += response.css('.col-md-10')
                except:
                    data = []
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        elif link.css("a").attrib["href"] not in llinks:
                            llinks.append(link.css("a").attrib["href"])
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title": link.css('h4::text').get().strip() if link.css('h4::text') else 'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": link.css('a').attrib['href'] if link.css('a') else 'na'
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title": link.css('h4::text').get().strip() if link.css('h4::text') else 'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": "http://www.moib.gov.pk" + link.css('a').attrib['href'] if link.css(
                                           'a') else 'na'
                                       }
                    except:
                        pass
        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1



class StzaSpider(scrapy.Spider):
    name = "StzaSpider"
    allowed_domains = ["www.stza.gov.pk"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                llinks =[]
                data = response.css('.nt_rounded_btn,.nt_rounded_btn,.elementor-button-wrapper')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        elif link.css("a").attrib["href"] not in llinks:
                            llinks.append(link.css("a").attrib["href"])
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title": 'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": link.css('a').attrib['href'] if link.css('a') else 'na'
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title": 'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": "https://www.stza.gov.pk" + link.css('a').attrib['href'] if link.css(
                                           'a') else 'na'
                                       }
                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class Commerce2Spider(scrapy.Spider):
    name = "Commerce2Spider"
    allowed_domains = ["www.commerce.gov.pk"]
    not_allowed_keyword = []
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                llinks = []
                try:
                    data = response.css('td')

                except:
                    data = []
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        elif link.css("a").attrib["href"] not in llinks:
                            llinks.append(link.css("a").attrib["href"])
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title": link.css('a::text').get().strip() if link.css('a::text') else 'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": link.css('a').attrib['href'] if link.css('a') else 'na'
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title": link.css('a::text').get().strip() if link.css('a::text') else 'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": "http://www.commerce.gov.pk" + link.css('a').attrib[
                                           'href'] if link.css('a') else 'na'
                                       }
                    except:
                        pass
        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1


class FiaSpider(scrapy.Spider):
    name = "FiaSpider"
    allowed_domains = ["www.fia.gov.pk"]
    not_allowed_keyword = []
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                headers={"X-Crawlera-Region": "US"},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                llinks = []
                try:
                    data = response.css('.how-to-complain li')

                except:
                    data = []
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        elif link.css("a").attrib["href"] not in llinks and ".pdf" in link.css("a").attrib['href']:
                            llinks.append(link.css("a").attrib["href"])
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title": link.css('a::text').get().strip() if link.css('a::text') else 'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": link.css('a').attrib['href'] if link.css('a') else 'na'
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title": link.css('a::text').get().strip() if link.css('a::text') else 'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": "https://www.fia.gov.pk/" + link.css('a').attrib['href'] if link.css(
                                           'a') else 'na'
                                       }
                    except:
                        pass
        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1




class PsqcaSpider(scrapy.Spider):
    name = "PsqcaSpider"
    allowed_domains = ["updated.psqca.com.pk"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                llinks = []
                data = response.css('.wpb_content_element .wpb_wrapper a')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        elif link.css("a").attrib["href"] not in llinks:
                            llinks.append(link.css("a").attrib["href"])
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title":  link.css('a::text').get().strip() if link.css('a::text') else 'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": link.css('a').attrib['href'] if link.css('a') else 'na'
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title": link.css('a::text').get().strip() if link.css('a::text') else 'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": "http://psqca.com.pk" + link.css('a').attrib['href'] if link.css(
                                           'a') else 'na'
                                       }
                    except:
                        pass
        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class MoittSpider(scrapy.Spider):
    name = "MoittSpider"
    allowed_domains = ["www.moitt.gov.pk"]
    not_allowed_keyword = []
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                llinks = []
                try:
                    data = response.css('p')

                except:
                    data = []
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        elif link.css("a").attrib["href"] not in llinks:
                            llinks.append(link.css("a").attrib["href"])
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title": link.css('a span::text').get().strip() if link.css(
                                           'a span::text') else 'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": link.css('a').attrib['href']
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title": link.css('a span::text').get().strip() if link.css(
                                           'a span::text') else 'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": "https://www.moitt.gov.pk" + link.css('a').attrib['href']
                                       }
                    except:
                        pass
        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1


class PtaSpider(scrapy.Spider):
    name = "PtaSpider"
    allowed_domains = ["www.pta.gov.pk"]
    not_allowed_keyword = []
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                headers={"X-Crawlera-Region": "US"},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                llinks = []
                try:
                    data = response.css('tr')

                except:
                    data = []
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        elif link.css("a").attrib["href"] not in llinks:
                            llinks.append(link.css("a").attrib["href"])
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title": link.css('td:nth-child(2)::text').get().strip() if link.css(
                                           'td:nth-child(2)::text') else 'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": link.css('td a').attrib['href'] if link.css('td a') else 'na'
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title": link.css('td:nth-child(2)::text').get().strip() if link.css(
                                           'td:nth-child(2)::text') else 'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": "https://www.pta.gov.pk" + link.css('td a').attrib[
                                           'href'] if link.css('td a') else 'na'
                                       }
                    except:
                        pass
        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1




class HukoomiSpider(scrapy.Spider):
    name = "HukoomiSpider"
    allowed_domains = ["hukoomi.gov.qa"]
    not_allowed_keyword = []
    check_ip_article_links = 0


    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                llinks =[]
                data = response.css('a.list_card')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        elif link.css("a").attrib["href"] not in llinks:
                            llinks.append(link.css("a").attrib["href"])
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title": 'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": link.css('a').attrib['href']
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title": 'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": "https://hukoomi.gov.qa" + link.css('a').attrib['href']
                                       }
                    except:
                        pass


        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1




class MojSpider(scrapy.Spider):
    name = "MojSpider"
    allowed_domains = ["moj.gov.iq"]
    not_allowed_keyword = []
    check_ip_article_links = 0


    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                llinks = []
                data = response.css(".bottom_tools a")
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        elif link.css("a").attrib["href"] not in llinks:
                            llinks.append(link.css("a").attrib["href"])
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": self.start_urls[0],
                                       "title": 'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": link.css('a').attrib['href']
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": self.start_urls[0],
                                       "title": 'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": "https://moj.gov.iq" + link.css('a').attrib['href']
                                       }
                    except:
                        pass

                for i in range(2, 7):
                    next_page = f"https://moj.gov.iq/iraqmagen/page_{i}/"
                    try:
                        yield scrapy.Request(
                            next_page,
                            meta={"dont_retry": True, "download_timeout": timeout},
                            callback=self.parse,
                            errback=handle_error,
                        )
                    except:
                        pass






        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class McitqaSpider(scrapy.Spider):
    name = "McitqaSpider"
    allowed_domains = ["mcit.gov.qa"]
    not_allowed_keyword = ["/en"]
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                llinks = []
                try:
                    data = response.css('.mb-4')
                except:
                    data = []
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        elif link.css("a").attrib["href"] not in llinks:
                            llinks.append(link.css("a").attrib["href"])
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title":  link.css('h5::text').get().strip() if link.css('h5::text') else 'na',
                                       "excerpt": 'na',
                                       "published_date":  'na',
                                       "pdf_url": link.css('a').attrib['href'] if link.css('a') else 'na'
                                        }
                            else:
                                yield  {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title":  link.css('h5::text').get().strip() if link.css('h5::text') else 'na',
                                       "excerpt": 'na',
                                       "published_date":  'na',
                                       "pdf_url":"" + link.css('a').attrib['href'] if link.css('a') else 'na'
                                        }
                    except:
                        pass
        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1




class Mod1Spider(scrapy.Spider):
    name = "Mod1Spider"
    allowed_domains = ["mod.gov.pk"]
    not_allowed_keyword = []
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"proxy": proxy,"dont_retry": True, "download_timeout": timeout},
                headers={"X-Crawlera-Region": "FR"},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                llinks = []
                try:
                    data = response.css('td')
                except:
                    data = []
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        elif link.css("a").attrib["href"] not in llinks:
                            llinks.append(link.css("a").attrib["href"])
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title": 'na',
                                       "excerpt": 'na',
                                       "published_date":  'na',
                                       "pdf_url":link.css('a').attrib['href'] if link.css('a') else 'na',
                                        }
                            else:
                                yield  {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title":  'na',
                                       "excerpt": 'na',
                                       "published_date":  'na',
                                       "pdf_url": "https://mod.gov.pk" + link.css('a').attrib['href'] if link.css('a') else 'na',
                                        }
                    except:
                        pass
        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1




class MoccSpider(scrapy.Spider):
    name = "MoccSpider"
    allowed_domains = ["www.mocc.gov.pk"]
    not_allowed_keyword = ["environment"]
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                llinks = []
                try:
                    data = response.css('p')
                except:
                    data = []
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        elif link.css("a").attrib["href"] not in llinks:
                            llinks.append(link.css("a").attrib["href"])
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title":link.css('a::text').get().strip() if link.css('a::text') else 'na',
                                       "excerpt": 'na',
                                       "published_date":  'na',
                                       "pdf_url":link.css('a').attrib['href'] if link.css('a') else 'na',
                                        }
                            else:
                                yield  {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title":  link.css('a::text').get().strip() if link.css('a::text') else 'na',
                                       "excerpt": 'na',
                                       "published_date":  'na',
                                       "pdf_url": "https://www.mocc.gov.pk" + link.css('a').attrib['href'] if link.css('a') else 'na',
                                        }
                    except:
                        pass
        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1



class EmrcSpider(scrapy.Spider):
    name = "EmrcSpider"
    allowed_domains = ["emrc.gov.jo"]
    not_allowed_keyword = []
    check_ip_article_links = 0
    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                llinks = []
                try:
                    data = response.css('p')
                except:
                    data = []
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        elif link.css("a").attrib["href"] not in llinks:
                            llinks.append(link.css("a").attrib["href"])
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title":'na',
                                       "excerpt": 'na',
                                       "published_date":  'na',
                                       "pdf_url":link.css('a').attrib['href'] if link.css('a') else 'na',
                                        }
                            else:
                                yield  {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title":  'na',
                                       "excerpt": 'na',
                                       "published_date":  'na',
                                       "pdf_url": "https://emrc.gov.jo" + link.css('a').attrib['href'] if link.css('a') else 'na'
                                        }
                    except:
                        pass
        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1




class BalochistanSpider(scrapy.Spider):
    name = "BalochistanSpider"
    allowed_domains = ["balochistan.gov.pk"]
    not_allowed_keyword = []
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):

                categories_links = []
                res = response.css("#main a")
                for r in res:
                    try:
                        if "https://" in str(r.css("a").attrib["href"]):
                            categories_links.append(r.css("a").attrib["href"])
                        else:
                            categories_links.append("" + r.css("a").attrib["href"])

                    except:
                        pass

                for i in categories_links:
                    try:
                        yield scrapy.Request(
                            i,
                            callback=self.article_links,
                            meta={"dont_retry": True, "download_timeout": cat_timeout,"base_url": response.url},
                            errback=handle_error,
                        )

                        self.start_urls = i

                    except:
                        pass

        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_category += 1

    def article_links(self, response, llinks = []):
        if str(response.status) == "200":
            if response.css("body"):
                base_url = response.meta.get('base_url')
                try:
                    data = response.css('tr')
                except:
                    data = []
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        elif link.css("a").attrib["href"] not in llinks:
                            llinks.append(link.css("a").attrib["href"])
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": base_url if not base_url.endswith('/') else base_url[:-1],
                                       "title": link.css('td::text').get().strip() if link.css('td::text') else 'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url":link.css('a').attrib['href'] if link.css('a') else 'na',
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": base_url if not base_url.endswith('/') else base_url[:-1],
                                       "title": link.css('td::text').get().strip() if link.css('td::text') else 'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": "" + link.css('a').attrib['href'] if link.css('a') else 'na',
                                       }
                    except:
                        pass
        else:
            while self.check_ip_article_links < 2:
                self.start_urls = response.url
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_article_links += 1





class CabinetSpider(scrapy.Spider):
    name = "CabinetSpider"
    allowed_domains = ["cabinet.gov.pk"]
    not_allowed_keyword = []
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                llinks = []
                try:
                    data = response.css('td')
                except:
                    data = []
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        elif link.css("a").attrib["href"] not in llinks:
                            llinks.append(link.css("a").attrib["href"])
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title":'na',
                                       "excerpt": 'na',
                                       "published_date":  'na',
                                       "pdf_url":link.css('a').attrib['href'] if link else 'na',
                                        }
                            else:
                                yield  {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title": 'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                        "pdf_url":"https://www.cabinet.gov.pk" +  link.css('a').attrib['href'] if link.css('a') else 'na',
                                        }
                    except:
                        pass
        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1




class MoheSpider(scrapy.Spider):
    name = "MoheSpider"
    allowed_domains = ["mohe.gov.jo"]
    not_allowed_keyword = ["#"]
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                llinks = []

                try:
                    data = response.css('.col-2.col-sm-2.text-center.text-black a')
                except:
                    data = []
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        elif link.css("a").attrib["href"] not in llinks:
                            llinks.append(link.css("a").attrib["href"])
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title":'na',
                                       "excerpt": 'na',
                                       "published_date":  'na',
                                       "pdf_url":link.css('a').attrib['href'] if link.css('a') else 'na',
                                        }
                            else:
                                yield  {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title": 'na',
                                       "excerpt": 'na',
                                       "published_date":  'na',
                                        "pdf_url":"https://mohe.gov.jo" +  link.css('a').attrib['href'] if link.css('a') else 'na',
                                        }
                    except:
                        pass
        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1




class QfcraSpider(scrapy.Spider):
    name = "QfcraSpider"
    allowed_domains = ["www.qfcra.com"]
    not_allowed_keyword = ["#"]
    check_ip_article_links = 0


    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                llinks = []

                try:
                    data = response.css('p')
                except:
                    data = []
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        elif link.css("a").attrib["href"] not in llinks and ".pdf" in link.css("a").attrib['href']:
                            llinks.append(link.css("a").attrib["href"])
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title":link.css('a::text').get().strip() if link.css('a::text') else 'na',
                                       "excerpt": 'na',
                                       "published_date":  'na',
                                       "pdf_url":link.css('a').attrib['href'] if link.css('a') else  'na'
                                        }
                            else:
                                yield  {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title": link.css('a::text').get().strip() if link.css('a::text') else 'na',
                                       "excerpt": 'na',
                                       "published_date":  'na',
                                       "pdf_url":"https://www.qfcra.com" +  link.css('a').attrib['href'] if link.css('a') else 'na'
                                        }
                    except:
                        pass
        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1




class CcdGovSpider(scrapy.Spider):
    name = "CcdGovSpider"
    allowed_domains = ["ccd.gov.jo"]
    not_allowed_keyword = []
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                llinks = []
                try:
                    data = response.css('.col-12.border-bottom.py-1.bdr-grey')
                except:
                    data = []
                for link in data:
                    try:
                        if any(n in str(link.css("a")[1].attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        elif link.css("a")[1].attrib["href"] not in llinks:
                            llinks.append(link.css("a")[1].attrib["href"])
                            if "http" in str(link.css("a")[1].attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title":link.css('.col-md-6 label::text').get().strip() if link.css('.col-md-6 label') else 'na',
                                       "excerpt": 'na',
                                       "published_date":  'na',
                                       "pdf_url":link.css('a')[1].attrib['href'] if link.css('a')[1] else  'na'
                                        }

                            else:
                                yield  {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title":link.css('.col-md-6 label::text').get().strip() if link.css('.col-md-6 label') else 'na',
                                       "excerpt": 'na',
                                       "published_date":  'na',
                                       "pdf_url":"https://www.ccd.gov.jo" +  link.css('a')[1].attrib['href'] if link.css('a')[1] else 'na'
                                        }
                    except:
                        pass
        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1




class MoinGovSpider(scrapy.Spider):
    name = "MoinGovSpider"
    allowed_domains = ["moin.gov.jo"]
    not_allowed_keyword = []
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                llinks = []

                try:
                    data = response.css('.row-hover tr')
                except:
                    data = []
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        elif link.css("a").attrib["href"] not in llinks:
                            llinks.append(link.css("a").attrib["href"])
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title":link.css('.column-2::text').get().strip() if link.css('.column-2') else'na',
                                       "excerpt": 'na',
                                       "published_date":  'na',
                                       "pdf_url":link.css('.column-1 a').attrib['href'] if link.css('a') else 'na',
                                        }
                            else:
                                yield  {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title":link.css('.column-2::text').get().strip() if link.css('.column-2') else'na',
                                       "excerpt": 'na',
                                       "published_date":  'na',
                                        "pdf_url":"https://www.moin.gov.jo" + link.css('.column-1 a').attrib['href'] if link.css('a') else 'na',
                                        }
                    except:
                        pass
        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1




class SindhlawsSpider(scrapy.Spider):
    name = "SindhlawsSpider"
    allowed_domains = ["sindhlaws.gov.pk"]
    not_allowed_keyword = []
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):

                categories_links = []
                res = response.css('.col-md-12')
                for r in res:
                    try:
                        if "http" in str(r.css("a").attrib["href"]):
                            categories_links.append(r.css("a").attrib["href"])
                        else:
                            categories_links.append("http://sindhlaws.gov.pk/" + r.css("a").attrib["href"])

                    except:
                        pass

                for i in categories_links:
                    try:
                        yield scrapy.Request(
                            i,
                            callback=self.article_links,
                            meta={"dont_retry": True, "download_timeout": cat_timeout,"base_url": response.url},
                            errback=handle_error,
                        )

                        self.start_urls = i

                    except:
                        pass

        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_category += 1

    def article_links(self, response, llinks = []):
        if str(response.status) == "200":
            if response.css("body"):
                base_url = response.meta.get('base_url')
                try:
                    data = response.css('tbody tr')
                except:
                    data = []
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        elif link.css("a").attrib["href"] not in llinks:
                            llinks.append(link.css("a").attrib["href"])
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": base_url if not base_url.endswith('/') else base_url[:-1],
                                       "title": link.css('td')[1].css('::text').get().strip() if link.css('td')[1] else 'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url":link.css('td')[2].css('a').attrib['href'],
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": base_url if not base_url.endswith('/') else base_url[:-1],
                                       "title": link.css('td')[1].css('::text').get().strip() if link.css('td')[1] else 'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": "http://sindhlaws.gov.pk/" + link.css('td')[2].css('a').attrib['href'],
                                       }
                    except:
                        pass
        else:
            while self.check_ip_article_links < 2:
                self.start_urls = response.url
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_article_links += 1




class Qfcra2Spider(scrapy.Spider):
    name = "Qfcra2Spider"
    allowed_domains = ["qfcra.com"]
    not_allowed_keyword = []
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                llinks = []

                try:
                    data = response.css('tbody tr')
                except:
                    data = []
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        elif link.css("a").attrib["href"] not in llinks:
                            llinks.append(link.css("a").attrib["href"])
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title":link.css('td a::text').get().strip() if link.css('td a::text') else'na',
                                       "excerpt": 'na',
                                       "published_date":  'na',
                                       "pdf_url":link.css('td a').attrib['href'],
                                        }
                            else:
                                yield  {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title":link.css('td a::text').get().strip() if link.css('td a::text') else'na',
                                       "excerpt": 'na',
                                       "published_date":  'na',
                                        "pdf_url":"https://www.qfcra.com" + link.css('td a').attrib['href'],
                                        }
                    except:
                        pass
        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1




class QfmaOrgSpider(scrapy.Spider):
    name = "QfmaOrgSpider"
    allowed_domains = ["qfma.org.qa"]
    not_allowed_keyword = []
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                llinks = []

                try:
                    data = response.css('tbody tr')
                except:
                    data = []
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        elif link.css("a").attrib["href"] not in llinks:
                            llinks.append(link.css("a").attrib["href"])
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title":link.css('td a::text').get().strip() if link.css('td a::text') else'na',
                                       "excerpt": 'na',
                                       "published_date":  'na',
                                       "pdf_url":link.css('td a').attrib['href'],
                                        }
                            else:
                                yield  {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title":link.css('td a::text').get().strip() if link.css('td a::text') else'na',
                                       "excerpt": 'na',
                                       "published_date":  'na',
                                        "pdf_url":"https://www.qfma.org.qa" + link.css('td a').attrib['href'],
                                        }
                    except:
                        pass
        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1




class MoinGovArticleSpider(scrapy.Spider):
    name = "MoinGovArticleSpider"
    allowed_domains = ["moin.gov.jo"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": self.start_urls,
                }
            )
            yield res_ip

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.news_cell a')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "link": link.css("a").attrib["href"]
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "link": "https://www.moin.gov.jo" + link.css("a").attrib["href"],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_category += 1




class JscArticleSpider(scrapy.Spider):
    name = "JscArticleSpider"
    allowed_domains = ["jsc.gov.jo"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": self.start_urls,
                }
            )
            yield res_ip

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.media-body a')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "link": link.css("a").attrib["href"]
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "link": "https://www.jsc.gov.jo" + link.css("a").attrib["href"],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_category += 1



class carcArticleSpider(scrapy.Spider):
    name = "carcArticleSpider"
    allowed_domains = ["carc.gov.jo"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": self.start_urls,
                }
            )
            yield res_ip

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                llinks = []
                data = response.css('.views-field-title a')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        elif link.css("a").attrib["href"] not in llinks:
                            llinks.append(link.css("a").attrib["href"])
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title": link.css('a::text').get().strip() if link.css('a::text') else 'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "link":link.css('a').attrib['href'] if link.css('a') else 'na',
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title": link.css('a::text').get().strip() if link.css('a::text') else 'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "link": "https://carc.gov.jo" + link.css('a').attrib['href'] if link.css('a') else 'na',
                                       }
                    except:
                        pass

        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_category += 1



class fbrSpider(scrapy.Spider):
    name = "fbrSpider"
    allowed_domains = ["fbr.gov.pk"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": self.start_urls,
                }
            )
            yield res_ip

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.caption')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "link": link.css("a").attrib["href"]
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "link": "https://www.fbr.gov.pk" + link.css("a").attrib["href"],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_category += 1




class mofaSpider(scrapy.Spider):
    name = "mofaSpider"
    allowed_domains = ["mofa.gov.qa"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": self.start_urls,
                }
            )
            yield res_ip

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.inner-news-header')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "link": link.css("a").attrib["href"]
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "link": "https://www.mofa.gov.qa" + link.css("a").attrib["href"],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_category += 1



class gdsPMSpider(scrapy.Spider):
    name = "gdsPMSpider"
    allowed_domains = ["gds.gov.iq"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": self.start_urls,
                }
            )
            yield res_ip

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.title a')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "link": link.css("a").attrib["href"]
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "link": "https://gds.gov.iq" + link.css("a").attrib["href"],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_category += 1




class moenSpider(scrapy.Spider):
    name = "moenSpider"
    allowed_domains = ["moen.gov.iq"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": self.start_urls,
                }
            )
            yield res_ip

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('#itemContainer a')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "link": link.css("a").attrib["href"]
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "link": "https://moen.gov.iq" + link.css("a").attrib["href"],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_category += 1



class mofaIQSpider(scrapy.Spider):
    name = "mofaIQSpider"
    allowed_domains = ["mofa.gov.iq"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": self.start_urls,
                }
            )
            yield res_ip

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('a.wp-block-latest-posts__post-title')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "link": link.css("a").attrib["href"]
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "link": "https://mofa.gov.iq" + link.css("a").attrib["href"],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_category += 1



class moj2Spider(scrapy.Spider):
    name = "moj2Spider"
    allowed_domains = ["moj.gov.iq"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": self.start_urls,
                }
            )
            yield res_ip

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.more_btn')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "link": link.css("a").attrib["href"]
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "link": "https://moj.gov.iq" + link.css("a").attrib["href"],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_category += 1




class tdapSpider(scrapy.Spider):
    name = "tdapSpider"
    allowed_domains = ["tdap.gov.pk"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": self.start_urls,
                }
            )
            yield res_ip

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('tbody tr')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "https://" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title": link.css('td')[0].css('::text').get() if link.css('td')[0] else 'na',
                                       "excerpt": 'na',
                                       "published_date":'na',
                                       "link": link.css('td')[1].css('a').attrib['href'],
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title": link.css('td')[0].css('::text').get() if link.css('td')[0] else 'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "link": "https://tdap.gov.pk" + link.css('td')[1].css('a').attrib['href'],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_category += 1



class trcSpider(scrapy.Spider):
    name = "trcSpider"
    allowed_domains = ["trc.gov.jo"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": self.start_urls,
                }
            )
            yield res_ip

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.js-readMore-item')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "https://" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title": link.css('span::text').get(),
                                       "excerpt": 'na',
                                       "published_date":'na',
                                       "pdf_url": link.css("a").attrib["href"],
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title": link.css('span::text').get(),
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": "https://www.trc.gov.jo" + str(link.css("a").attrib["href"])[2:],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_category += 1



class pbsSpider(scrapy.Spider):
    name = "pbsSpider"
    allowed_domains = ["pbs.gov.pk"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": self.start_urls,
                }
            )
            yield res_ip

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.field-content a')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "https://" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title": link.css('a::text').get(),
                                       "excerpt": 'na',
                                       "published_date":'na',
                                       "link": link.css("a").attrib["href"],
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title": link.css('a::text').get(),
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "link": "https://www.pbs.gov.pk" + str(link.css("a").attrib["href"]),
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_category += 1



class qatarchamberSpider(scrapy.Spider):
    name = "qatarchamberSpider"
    allowed_domains = ["qatarchamber.com"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": self.start_urls,
                }
            )
            yield res_ip

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.wpb_wrapper').css('ul').css('li').css('a')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "https://" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title": link.css('strong::text').get() if link.css('strong::text') else  'na',
                                       "excerpt": 'na',
                                       "published_date":'na',
                                       "link": link.css("a").attrib["href"],
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title": link.css('strong::text').get() if link.css('strong::text') else  'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "link": "https://www.qatarchamber.com" + str(link.css("a").attrib["href"]),
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_category += 1



class ModPkSpider(scrapy.Spider):
    name = "ModPkSpider"
    allowed_domains = ["mod.gov.pk"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": self.start_urls,
                }
            )
            yield res_ip

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('tbody tr')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "https://" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title": link.css('td')[0].css('::text').get() if link.css('td')[0] else  'na',
                                       "excerpt": 'na',
                                       "published_date":link.css('td')[1].css('::text').get() if link.css('td')[0] else  'na',
                                       "link": link.css('td')[2].css('a').attrib['href'],
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title": link.css('td')[0].css('::text').get() if link.css('td')[0] else  'na',
                                       "excerpt": 'na',
                                       "published_date": link.css('td')[1].css('::text').get() if link.css('td')[0] else  'na',
                                       "link": "https://mod.gov.pk" + link.css('td')[2].css('a').attrib['href'],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_category += 1




class PemraPkSpider(scrapy.Spider):
    name = "PemraPkSpider"
    allowed_domains = ["pemra.gov.pk"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                headers={"X-Crawlera-Region": "US"},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": self.start_urls,
                }
            )
            yield res_ip

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('tbody tr')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title": link.css('td a').css('::text').get() if link.css('td a') else  'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "link": link.css('td a').attrib['href'],
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title": link.css('td a').css('::text').get() if link.css('td a::text') else  'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "link": "https://pemra.gov.pk" + link.css('td a').attrib['href'],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_category += 1



class fiaPkSpider(scrapy.Spider):
    name = "fiaPkSpider"
    allowed_domains = ["fia.gov.pk"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": self.start_urls,
                }
            )
            yield res_ip

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.how-to-complain li')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title": link.css('a::text').get().strip() if link.css('a') else  'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "link": link.css('a').attrib['href'],
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title": link.css('a::text').get().strip() if link.css('a') else  'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "link": "https://www.fia.gov.pk" + link.css('a').attrib['href'],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_category += 1


class MoibSpider(scrapy.Spider):
    name = "MoibSpider"
    allowed_domains = ["www.moib.gov.pk"]
    not_allowed_keyword = []
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                llinks = []

                try:
                    data = response.css('li')
                except:
                    data = []
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        elif link.css("a").attrib["href"] not in llinks and "pdf" in link.css("a").attrib["href"]:
                            llinks.append(link.css("a").attrib["href"])
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": 'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": link.css('a').attrib['href'] if link.css('a') else 'na'
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": 'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": "http://www.moib.gov.pk" + link.css('a').attrib['href'] if link.css(
                                           'a') else 'na'
                                       }
                    except:
                        pass
        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1




class Jsc2Spider(scrapy.Spider):
    name = "Jsc2Spider"
    allowed_domains = ["jsc.gov.jo"]
    not_allowed_keyword = []
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                llinks = []
                try:
                    data = response.css('.media-body')
                except:
                    data = []
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        elif link.css("a").attrib["href"] not in llinks:
                            llinks.append(link.css("a").attrib["href"])
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title": link.css('a::text').get().strip(),
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": link.css('a').attrib['href']
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title": link.css('a::text').get().strip(),
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": "https://www.jsc.gov.jo" + link.css('a').attrib[
                                           'href']
                                       }
                    except:
                        pass
        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1




class StzaArticleSpider(scrapy.Spider):
    name = "StzaArticleSpider"
    allowed_domains = ["stza.gov.pk"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                llinks =[]
                data = response.css('.nt_rounded_btn,.nt_rounded_btn,.elementor-button-wrapper')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        elif link.css("a").attrib["href"] not in llinks:
                            llinks.append(link.css("a").attrib["href"])
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title": 'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "link": link.css('a').attrib['href'] if link.css('a') else 'na'
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title": 'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "link": "https://www.stza.gov.pk" + link.css('a').attrib['href'] if link.css(
                                           'a') else 'na'
                                       }
                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class publicworksSpider(scrapy.Spider):
    name = "publicworksSpider"
    allowed_domains = ["publicworks.gov.za"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"proxy": proxy, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.press-release-content li')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[
                                                                                                       :-1],
                                       "title": link.css('span::text').get().strip() if link.css('span::text') else 'na',
                                       "excerpt": 'na',
                                       "published_date": link.css('li::text').get().strip() if link.css('li::text') else 'na',
                                       "link": link.css("a").attrib["href"].replace(' ', '%20')
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[
                                                                                                       :-1],
                                       "title": link.css('span::text').get().strip() if link.css('span::text') else 'na',
                                       "excerpt": 'na',
                                       "published_date": link.css('li::text').get().strip() if link.css('li::text') else 'na',
                                       "link": "https://www.publicworks.gov.za/" + link.css("a").attrib["href"].replace(
                                           ' ', '%20')
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1



class dmrSpider(scrapy.Spider):
    name = "dmrSpider"
    allowed_domains = ["dmr.gov.za"]
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

        logging.info(
            {
                "proxy": "1",
                "clean_url": self.allowed_domains[0],
                "link": self.start_urls,
            }
        )

    def parse(self, response):

        if str(response.status) == "200":
            if response.css("body"):
                data = response.css(".BlogMoreLink")

                for link in data:
                    try:
                        if "http" in str(link.css("a").attrib["href"]):
                            yield {"clean_url": self.allowed_domains[0],
                                   "base_url": response.url,
                                   "link": link.css("a").attrib["href"]
                                   }
                        else:
                            yield {"clean_url": self.allowed_domains[0],
                                   "base_url": response.url,
                                   "link": "https://www.dmr.gov.za" + link.css("a").attrib["href"]
                                   }

                    except:
                        pass

        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1


class dircoSpider(scrapy.Spider):
    name = "dircoSpider"
    allowed_domains = ["dirco.gov.za"]
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

        logging.info(
            {
                "proxy": "1",
                "clean_url": self.allowed_domains[0],
                "link": self.start_urls,
            }
        )

    def parse(self, response):

        if str(response.status) == "200":
            if response.css("body"):
                data = response.css(".body .body .body a")

                for link in data:
                    try:
                        if "http" in str(link.css("a").attrib["href"]):
                            yield {"clean_url": self.allowed_domains[0],
                                   "base_url": response.url,
                                   "link": link.css("a").attrib["href"]
                                   }
                        else:
                            yield {"clean_url": self.allowed_domains[0],
                                   "base_url": response.url,
                                   "link": "http://www.dirco.gov.za/docs/" + link.css("a").attrib["href"]
                                   }

                    except:
                        pass

        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1


class dpeSpider(scrapy.Spider):
    name = "dpeSpider"
    allowed_domains = ["dpe.gov.za"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('h5.entry_title a')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "https://" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[
                                                                                                       :-1],
                                       "title": 'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "link": link.css("a").attrib["href"],

                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[
                                                                                                       :-1],
                                       "title": 'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "link": "https://dpe.gov.za" + link.css("a").attrib["href"],

                                       }

                    except:
                        pass


class transportSpider(scrapy.Spider):
    name = "transportSpider"
    allowed_domains = ["transport.gov.za"]
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

        logging.info(
            {
                "proxy": "1",
                "clean_url": self.allowed_domains[0],
                "link": self.start_urls,
            }
        )

    def parse(self, response):

        if str(response.status) == "200":
            if response.css("body"):
                data = response.css("#buttonread a")

                for link in data:
                    try:
                        if "http" in str(link.css("a").attrib["href"]):
                            yield {"clean_url": self.allowed_domains[0],
                                   "base_url": response.url,
                                   "link": link.css("a").attrib["href"]
                                   }
                        else:
                            yield {"clean_url": self.allowed_domains[0],
                                   "base_url": response.url,
                                   "link": "https://www.transport.gov.za" + link.css("a").attrib["href"]
                                   }

                    except:
                        pass

        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1


class dhetMidSpeechesSpider(scrapy.Spider):
    name = "dhetMidSpeechesSpider"
    allowed_domains = ["dhet.gov.za"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('tr.ms-rteTableOddRow-default')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": 'na',
                                       "excerpt": link.css('span::text').get().strip() if link.css(
                                           'span::text') else 'na',
                                       "published_date": link.css('a::text').get().strip(),
                                       "link": link.css("a").attrib["href"].replace('..', '')
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": 'na',
                                       "excerpt": link.css('span::text').get().strip() if link.css(
                                           'span::text') else 'na',
                                       "published_date": link.css('a::text').get().strip(),
                                       "link": "https://www.dhet.gov.za" + link.css("a").attrib["href"].replace('..',
                                                                                                                '')
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class dhetMediaReleaseSpider(scrapy.Spider):
    name = "dhetMediaReleaseSpider"
    allowed_domains = ["dhet.gov.za"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.ms-rteThemeForeColor-5-0 a')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": 'na',
                                       "excerpt": link.css('a::text').get().strip() if link.css('a::text') else 'na',
                                       "published_date": 'na',
                                       "link": link.css("a").attrib["href"].replace('..', '')
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": 'na',
                                       "excerpt": link.css('a::text').get().strip() if link.css('a::text') else 'na',
                                       "published_date": 'na',
                                       "link": "https://www.dhet.gov.za" + link.css("a").attrib["href"].replace('..',
                                                                                                                '')
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class healthSpeechesSpider(scrapy.Spider):
    name = "healthSpeechesSpider"
    allowed_domains = ["health.gov.za"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('tbody.row-hover').css('tr')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[
                                                                                                       :-1],
                                       "title": 'na',
                                       "excerpt": link.css('td.column-1::text').get().strip() if link.css(
                                           'td.column-1::text') else 'na',
                                       "published_date": 'na',
                                       "link": link.css("td.column-2").css('a').attrib["href"].replace('..', '')
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[
                                                                                                       :-1],
                                       "title": 'na',
                                       "excerpt": link.css('td.column-1::text').get().strip() if link.css(
                                           'td.column-1::text') else 'na',
                                       "published_date": 'na',
                                       "link": "https://www.health.gov.za" + link.css("td.column-2").css('a').attrib[
                                           "href"].replace('..', '')
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class healthPressStatementSpider(scrapy.Spider):
    name = "healthPressStatementSpider"
    allowed_domains = ["health.gov.za"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('tbody.row-hover').css('tr')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[
                                                                                                       :-1],
                                       "title": 'na',
                                       "excerpt": link.css('td.column-1::text').get().strip() if link.css(
                                           'td.column-1::text') else 'na',
                                       "published_date": 'na',
                                       "link": link.css("td.column-2").css('a').attrib["href"].replace('..', '')
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[
                                                                                                       :-1],
                                       "title": 'na',
                                       "excerpt": link.css('td.column-1::text').get().strip() if link.css(
                                           'td.column-1::text') else 'na',
                                       "published_date": 'na',
                                       "link": "https://www.health.gov.za" + link.css("td.column-2").css('a').attrib[
                                           "href"].replace('..', '')
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class labourSpider(scrapy.Spider):
    name = "labourSpider"
    allowed_domains = ["labour.gov.za"]
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

        logging.info(
            {
                "proxy": "1",
                "clean_url": self.allowed_domains[0],
                "link": self.start_urls,
            }
        )

    def parse(self, response):

        if str(response.status) == "200":
            if response.css("body"):
                data = response.css("tr.ms-itmhover")

                for link in data:
                    try:
                        if "http" in str(link.css("a").attrib["href"]):
                            yield {"clean_url": self.allowed_domains[0],
                                   "base_url": response.url,
                                   "link": link.css(".ms-vb2").css('a').attrib["href"]
                                   }
                        else:
                            yield {"clean_url": self.allowed_domains[0],
                                   "base_url": response.url,
                                   "link": "https://www.labour.gov.za" + link.css(".ms-vb2").css('a').attrib["href"]
                                   }

                    except:
                        pass

        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1


#
class dffeSpider(scrapy.Spider):
    name = "dffeSpider"
    allowed_domains = ["dffe.gov.za"]
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

        logging.info(
            {
                "proxy": "1",
                "clean_url": self.allowed_domains[0],
                "link": self.start_urls,
            }
        )

    def parse(self, response):

        if str(response.status) == "200":
            if response.css("body"):
                data = response.css("#block-views-speeches-block a,#block-views-media_releases-block a")

                for link in data:
                    try:
                        if "http" in str(link.css("a").attrib["href"]):
                            yield {"clean_url": self.allowed_domains[0],
                                   "base_url": response.url,
                                   "link": link.css("a").attrib["href"]
                                   }
                        else:
                            yield {"clean_url": self.allowed_domains[0],
                                   "base_url": response.url,
                                   "link": "https://www.dffe.gov.za" + link.css("a").attrib["href"]
                                   }

                    except:
                        pass

        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1


class thedticSpider(scrapy.Spider):
    name = "thedticSpider"
    allowed_domains = ["thedtic.gov.za"]
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

        logging.info(
            {
                "proxy": "1",
                "clean_url": self.allowed_domains[0],
                "link": self.start_urls,
            }
        )

    def parse(self, response):

        if str(response.status) == "200":
            if response.css("body"):
                data = response.css("#search_table a")

                for link in data:
                    try:
                        if "http" in str(link.css("a").attrib["href"]):
                            yield {"clean_url": self.allowed_domains[0],
                                   "base_url": response.url,
                                   "link": link.css("a").attrib["href"]
                                   }
                        else:
                            yield {"clean_url": self.allowed_domains[0],
                                   "base_url": response.url,
                                   "link": "http://www.thedtic.gov.za" + link.css("a").attrib["href"]
                                   }

                    except:
                        pass

        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1


class dcdtSpider(scrapy.Spider):
    name = "dcdtSpider"
    allowed_domains = ["dcdt.gov.za"]
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

        logging.info(
            {
                "proxy": "1",
                "clean_url": self.allowed_domains[0],
                "link": self.start_urls,
            }
        )

    def parse(self, response):

        if str(response.status) == "200":
            if response.css("body"):
                data = response.css(".list-title a")

                for link in data:
                    try:
                        if "http" in str(link.css("a").attrib["href"]):
                            yield {"clean_url": self.allowed_domains[0],
                                   "base_url": response.url,
                                   "link": link.css("a").attrib["href"]
                                   }
                        else:
                            yield {"clean_url": self.allowed_domains[0],
                                   "base_url": response.url,
                                   "link": "https://www.dcdt.gov.za" + link.css("a").attrib["href"]
                                   }

                    except:
                        pass

        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1


class educationArticleSpider(scrapy.Spider):
    name = "educationArticleSpider"
    allowed_domains = ["education.gov.za"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('#dnn_leftPane8 .Normal,#dnn_ContentPane .Normal')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[
                                                                                                       :-1],
                                       "title": 'na',
                                       "excerpt": link.css('a::text').get().strip() if link.css('a') else 'na',
                                       "published_date": 'na',
                                       "link": link.css("a").attrib["href"].replace('..', '')
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[
                                                                                                       :-1],
                                       "title": link.css(
                                           '.item.d-flex.align-items-center span::text').get().strip() if link.css(
                                           '.item.d-flex.align-items-center span') else 'na',
                                       "excerpt": link.css('a::text').get().strip() if link.css('a') else 'na',
                                       "published_date": 'na',
                                       "link": "https://www.education.gov.za" + link.css("a").attrib["href"].replace(
                                           '..', '')
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class dsbdArticleSpider(scrapy.Spider):
    name = "dsbdArticleSpider"
    allowed_domains = ["dsbd.gov.za"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('tbody tr a')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "https://" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[
                                                                                                       :-1],
                                       "title": link.css('a::text').get().strip() if link.css('a::text') else 'na',
                                       "excerpt": 'na',
                                       "published_date": response.css('.datetime::text').get() if response.css(
                                           '.datetime::text') else 'na',
                                       "link": link.css("a").attrib["href"],

                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[
                                                                                                       :-1],
                                       "title": link.css('a::text').get().strip() if link.css('a::text') else 'na',
                                       "excerpt": 'na',
                                       "published_date": response.css('.datetime::text').get() if response.css(
                                           '.datetime::text') else 'na',
                                       "link": "http://www.dsbd.gov.za" + link.css("a").attrib[
                                           "href"],

                                       }

                    except:
                        pass


class dsbdPublicationsSpider(scrapy.Spider):
    name = "dsbdPublicationsSpider"
    allowed_domains = ["dsbd.gov.za"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.views-col')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[
                                                                                                       :-1],
                                       "title": link.css('span.field-content::text').get().strip() if link.css(
                                           'span.field-content') else 'na',
                                       "excerpt": link.css('div.views-field-body').css('div.field-content').css(
                                           'p::text').get().strip() if link.css('div.views-field-body').css(
                                           'div.field-content').css('p::text') else 'na',
                                       "published_date": 'na',
                                       "link": link.css("a").attrib["href"].replace('..', '')
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[
                                                                                                       :-1],
                                       "title": link.css('span.field-content::text').get().strip() if link.css(
                                           'span.field-content') else 'na',
                                       "excerpt": link.css('div.views-field-body').css('div.field-content').css(
                                           'p::text').get().strip() if link.css('div.views-field-body').css(
                                           'div.field-content').css('p::text') else 'na',
                                       "published_date": 'na',
                                       "link": "https://tejarah.gov.om" + link.css("a").attrib["href"].replace('..', '')
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class sahpraArticleSpider(scrapy.Spider):
    name = "sahpraArticleSpider"
    allowed_domains = ["sahpra.org.za"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                categories_links = []
                res = response.css('h5.entry_title')
                for r in res:
                    try:
                        if "https://" in str(r.css("a").attrib["href"]) or "http://" in str(r.css("a").attrib["href"]):
                            categories_links.append(r.css("a").attrib["href"])
                        else:
                            categories_links.append(
                                "https://www.sahpra.org.za"
                                + r.css("a").attrib["href"]
                            )

                    except:
                        pass

                for i in set(categories_links):
                    try:
                        yield scrapy.Request(
                            i,
                            callback=self.article_links,
                            meta={"dont_retry": True, "download_timeout": cat_timeout, "base_url": response.url},
                            errback=handle_error,
                        )


                    except:
                        pass

    def article_links(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                base_url = response.meta.get('base_url')
                data = response.css('a.qbutton.medium.center.default')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "https://" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": base_url,
                                       "title": response.css('h2.entry_title::text').get().strip(),
                                       "excerpt": 'na',
                                       "published_date": response.css('.post_text').css(
                                           'strong::text').get().strip() if response.css('.post_text').css(
                                           'strong::text').get().strip() else 'na',
                                       "link": link.css("a").attrib["href"],

                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": base_url,
                                       "title": response.css('h2.entry_title::text').get().strip(),
                                       "excerpt": 'na',
                                       "published_date": response.css('.post_text').css(
                                           'strong::text').get().strip() if response.css('.post_text').css(
                                           'strong::text').get().strip() else 'na',
                                       "link": "https://www.sahpra.org.za" + link.css("a").attrib[
                                           "href"],

                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class ficSpider(scrapy.Spider):
    name = "ficSpider"
    allowed_domains = ["fic.gov.za"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.cc-pdf-imagecol+ td')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('a::text').get().strip() if link.css('a') else 'na',
                                       "excerpt": link.css('.cc-pdf-description::text').get().strip() if link.css(
                                           '.cc-pdf-description') else 'na',
                                       "published_date": link.css('.cc-pdf-date').css('i::text').get().strip(),
                                       "pdf_url": link.css("a").attrib["href"].replace(' ', '%20')
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('a::text').get().strip() if link.css('a') else 'na',
                                       "excerpt": link.css('.cc-pdf-description::text').get().strip() if link.css(
                                           '.cc-pdf-description') else 'na',
                                       "published_date": link.css('.cc-pdf-date').css('i::text').get().strip(),
                                       "pdf_url": "https://tejarah.gov.om" + link.css("a").attrib["href"].replace(' ',
                                                                                                                  '%20')
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class jseArticleSpider(scrapy.Spider):
    name = "jseArticleSpider"
    allowed_domains = ["jse.co.za"]
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

        logging.info(
            {
                "proxy": "1",
                "clean_url": self.allowed_domains[0],
                "link": self.start_urls,
            }
        )

    def parse(self, response):

        if str(response.status) == "200":
            if response.css("body"):
                data = response.css(".btn-inverse-primary")

                for link in data:
                    try:
                        if "http" in str(link.css("a").attrib["href"]):
                            yield {"clean_url": self.allowed_domains[0],
                                   "base_url": response.url,
                                   "link": link.css("a").attrib["href"]
                                   }
                        else:
                            yield {"clean_url": self.allowed_domains[0],
                                   "base_url": response.url,
                                   "link": "'https://www.jse.co.za" + link.css("a").attrib["href"]
                                   }

                    except:
                        pass

        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1


class ncrArticleSpider(scrapy.Spider):
    name = "ncrArticleSpider"
    allowed_domains = ["ncr.org.za"]
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

        logging.info(
            {
                "proxy": "1",
                "clean_url": self.allowed_domains[0],
                "link": self.start_urls,
            }
        )

    def parse(self, response):

        if str(response.status) == "200":
            if response.css("body"):
                data = response.css("#sp-component a")

                for link in data:
                    try:
                        if "http" in str(link.css("a").attrib["href"]):
                            yield {"clean_url": self.allowed_domains[0],
                                   "base_url": response.url,
                                   "link": link.css("a").attrib["href"]
                                   }
                        else:
                            yield {"clean_url": self.allowed_domains[0],
                                   "base_url": response.url,
                                   "link": "https://www.ncr.org.za" + link.css("a").attrib["href"]
                                   }

                    except:
                        pass

        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1


class ncrCircularsArticlesSpider(scrapy.Spider):
    name = "ncrCircularsArticlesSpider"
    allowed_domains = ["ncr.org.za"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('tbody').css('tr')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('td').css('a::text').get().strip() if link.css('td').css(
                                           'a') else 'na',
                                       "excerpt": 'na',
                                       "published_date": link.css('td::text').get(),
                                       "link": link.css('td').css('a').attrib["href"].replace(' ', '%20')
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('td').css('a::text').get().strip() if link.css('td').css(
                                           'a') else 'na',
                                       "excerpt": 'na',
                                       "published_date": link.css('td::text').get(),
                                       "link": "https://www.ncr.org.za" + link.css('td').css('a').attrib[
                                           "href"].replace(' ', '%20')
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


#
class nmdpraArticleSpider(scrapy.Spider):
    name = "nmdpraArticleSpider"
    allowed_domains = ["nmdpra.gov.ng"]
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

        logging.info(
            {
                "proxy": "1",
                "clean_url": self.allowed_domains[0],
                "link": self.start_urls,
            }
        )

    def parse(self, response):

        if str(response.status) == "200":
            if response.css("body"):
                data = response.css(".jet-listing-dynamic-link__link")

                for link in data:
                    try:
                        if "http" in str(link.css("a").attrib["href"]):
                            yield {"clean_url": self.allowed_domains[0],
                                   "base_url": response.url,
                                   "link": link.css("a").attrib["href"]
                                   }
                        else:
                            yield {"clean_url": self.allowed_domains[0],
                                   "base_url": response.url,
                                   "link": "https://www.nmdpra.gov.ng" + link.css("a").attrib["href"]
                                   }

                    except:
                        pass

        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1


class nuprcArticleSpider(scrapy.Spider):
    name = "nuprcArticleSpider"
    allowed_domains = ["nuprc.gov.ng"]
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

        logging.info(
            {
                "proxy": "1",
                "clean_url": self.allowed_domains[0],
                "link": self.start_urls,
            }
        )

    def parse(self, response):

        if str(response.status) == "200":
            if response.css("body"):
                data = response.css(".read-more a")

                for link in data:
                    try:
                        if "http" in str(link.css("a").attrib["href"]):
                            yield {"clean_url": self.allowed_domains[0],
                                   "base_url": response.url,
                                   "link": link.css("a").attrib["href"]
                                   }
                        else:
                            yield {"clean_url": self.allowed_domains[0],
                                   "base_url": response.url,
                                   "link": "https://www.nuprc.gov.ng" + link.css("a").attrib["href"]
                                   }

                    except:
                        pass

        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1


class nercArticleSpider(scrapy.Spider):
    name = "nercArticleSpider"
    allowed_domains = ["nerc.gov.ng"]
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

        logging.info(
            {
                "proxy": "1",
                "clean_url": self.allowed_domains[0],
                "link": self.start_urls,
            }
        )

    def parse(self, response):

        if str(response.status) == "200":
            if response.css("body"):
                data = response.css(".list-title a")

                for link in data:
                    try:
                        if "http" in str(link.css("a").attrib["href"]):
                            yield {"clean_url": self.allowed_domains[0],
                                   "base_url": response.url,
                                   "link": link.css("a").attrib["href"]
                                   }
                        else:
                            yield {"clean_url": self.allowed_domains[0],
                                   "base_url": response.url,
                                   "link": "https://nerc.gov.ng" + link.css("a").attrib["href"]
                                   }

                    except:
                        pass

        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1


class ngxgroupArticleSpider(scrapy.Spider):
    name = "ngxgroupArticleSpider"
    allowed_domains = ["ngxgroup.com"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.website-link')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[
                                                                                                       :-1],
                                       "title": link.css('a::text').get().strip() if link.css('a') else 'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "link": link.css("a").attrib["href"].replace('..', '')
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[
                                                                                                       :-1],
                                       "title": link.css('a::text').get().strip() if link.css('a') else 'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "link": "https://ngxgroup.com" + link.css("a").attrib["href"].replace('..', '')
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class cbnArticleSpider(scrapy.Spider):
    name = "cbnArticleSpider"
    allowed_domains = ["cbn.gov.ng"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.dbasetable td:nth-child(2)')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('a::text').get().strip() if link.css('a') else 'na',
                                       "excerpt": 'na',
                                       "published_date": link.css('#publishedDt::text').get().strip(),
                                       "link": link.css("a").attrib["href"].replace(' ', '%20')
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('a::text').get().strip() if link.css('a') else 'na',
                                       "excerpt": 'na',
                                       "published_date": link.css('#publishedDt::text').get().strip(),
                                       "link": "https://www.cbn.gov.ng" + link.css("a").attrib["href"].replace(' ',
                                                                                                               '%20')
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class ncdcArticleSpider(scrapy.Spider):
    name = "ncdcArticleSpider"
    allowed_domains = ["ncdc.gov.ng"]
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

        logging.info(
            {
                "proxy": "1",
                "clean_url": self.allowed_domains[0],
                "link": self.start_urls,
            }
        )

    def parse(self, response):

        if str(response.status) == "200":
            if response.css("body"):
                data = response.css("a.white-text")

                for link in data:
                    try:
                        if "http" in str(link.css("a").attrib["href"]):
                            yield {"clean_url": self.allowed_domains[0],
                                   "base_url": response.url,
                                   "link": link.css("a").attrib["href"]
                                   }
                        else:
                            yield {"clean_url": self.allowed_domains[0],
                                   "base_url": response.url,
                                   "link": "https://ncdc.gov.ng" + link.css("a").attrib["href"]
                                   }

                    except:
                        pass

        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1


class nccArticleSpider(scrapy.Spider):
    name = "nccArticleSpider"
    allowed_domains = ["ncc.gov.ng"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.page-header a')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css('a').attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css('a').attrib["href"],

                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www.ncc.gov.ng" + link.css('a').attrib["href"],

                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class parliamentSpider(scrapy.Spider):
    name = "parliamentSpider"
    allowed_domains = ["parliament.gov.za"]
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"proxy": proxy, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

        logging.info(
            {
                "proxy": "1",
                "clean_url": self.allowed_domains[0],
                "link": self.start_urls,
            }
        )

    def parse(self, response):

        if str(response.status) == "200":
            if response.css("body"):
                data = response.css("#archiveGoesHere a")

                for link in data:
                    try:
                        if "http" in str(link.css("a").attrib["href"]):
                            yield {"clean_url": self.allowed_domains[0],
                                   "base_url": response.url,
                                   "link": link.css("a").attrib["href"]
                                   }
                        else:
                            yield {"clean_url": self.allowed_domains[0],
                                   "base_url": response.url,
                                   "link": "https://www.parliament.gov.za" + link.css("a").attrib["href"]
                                   }

                    except:
                        pass

        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1


class justiceArticleSpider(scrapy.Spider):
    name = "justiceArticleSpider"
    allowed_domains = ["justice.gov.ng"]
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"proxy": proxy, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

        logging.info(
            {
                "proxy": "1",
                "clean_url": self.allowed_domains[0],
                "link": self.start_urls,
            }
        )

    def parse(self, response):

        if str(response.status) == "200":
            if response.css("body"):
                data = response.css("a.read_more")

                for link in data:
                    try:
                        if "http" in str(link.css("a").attrib["href"]):
                            yield {"clean_url": self.allowed_domains[0],
                                   "base_url": response.url,
                                   "link": link.css("a").attrib["href"]
                                   }
                        else:
                            yield {"clean_url": self.allowed_domains[0],
                                   "base_url": response.url,
                                   "link": "https://justice.gov.ng" + link.css("a").attrib["href"]
                                   }

                    except:
                        pass

        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1


class nnraSpider(scrapy.Spider):
    name = "nnraSpider"
    allowed_domains = ["nnra.gov.ng"]
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"proxy": proxy, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

        logging.info(
            {
                "proxy": "1",
                "clean_url": self.allowed_domains[0],
                "link": self.start_urls,
            }
        )

    def parse(self, response):

        if str(response.status) == "200":
            if response.css("body"):
                data = response.css(".readmore")

                for link in data:
                    try:
                        if "http" in str(link.css("a").attrib["href"]):
                            yield {"clean_url": self.allowed_domains[0],
                                   "base_url": response.url,
                                   "link": link.css("a").attrib["href"]
                                   }
                        else:
                            yield {"clean_url": self.allowed_domains[0],
                                   "base_url": response.url,
                                   "link": "https://nnra.gov.ng/nnra/" + link.css("a").attrib["href"]
                                   }

                    except:
                        pass

        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1


class labourNGSpider(scrapy.Spider):
    name = "labourNGSpider"
    allowed_domains = ["labour.gov.ng"]
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"proxy": proxy, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

        logging.info(
            {
                "proxy": "1",
                "clean_url": self.allowed_domains[0],
                "link": self.start_urls,
            }
        )

    def parse(self, response):

        if str(response.status) == "200":
            if response.css("body"):
                data = response.css("a.btn-primary")

                for link in data:
                    try:
                        if "http" in str(link.css("a").attrib["href"]):
                            yield {"clean_url": self.allowed_domains[0],
                                   "base_url": response.url,
                                   "link": link.css("a").attrib["href"]
                                   }
                        else:
                            yield {"clean_url": self.allowed_domains[0],
                                   "base_url": response.url,
                                   "link": "https://labour.gov.ng" + link.css("a").attrib["href"]
                                   }

                    except:
                        pass

        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1


class powerSpider(scrapy.Spider):
    name = "powerSpider"
    allowed_domains = ["power.gov.ng"]
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"proxy": proxy, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

        logging.info(
            {
                "proxy": "1",
                "clean_url": self.allowed_domains[0],
                "link": self.start_urls,
            }
        )

    def parse(self, response):

        if str(response.status) == "200":
            if response.css("body"):
                data = response.css("a.details-type-link")

                for link in data:
                    try:
                        if "http" in str(link.css("a").attrib["href"]):
                            yield {"clean_url": self.allowed_domains[0],
                                   "base_url": response.url,
                                   "link": link.css("a").attrib["href"]
                                   }
                        else:
                            yield {"clean_url": self.allowed_domains[0],
                                   "base_url": response.url,
                                   "link": "https://www.power.gov.ng" + link.css("a").attrib["href"]
                                   }

                    except:
                        pass

        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_article_links += 1


class NsiaArticleSpider(scrapy.Spider):
    name = "NsiaArticleSpider"
    allowed_domains = ["nsia.com.ng"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                headers={"X-Crawlera-Region": "IN"},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.elementor-heading-title a')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[
                                                                                                       :-1],
                                       "link": link.css("a").attrib["href"].replace('..', '')
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[
                                                                                                       :-1],
                                       "link": "https://nsia.com.ng" + link.css("a").attrib["href"].replace('..', '')
                                       }


                    except:
                        pass


        else:
            while self.check_ip_category < 5:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class NnraPdfSpider(scrapy.Spider):
    name = "NnraPdfSpider"
    allowed_domains = ["nnra.gov.ng"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                categories_links = []
                res = response.css('.pagin a')
                for r in res:
                    try:
                        if "http" in str(r.css("a").attrib["href"]):
                            categories_links.append(r.css("a").attrib["href"])
                        else:
                            categories_links.append(f"https://nnra.gov.ng/nnra/" + r.css("a").attrib["href"])

                    except:
                        pass

                for i in set(categories_links):
                    try:
                        yield scrapy.Request(
                            i,
                            callback=self.article_links,
                            meta={"dont_retry": True, "download_timeout": cat_timeout,"base_url": response.url if not response.url.endswith('/') else response.url[:-1]},
                            errback=handle_error,
                        )
                    except:
                        pass
                llinks = []
                data = response.css('.content-post')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        elif link.css("a").attrib["href"] not in llinks:
                            llinks.append(link.css("a").attrib["href"])
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[
                                                                                                       :-1],
                                       "title": link.css('a::text').get().strip() if link.css('a::text') else 'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": link.css('a').attrib['href'] if link.css('a') else 'na'
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[
                                                                                                       :-1],
                                       "title": link.css('a::text').get().strip() if link.css('a::text') else 'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": "https://nnra.gov.ng/nnra/" + link.css('a').attrib[
                                           'href'] if link.css(
                                           'a') else 'na'
                                       }
                    except:
                        pass



    def article_links(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                base_url = response.meta.get('base_url')
                llinks = []
                data = response.css('.content-post')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        elif link.css("a").attrib["href"] not in llinks:
                            llinks.append(link.css("a").attrib["href"])
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": base_url,
                                       "title": link.css('a::text').get().strip() if link.css('a::text') else 'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": link.css('a').attrib['href'] if link.css('a') else 'na'
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": base_url,
                                       "title": link.css('a::text').get().strip() if link.css('a::text') else 'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": "https://nnra.gov.ng/nnra/" + link.css('a').attrib[
                                           'href'] if link.css(
                                           'a') else 'na'
                                       }
                    except:
                        pass



class JusticePdfSpider(scrapy.Spider):
    name = "JusticePdfSpider"
    allowed_domains = ["justice.gov.ng"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                llinks = []
                data = response.css('h2 a')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        elif link.css("a").attrib["href"] not in llinks:
                            llinks.append(link.css("a").attrib["href"])
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[
                                                                                                       :-1],
                                       "title": link.css('a::text').get().strip() if link.css('a::text') else 'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": link.css('a').attrib['href'] if link.css('a') else 'na'
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[
                                                                                                       :-1],
                                       "title": link.css('a::text').get().strip() if link.css('a::text') else 'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": "https://justice.gov.ng" + link.css('a').attrib['href'] if link.css(
                                           'a') else 'na'
                                       }
                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class ParliamentSpecialSpider(scrapy.Spider):
    name = "ParliamentSpecialSpider"
    allowed_domains = ['parliament.gov.za']
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def parse(self, response):
        for i in range(0, 430, 10):
            cookies = {
                'cookiesession1': '678A3E0EA1BCB5C97A8130AAA8AFDB93',
                '_ga': 'GA1.3.11888042.1674199398',
                '_gid': 'GA1.3.1680589137.1674199398',
                '_gat': '1',
                '_gat_gtag_UA_1123456_78': '1',
                'october_session': 'eyJpdiI6IlFzdjRmTGVRS2VscFRwcjNLNmVRN2c9PSIsInZhbHVlIjoiT2k4Q0FJUGdcLzQzQmJoZkIxWko3SWU0eDhxM3RNdE42b3YzVzh6bVwvVjZcL0YzNE51b1NmOThEdVFFT3h5dm1MZWpWbGdnSWNXUWNBWU9lS1ZEYzlZVGc9PSIsIm1hYyI6ImMyZDgzOTNkYTlmNWM4ZTcwNjI0YmU4ZDRkMGI5NjZiYWQ1NzY5NGQwM2M0ODNjYmY0MjUwMWIwYWFlM2E4YmUifQ%3D%3D',
            }

            headers = {
                'Accept': 'application/json, text/javascript, */*; q=0.01',
                'Accept-Language': 'en-GB,en-US;q=0.9,en;q=0.8',
                'Connection': 'keep-alive',
                # 'Cookie': 'cookiesession1=678A3E0EA1BCB5C97A8130AAA8AFDB93; _ga=GA1.3.11888042.1674199398; _gid=GA1.3.1680589137.1674199398; _gat=1; _gat_gtag_UA_1123456_78=1; october_session=eyJpdiI6IlFzdjRmTGVRS2VscFRwcjNLNmVRN2c9PSIsInZhbHVlIjoiT2k4Q0FJUGdcLzQzQmJoZkIxWko3SWU0eDhxM3RNdE42b3YzVzh6bVwvVjZcL0YzNE51b1NmOThEdVFFT3h5dm1MZWpWbGdnSWNXUWNBWU9lS1ZEYzlZVGc9PSIsIm1hYyI6ImMyZDgzOTNkYTlmNWM4ZTcwNjI0YmU4ZDRkMGI5NjZiYWQ1NzY5NGQwM2M0ODNjYmY0MjUwMWIwYWFlM2E4YmUifQ%3D%3D',
                'Referer': 'https://www.parliament.gov.za/mandates?sorts[date]=-1',
                'Sec-Fetch-Dest': 'empty',
                'Sec-Fetch-Mode': 'cors',
                'Sec-Fetch-Site': 'same-origin',
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36',
                'X-Requested-With': 'XMLHttpRequest',
                'sec-ch-ua': '"Not?A_Brand";v="8", "Chromium";v="108", "Google Chrome";v="108"',
                'sec-ch-ua-mobile': '?0',
                'sec-ch-ua-platform': '"macOS"',
            }

            params = {
                'queries[type]': 'FIN_MAN',
                'sorts[date]': '-1',
                'page': '1',
                'perPage': '10',
                'offset': f'{i}',
            }

            responses = rq.get('https://www.parliament.gov.za/docsjson', params=params, cookies=cookies,
                               headers=headers, verify=False)

            result = responses.json()['records']
            for i in result:
                yield {"clean_url": self.allowed_domains[0],
                       "base_url": self.start_urls[0],
                       "title": i['name'],
                       "excerpt": 'na',
                       "published_date": i['date'],
                       'pdf_url': "https://www.parliament.gov.za/storage/app/media/Docs/" + i['file_location'],
                       }




class JusticeSpider(scrapy.Spider):
    name = "JusticeSpider"
    allowed_domains = ["www.justice.gov.za"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('hr+ ul a')

                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if ".pdf" in link.css("a").attrib["href"]:
                                if "http" in str(link.css("a").attrib["href"]):
                                    yield {"clean_url": self.allowed_domains[0],
                                           "base_url": response.url,
                                           "title": link.css('a::text').get().strip() if link.css('a') else 'na',
                                           "excerpt": 'na',
                                           "published_date": 'na',
                                           "pdf_url": link.css("a").attrib["href"].replace('..', '')
                                           }


                                else:
                                    yield {"clean_url": self.allowed_domains[0],
                                           "base_url": response.url,
                                           "title": link.css('::text').get().strip() if link.css('a') else 'na',
                                           "excerpt": 'na',
                                           "published_date": 'na',
                                           "pdf_url": "https://www.justice.gov.za/legislation/notices/" +
                                                      link.css("a").attrib["href"].replace('..', '')
                                           }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


# 2
class Justice2Spider(scrapy.Spider):
    name = "Justice2Spider"
    allowed_domains = ["www.justice.gov.za"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('ul ul a , strong+ a')

                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if ".pdf" in link.css("a").attrib["href"]:
                                if "http" in str(link.css("a").attrib["href"]):
                                    yield {"clean_url": self.allowed_domains[0],
                                           "base_url": response.url,
                                           "title": link.css('a::text').get().strip() if link.css('a') else 'na',
                                           "excerpt": 'na',
                                           "published_date": 'na',
                                           "pdf_url": link.css("a").attrib["href"].replace('..', '')
                                           }


                                else:
                                    yield {"clean_url": self.allowed_domains[0],
                                           "base_url": response.url,
                                           "title": link.css('::text').get().strip() if link.css('a') else 'na',
                                           "excerpt": 'na',
                                           "published_date": 'na',
                                           "pdf_url": "https://www.justice.gov.za/legislation/acts/" +
                                                      link.css("a").attrib["href"].replace('..', '')
                                           }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


# 3
class TransporPDFtSpider(scrapy.Spider):
    name = "TransporPDFtSpider"
    allowed_domains = ["www.transport.gov.za"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css("#p_p_id_56_INSTANCE_nagdmANIxVQ1_ a")

                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if ".pdf" in link.css("a").attrib["href"]:
                                if "http" in str(link.css("a").attrib["href"]):
                                    yield {"clean_url": self.allowed_domains[0],
                                           "base_url": response.url,
                                           "title": link.css('a::text').get().strip() if link.css('a') else 'na',
                                           "excerpt": 'na',
                                           "published_date": 'na',
                                           "pdf_url": link.css("a").attrib["href"].replace('..', '')
                                           }


                                else:
                                    yield {"clean_url": self.allowed_domains[0],
                                           "base_url": response.url,
                                           "title": link.css('::text').get().strip() if link.css('a') else 'na',
                                           "excerpt": 'na',
                                           "published_date": 'na',
                                           "pdf_url": "https://www.transport.gov.za" + link.css("a").attrib[
                                               "href"].replace('..', '')
                                           }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


# 4
class HealthSpider(scrapy.Spider):
    name = "HealthSpider"
    allowed_domains = ["www.health.gov.za"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):

                data = response.css("tr")

                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if ".pdf" in link.css("a").attrib["href"]:
                                if "http" in str(link.css("a").attrib["href"]):
                                    yield {"clean_url": self.allowed_domains[0],
                                           "base_url": response.url if not response.url.endswith('/') else response.url[
                                                                                                           :-1],
                                           "title": link.css('.column-1::text').get().strip() if link.css(
                                               'a') else 'na',
                                           "excerpt": 'na',
                                           "published_date": 'na',
                                           "pdf_url": link.css("td a").attrib["href"].replace('..', '')
                                           }


                                else:
                                    yield {"clean_url": self.allowed_domains[0],
                                           "base_url": response.url if not response.url.endswith('/') else response.url[
                                                                                                           :-1],
                                           "title": link.css('.column-1::text').get().strip() if link.css(
                                               'a') else 'na',
                                           "excerpt": 'na',
                                           "published_date": 'na',
                                           "pdf_url": "https://www.health.gov.za" + link.css("td a").attrib[
                                               "href"].replace('..', '')
                                           }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


# 5
class Health2Spider(scrapy.Spider):
    name = "Health2Spider"
    allowed_domains = ["www.health.gov.za"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):

                data = response.css(".row-6")
                data += response.css(".row-7")
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if ".pdf" in link.css("a").attrib["href"]:
                                if "http" in str(link.css("a").attrib["href"]):
                                    yield {"clean_url": self.allowed_domains[0],
                                           "base_url": response.url if not response.url.endswith('/') else response.url[
                                                                                                           :-1],
                                           "title": link.css('a::text').get().strip() if link.css('a') else 'na',
                                           "excerpt": 'na',
                                           "published_date": 'na',
                                           "pdf_url": link.css("a").attrib["href"].replace('..', '')
                                           }


                                else:
                                    yield {"clean_url": self.allowed_domains[0],
                                           "base_url": response.url if not response.url.endswith('/') else response.url[
                                                                                                           :-1],
                                           "title": link.css('a::text').get().strip() if link.css('a') else 'na',
                                           "excerpt": 'na',
                                           "published_date": 'na',
                                           "pdf_url": "https://www.health.gov.za" + link.css("a").attrib[
                                               "href"].replace('..', '')
                                           }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


# 6
class ThedticPdfSpider(scrapy.Spider):
    name = "ThedticPdfSpider"
    allowed_domains = ["www.thedtic.gov.za"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):

                data = response.css(".clear a")

                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:

                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[
                                                                                                       :-1],
                                       "title": link.css('a::text').get().strip() if link.css('a::text') else 'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": link.css("a").attrib["href"].replace('..', '')
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[
                                                                                                       :-1],
                                       "title": link.css('a::text').get().strip() if link.css('a::text') else 'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": "" + link.css("a").attrib["href"].replace('..', '')
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


# 7 10.01
class DsbdSpider(scrapy.Spider):
    name = "DsbdSpider"
    allowed_domains = ["www.dsbd.gov.za"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css("#block-zircon-content a")
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if ".pdf" in link.css("a").attrib["href"]:
                                if "http" in str(link.css("a").attrib["href"]):
                                    yield {"clean_url": self.allowed_domains[0],
                                           "base_url": response.url,
                                           "title": link.css('a::text').get().strip() if link.css('a') else 'na',
                                           "excerpt": 'na',
                                           "published_date": 'na',
                                           "pdf_url": link.css("a").attrib["href"].replace('..', '')
                                           }


                                else:
                                    yield {"clean_url": self.allowed_domains[0],
                                           "base_url": response.url,
                                           "title": link.css('a::text').get().strip() if link.css('a') else 'na',
                                           "excerpt": 'na',
                                           "published_date": 'na',
                                           "pdf_url": "" + link.css("a").attrib["href"].replace('..', '')
                                           }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


# 8
class TreasurySpider(scrapy.Spider):
    name = "TreasurySpider"
    allowed_domains = ["www.treasury.org.za"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):

                data = response.css(".BasicText")
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if ".pdf" in link.css("a").attrib["href"]:
                                if "http" in str(link.css("a").attrib["href"]):
                                    yield {"clean_url": self.allowed_domains[0],
                                           "base_url": response.url,
                                           "title": link.css('a::text').get().strip() if link.css('a') else 'na',
                                           "excerpt": 'na',
                                           "published_date": 'na',
                                           "pdf_url": link.css("a").attrib["href"].replace('..', '')
                                           }


                                else:
                                    yield {"clean_url": self.allowed_domains[0],
                                           "base_url": response.url,
                                           "title": link.css('a::text').get().strip() if link.css('a') else 'na',
                                           "excerpt": 'na',
                                           "published_date": 'na',
                                           "pdf_url": "https://www.treasury.gov.za" + link.css("a").attrib[
                                               "href"].replace('..', '')
                                           }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


# 9
class SarsSpider(scrapy.Spider):
    name = "SarsSpider"
    allowed_domains = ["www.sars.org.za"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):

                data = response.css("tr")
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:

                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title": link.css('a::text').get().strip() if link.css('a::text') else 'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": link.css("li a").attrib["href"].replace('..', '')
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title": link.css('a::text').get().strip() if link.css('a::text') else 'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": "https://www.sars.gov.za" + link.css("li a").attrib["href"].replace(
                                           '..', '')
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


# 10
class EducationSpider(scrapy.Spider):
    name = "EducationSpider"
    allowed_domains = ["www.education.gov.za"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css("tr")
                llinks = []
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if link.css("a").attrib["href"] not in llinks:
                                llinks.append(link.css('a').attrib['href'])
                                if "http" in str(link.css("a").attrib["href"]):
                                    yield {"clean_url": self.allowed_domains[0],
                                           "base_url": response.url,
                                           "title": link.css('.TitleCell a::text').get().strip() if link.css(
                                               'a') else 'na',
                                           "excerpt": 'na',
                                           "published_date": link.css(".DateCell::text").get().strip() if link.css(
                                               'a') else 'na',
                                           "pdf_url": link.css(".TitleCell a").attrib["href"].replace('..', '')
                                           }
                                else:
                                    yield {"clean_url": self.allowed_domains[0],
                                           "base_url": response.url,
                                           "title": link.css('.TitleCell a::text').get().strip() if link.css(
                                               'a') else 'na',
                                           "excerpt": 'na',
                                           "published_date": link.css(".DateCell::text").get().strip() if link.css(
                                               'a') else 'na',
                                           "pdf_url": "https://www.education.gov.za" + link.css(".TitleCell a").attrib[
                                               "href"].replace('..', '')
                                           }
                            else:
                                pass

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


# 11
class NcrSpider(scrapy.Spider):
    name = "NcrSpider"
    allowed_domains = ["www.ncr.gov.za"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css(".table-striped a")
                data += response.css(".media a")
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:

                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('a::text').get().strip() if link.css('a') else 'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": link.css("a").attrib["href"].replace('..', '')
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('a::text').get().strip() if link.css('a') else 'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": "https://www.ncr.org.za" + link.css("a").attrib["href"].replace('..',
                                                                                                                  '')
                                       }


                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


# 12
class NmdpraSpider(scrapy.Spider):
    name = "NmdpraSpider"
    allowed_domains = ["www.nmdpra.gov.ng"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css(".elementor-button-wrapper")

                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if ".pdf" in link.css("a").attrib["href"]:
                                if "http" in str(link.css("a").attrib["href"]):
                                    yield {"clean_url": self.allowed_domains[0],
                                           "base_url": response.url if not response.url.endswith('/') else response.url[
                                                                                                           :-1],
                                           "title": 'na',
                                           "excerpt": 'na',
                                           "published_date": 'na',
                                           "pdf_url": link.css("a").attrib["href"].replace('..', '')
                                           }
                                else:
                                    yield {"clean_url": self.allowed_domains[0],
                                           "base_url": response.url if not response.url.endswith('/') else response.url[
                                                                                                           :-1],
                                           "title": 'na',
                                           "excerpt": 'na',
                                           "published_date": 'na',
                                           "pdf_url": "https://www.nmdpra.gov.ng" + link.css("a").attrib[
                                               "href"].replace('..', '')
                                           }


                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


# 13
class NuprcSpider(scrapy.Spider):
    name = "NuprcSpider"
    allowed_domains = ["www.nuprc.gov.ng"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                categories_links = []
                res = response.css('.vc_tta-panel-title.vc_tta-controls-icon-position-right a')
                for r in res:
                    try:
                        if "http" in str(r.css("a").attrib["href"]):
                            categories_links.append(r.css("a").attrib["href"])
                        else:
                            categories_links.append(
                                "https://www.nuprc.gov.ng/nuprc-guidelines/" + r.css("a").attrib["href"])

                    except:
                        pass

                for i in categories_links:
                    try:
                        yield scrapy.Request(
                            i,
                            callback=self.article_links,
                            meta={"dont_retry": True, "download_timeout": cat_timeout,
                                  "base_url": response.url},
                            errback=handle_error,
                        )
                    except:
                        pass

    def article_links(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css(".box-content")
                base_url = response.meta.get('base_url')

                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if ".pdf" in link.css("a").attrib["href"]:
                                if "http" in str(link.css("a").attrib["href"]):
                                    yield {"clean_url": self.allowed_domains[0],
                                           "base_url": base_url if not base_url.endswith('/') else base_url[:-1],
                                           "title": link.css('p::text').get().strip() if link.css('p::text') else 'na',
                                           "excerpt": 'na',
                                           "published_date": 'na',
                                           "pdf_url": link.css("p a").attrib["href"].replace('..', '')
                                           }
                                else:
                                    yield {"clean_url": self.allowed_domains[0],
                                           "base_url": base_url if not base_url.endswith('/') else base_url[:-1],
                                           "title": link.css('p::text').get().strip() if link.css('p::text') else 'na',
                                           "excerpt": 'na',
                                           "published_date": 'na',
                                           "pdf_url": "" + link.css("p a").attrib["href"].replace('..', '')
                                           }


                    except:
                        pass


# 14
class LabourPdfSpider(scrapy.Spider):
    name = "LabourPdfSpider"
    allowed_domains = ["labour.gov.ng"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):

        if str(response.status) == "200":
            if response.css("body"):

                data = response.css("ol li")

                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if ".pdf" in link.css("a").attrib["href"]:
                                if "http" in str(link.css("a").attrib["href"]):
                                    yield {"clean_url": self.allowed_domains[0],
                                           "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                           "title": link.css('a::text').get().strip() if link.css('a') else 'na',
                                           "excerpt": 'na',
                                           "published_date": 'na',
                                           "pdf_url": link.css("a").attrib["href"].replace('..', '')
                                           }
                                else:
                                    yield {"clean_url": self.allowed_domains[0],
                                          "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                           "title": link.css('a::text').get().strip() if link.css('a') else 'na',
                                           "excerpt": 'na',
                                           "published_date": 'na',
                                           "pdf_url": "" + link.css("a").attrib["href"].replace('..', '')
                                           }


                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


# 15
class NgxgroupSpider(scrapy.Spider):
    name = "NgxgroupSpider"
    allowed_domains = ["ngxgroup.com"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):

        if str(response.status) == "200":
            if response.css("body"):
                data = response.css(".widget-download-list-item a")
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:

                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('a::text').get().strip() if link.css('a') else 'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": link.css("a").attrib["href"].replace('..', '')
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('a::text').get().strip() if link.css('a') else 'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": "" + link.css("a").attrib["href"].replace('..', '')
                                       }


                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


# 16
class NcdcSpider(scrapy.Spider):
    name = "NcdcSpider"
    allowed_domains = ["ncdc.gov.ng"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):

        if str(response.status) == "200":
            if response.css("body"):
                data = response.css(".card a")
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if ".pdf" in link.css("a").attrib["href"]:
                                if "http" in str(link.css("a").attrib["href"]):
                                    yield {"clean_url": self.allowed_domains[0],
                                           "base_url": response.url,
                                           "title": link.css('a .card-content::text').get().strip() if link.css(
                                               'a') else 'na',
                                           "excerpt": 'na',
                                           "published_date": 'na',
                                           "pdf_url": link.css("a").attrib["href"].replace('..', '')
                                           }
                                else:
                                    yield {"clean_url": self.allowed_domains[0],
                                           "base_url": response.url,
                                           "title": link.css('a .card-content::text').get().strip() if link.css(
                                               'a') else 'na',
                                           "excerpt": 'na',
                                           "published_date": 'na',
                                           "pdf_url": "https://ncdc.gov.ng" + link.css("a").attrib["href"].replace('..',
                                                                                                                   '')
                                           }


                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


# 17
class NesreaSpider(scrapy.Spider):
    name = "NesreaSpider"
    allowed_domains = ["www.nesrea.gov.ng"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):

                data = response.css(".accordion-each")
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:

                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[
                                                                                                       :-1],
                                       "title": link.css('label::text').get().strip() if link.css('a') else 'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": link.css(".ac-small a").attrib["href"].replace('..', '')
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[
                                                                                                       :-1],
                                       "title": link.css('label::text').get().strip() if link.css('a') else 'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": "https://www.nesrea.gov.ng" + link.css(".ac-small a").attrib[
                                           "href"].replace('..', '')
                                       }


                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


# 18
class NitdaSpider(scrapy.Spider):
    name = "NitdaSpider"
    allowed_domains = ["nitda.gov.ng"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css(".download-links li")
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if ".pdf" in link.css("a").attrib["href"]:
                                if "http" in str(link.css("a").attrib["href"]):
                                    yield {"clean_url": self.allowed_domains[0],
                                           "base_url": response.url if not response.url.endswith('/') else response.url[
                                                                                                           :-1],
                                           "title": link.css('a span.ttl::text').get().strip() if link.css(
                                               'a') else 'na',
                                           "excerpt": 'na',
                                           "published_date": 'na',
                                           "pdf_url": link.css("a").attrib["href"].replace('..', '')
                                           }
                                else:
                                    yield {"clean_url": self.allowed_domains[0],
                                           "base_url": response.url if not response.url.endswith('/') else response.url[
                                                                                                           :-1],
                                           "title": link.css('a span.ttl::text').get().strip() if link.css(
                                               'a') else 'na',
                                           "excerpt": 'na',
                                           "published_date": 'na',
                                           "pdf_url": "" + link.css("a").attrib["href"].replace('..', '')
                                           }


                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


# 19
class NccSpider(scrapy.Spider):
    name = "NccSpider"
    allowed_domains = ["www.ncc.gov.ng"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css("li.row0")
                data += response.css("li.row1")
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:

                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('a::text').get().strip() if link.css('a') else 'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": link.css("a").attrib["href"].replace('..', '')
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('a::text').get().strip() if link.css('a') else 'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": "https://www.ncc.gov.ng" + link.css("a").attrib["href"].replace('..',
                                                                                                                  '')
                                       }


                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


# 20
class NafdacSpider(scrapy.Spider):
    name = "NafdacSpider"
    allowed_domains = ["www.nafdac.gov.ng"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css("td a")

                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:

                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title": link.css('a::text').get().strip() if link.css('a::text') else 'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": link.css("a").attrib["href"].replace('..', '')
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title": link.css('a::text').get().strip() if link.css('a::text') else 'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": "https://www.ncc.gov.ng" + link.css("a").attrib["href"].replace('..',
                                                                                                                  '')
                                       }


                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


# 21
class DffePdfSpider(scrapy.Spider):
    name = "DffePdfSpider"
    allowed_domains = ["www.dffe.gov.za"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):

                data = response.css(".td_cell a")

                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:

                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('a::text').get().strip() if link.css('a') else 'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": link.css("a").attrib["href"].replace('..', '')
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('a::text').get().strip() if link.css('a') else 'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": "https://www.dffe.gov.za" + link.css("a").attrib["href"].replace('..',
                                                                                                                   '')
                                       }


                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


# 22
class Dffe2Spider(scrapy.Spider):
    name = "Dffe2Spider"
    allowed_domains = ["www.dffe.gov.za"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):

                data = response.css(".views-field.views-field-title a")

                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:

                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('a::text').get().strip() if link.css('a') else 'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": link.css("a").attrib["href"].replace('..', '')
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('a::text').get().strip() if link.css('a') else 'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": "https://www.dffe.gov.za" + link.css("a").attrib["href"].replace('..',
                                                                                                                   '')
                                       }


                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


# 23
class FirsSpider(scrapy.Spider):
    name = "FirsSpider"
    allowed_domains = ["www.firs.gov.ng"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):

                data = response.css(".elementor-text-editor.elementor-clearfix a")

                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:

                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title": link.css('a::text').get().strip() if link.css('a::text') else 'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": link.css("a").attrib["href"].replace('..', '')
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title": link.css('a::text').get().strip() if link.css('a::text') else 'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": "" + link.css("a").attrib["href"].replace('..', '')
                                       }


                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1

# 24
class CbnSpider(scrapy.Spider):
    name = "CbnSpider"
    allowed_domains = ["www.cbn.gov.ng"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                categories_links = []
                res = response.css('#ContentTextinner > a')
                for r in res:
                    try:
                        if "http" in str(r.css("a").attrib["href"]):
                            categories_links.append(r.css("a").attrib["href"])
                        else:
                            categories_links.append(f"https://www.cbn.gov.ng/Documents/" + r.css("a").attrib["href"])

                    except:
                        pass

                for i in categories_links:
                    try:
                        yield scrapy.Request(
                            i,
                            callback=self.article_links,
                            meta={"dont_retry": True, "download_timeout": cat_timeout,"base_url": response.url},
                            errback=handle_error,
                        )
                    except:
                        pass

                data = response.css(".dbasetable")

                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:

                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('a::text').get().strip() if link.css('a::text') else 'na',
                                       "excerpt": 'na',
                                       "published_date": link.css("#publishedDt::text").get().strip() if link.css("#publishedDt::text") else 'na',
                                       "pdf_url": link.css("a").attrib["href"].replace('..', '')
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('a::text').get().strip() if link.css('a::text') else 'na',
                                       "excerpt": 'na',
                                       "published_date": link.css("#publishedDt::text").get().strip() if link.css("#publishedDt::text") else 'na',
                                       "pdf_url": "https://www.cbn.gov.ng" + link.css("a").attrib["href"].replace('..','')
                                       }


                    except:
                        pass



    def article_links(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                base_url = response.meta.get('base_url')
                data = response.css(".dbasetable")
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:

                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": base_url,
                                       "title": link.css('a::text').get().strip() if link.css('a::text') else 'na',
                                       "excerpt": 'na',
                                       "published_date": link.css("#publishedDt::text").get().strip() if link.css("#publishedDt::text") else 'na',
                                       "pdf_url": link.css("a").attrib["href"].replace('..', '')
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": base_url,
                                       "title": link.css('a::text').get().strip() if link.css('a::text') else 'na',
                                       "excerpt": 'na',
                                       "published_date": link.css("#publishedDt::text").get().strip() if link.css("#publishedDt::text") else 'na',
                                       "pdf_url": "https://www.cbn.gov.ng" + link.css("a").attrib["href"].replace('..','')
                                       }


                    except:
                        pass




# 25
class JseSpider(scrapy.Spider):
    name = "JseSpider"
    allowed_domains = ["www.jse.co.za"]
    not_allowed_keyword = [".doc"]
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):

                data = response.css(".file--download--target-blank")

                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:

                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": 'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": link.css("a").attrib["href"].replace('..', '')
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": 'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": "" + link.css("a").attrib["href"].replace('..', '')
                                       }


                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


# 26
class SahpraSpider(scrapy.Spider):
    name = "SahpraSpider"
    allowed_domains = ["www.sahpra.org.za"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css("td")
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if ".pdf" in link.css("a").attrib["href"]:
                                if "http" in str(link.css("a").attrib["href"]):
                                    yield {"clean_url": self.allowed_domains[0],
                                           "base_url": response.url,
                                           "title": 'na',
                                           "excerpt": 'na',
                                           "published_date": 'na',
                                           "pdf_url": link.css("a").attrib["href"].replace('..', '')
                                           }
                                else:
                                    yield {"clean_url": self.allowed_domains[0],
                                           "base_url": response.url,
                                           "title": 'na',
                                           "excerpt": 'na',
                                           "published_date": 'na',
                                           "pdf_url": "" + link.css("a").attrib["href"].replace('..', '')
                                           }


                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


# 27
class SapsSpider(scrapy.Spider):
    name = "SapsSpider"
    allowed_domains = ["www.saps.gov.za"]
    not_allowed_keyword = [".php"]
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):

                data = response.css(".panel-body li")

                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:

                            if response.url == "https://www.saps.gov.za/resource_centre/bills/bills.php":
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('a::text').get().strip() if link.css('a') else "na",
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": "https://www.saps.gov.za/resource_centre/bills/" +
                                                  link.css("a").attrib["href"].replace('..', '')}
                            elif response.url == 'https://www.saps.gov.za/resource_centre/acts/acts.php':
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css('a::text').get().strip() if link.css('a') else "na",
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": "https://www.saps.gov.za/resource_centre/acts/" +
                                                  link.css("a").attrib["href"].replace('..', '')}
                            elif response.url == 'https://www.saps.gov.za/resource_centre/notices/notices.php':
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": 'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": "https://www.saps.gov.za/resource_centre/notices/" +
                                                  link.css("a").attrib["href"].replace('..', '')}
                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


# 28
class NercSpider(scrapy.Spider):
    name = "NercSpider"
    allowed_domains = ["nerc.gov.ng"]
    not_allowed_keyword = [
        "https://nerc.gov.ng/index.php/library/documents/NERC-Orders/2016---2018-Minor-Review--and--Minimum-Remittance-Orders-for-the-11-DisCos/",
        "https://nerc.gov.ng/index.php/library/documents/NERC-Orders/Amended-Orders-on-the-Capping-of-Estimated-Bills/",
        "https://nerc.gov.ng/index.php/library/documents/NERC-Orders/December-2019-MYTO-Minor-Review-Orders-for-NESI/",
        "https://nerc.gov.ng/index.php/library/documents/NERC-Orders/NERC-Orders-on-Approved-Performance-Improvement-Plans-(PIPs)/"]
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                categories_links = []
                res = response.css('.remositoryfilelistingfooter .remositorypagenav a')
                for r in res:
                    try:
                        if "http" in str(r.css("a").attrib["href"]):
                            categories_links.append(r.css("a").attrib["href"])
                        else:
                            categories_links.append(f"https://www.cbn.gov.ng/Documents/" + r.css("a").attrib["href"])

                    except:
                        pass

                for i in set(categories_links):
                    try:
                        yield scrapy.Request(
                            i,
                            callback=self.article_links,
                            meta={"dont_retry": True, "download_timeout": cat_timeout,"base_url": response.url},
                            errback=handle_error,
                        )
                    except:
                        pass
                data = response.css("h3 a")

                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css("a::text").get().strip() if link.css('a::text') else "na",
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": link.css("a").attrib["href"].replace('..', '')
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css("a::text").get().strip() if link.css('a::text') else "na",
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": "" + link.css("a").attrib["href"].replace('..', '')
                                       }
                    except:
                        pass

    def article_links(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                base_url = response.meta.get('base_url')
                data = response.css("h3 a")

                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": base_url,
                                       "title": link.css("a::text").get().strip() if link.css('a::text') else "na",
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": link.css("a").attrib["href"].replace('..', '')
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": base_url,
                                       "title": link.css("a::text").get().strip() if link.css('a::text') else "na",
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": "" + link.css("a").attrib["href"].replace('..', '')
                                       }
                    except:
                        pass




# 29
class DcdtPdfSpider(scrapy.Spider):
    name = "DcdtPdfSpider"
    allowed_domains = ["dcdt.co.za","dcdt.gov.za"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                categories_links = []
                res = response.css('.pagenav')
                for r in res:
                    try:
                        if "http" in str(r.css("a").attrib["href"]):
                            categories_links.append(r.css("a").attrib["href"])
                        else:
                            categories_links.append(f"https://www.dcdt.gov.za" + r.css("a").attrib["href"])

                    except:
                        pass

                for i in set(categories_links):
                    try:
                        yield scrapy.Request(
                            i,
                            callback=self.article_links,
                            meta={"dont_retry": True, "download_timeout": cat_timeout,"base_url": response.url},
                            errback=handle_error,
                        )
                    except:
                        pass
                data = response.css(".pd-float a")
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css("a::text").get().strip(),
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": link.css("a").attrib["href"].replace('..', '')
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css("a::text").get().strip(),
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": "https://www.dcdt.gov.za" + link.css("a").attrib["href"].replace('..',
                                                                                                                   '')
                                       }
                    except:
                        pass

    def article_links(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                base_url = response.meta.get('base_url')
                data = response.css(".pd-float a")
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": base_url,
                                       "title": link.css("a::text").get().strip(),
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": link.css("a").attrib["href"].replace('..', '')
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": base_url,
                                       "title": link.css("a::text").get().strip(),
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": "https://www.dcdt.gov.za" + link.css("a").attrib["href"].replace('..',
                                                                                                                   '')
                                       }
                    except:
                        pass





# 30
class TransportationSpider(scrapy.Spider):
    name = "TransportationSpider"
    allowed_domains = ["transportation.gov.ng"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                categories_links = []
                res = response.css("h2 a")
                for r in res:
                    try:
                        if "https://" in str(r.css("a").attrib["href"]):
                            categories_links.append(r.css("a").attrib["href"])
                        else:
                            categories_links.append("" + r.css("a").attrib["href"])

                    except:
                        pass

                for i in categories_links:
                    try:
                        yield scrapy.Request(
                            i,
                            callback=self.article_links,
                            meta={"proxy": proxy, "dont_retry": True, "download_timeout": cat_timeout},
                            errback=handle_error,
                        )

                        self.start_urls = i

                    except:
                        pass

    def article_links(self, response):
        if str(response.status) == "200":
            if response.css("body"):

                result = response.css('h3 a')

                for r in result:
                    try:
                        if ".pdf" in str(r.css("a").attrib["href"]):
                            yield {"clean_url": self.allowed_domains[0],
                                   "base_url": response.url,
                                   "title": "na",
                                   "excerpt": 'na',
                                   "published_date": 'na',
                                   "pdf_url": r.css("a").attrib["href"].replace('..', '')
                                   }
                    except:
                        pass
        else:
            while self.check_ip_article_links < 2:
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_article_links += 1





class Bundesnetzagentur1PdfSpider(scrapy.Spider):
    name = "Bundesnetzagentur1PdfSpider"
    allowed_domains = ["bundesnetzagentur.de"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css(".beschlussFile")
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title": 'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": link.css("a").attrib["href"].replace('..', '')
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title": 'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": "https://www.bundesnetzagentur.de" + link.css("a").attrib["href"].replace('..', '')
                                       }
                    except:
                        pass

        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1
#2
class Bundesnetzagentur2PdfSpider(scrapy.Spider):
    name = "Bundesnetzagentur2PdfSpider"
    allowed_domains = ["bundesnetzagentur.de"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={ "dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                categories_links = []
                res = response.css('td a')
                for r in res:
                    try:

                        if "https://" in str(r.css("a").attrib["href"]) or "http://" in str(r.css("a").attrib["href"]):
                            categories_links.append(r.css("a").attrib["href"].split(";")[0])

                        else:
                            categories_links.append("https://www.bundesnetzagentur.de/"+ r.css("a").attrib["href"].split(";")[0])
                    except:
                        pass

                for i in set(categories_links):
                    try:
                        yield scrapy.Request(
                            i,
                            callback=self.article_links,
                            meta={"dont_retry": True, "download_timeout": cat_timeout,"base_url": self.start_urls[0]},
                            errback=handle_error,
                        )


                    except:
                        pass
                try:
                    next_page = response.css('.forward a')
                    if next_page:
                        for i in next_page:
                            if 'http' in i.css('a').attrib['href']:
                                yield scrapy.Request(
                                i.css('a').attrib['href'],
                                callback=self.parse,
                                meta={"dont_retry": True, "download_timeout": cat_timeout,"base_url": response.url},
                                errback=handle_error,
                            )
                            else:
                                yield scrapy.Request(
                                "https://www.bundesnetzagentur.de/"+i.css('a').attrib['href'],
                                callback=self.parse,
                                meta={"dont_retry": True, "download_timeout": cat_timeout,"base_url": response.url},
                                errback=handle_error,
                            )
                except:
                    pass

    def article_links(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                base_url = response.meta.get('base_url')
                try:
                    data = response.css('.FTpdf')

                    for link in data:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                    "base_url": base_url if not base_url.endswith('/') else base_url[:-1],
                                    "title": 'na',
                                    "excerpt": 'na',
                                    "published_date": 'na',
                                    "pdf_url": link.css("a").attrib["href"],
                                    }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                    "base_url": base_url if not base_url.endswith('/') else base_url[:-1],
                                    "title": 'na',
                                    "excerpt": 'na',
                                    "published_date":'na',
                                    "pdf_url": "https://www.bundesnetzagentur.de" + link.css("a").attrib["href"],
                                    }

                except:
                        pass
#3
class BundesbankPdfSpider(scrapy.Spider):
    name = "BundesbankPdfSpider"
    allowed_domains = ["bundesbank.de"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css(".linklist__item")
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if ".pdf" in str(link.css("a").attrib["href"]):
                                if "http" in str(link.css("a").attrib["href"]):
                                    yield {"clean_url": self.allowed_domains[0],
                                        "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                        "title": 'na',
                                        "excerpt": 'na',
                                        "published_date": 'na',
                                        "pdf_url": link.css("a").attrib["href"].replace('..', '')
                                        }
                                else:
                                    yield {"clean_url": self.allowed_domains[0],
                                        "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                        "title": 'na',
                                        "excerpt": 'na',
                                        "published_date": 'na',
                                        "pdf_url": "https://www.bundesbank.de" + link.css("a").attrib["href"].replace('..', '')
                                        }
                    except:
                        pass

        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1
#4
class BundesfinanzministeriumPdfSpider(scrapy.Spider):
    name = "BundesfinanzministeriumPdfSpider"
    allowed_domains = ["bundesfinanzministerium.de"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={ "dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                categories_links = []
                res = response.css('.bmf-resultlist-teaser-link')
                for r in res:
                    try:

                        if "http" in str(r.css("a").attrib["href"]) or "http://" in str(r.css("a").attrib["href"]):
                            categories_links.append(r.css("a").attrib["href"])

                        else:
                            categories_links.append("https://www.bundesfinanzministerium.de/"+ r.css("a").attrib["href"])
                    except:
                        pass



                for i in set(categories_links):
                    try:
                        yield scrapy.Request(
                            i,
                            callback=self.article_links,
                            meta={"dont_retry": True, "download_timeout": cat_timeout,"base_url": response.url},
                            errback=handle_error,
                        )


                    except:
                        pass

    def article_links(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                base_url = response.meta.get('base_url')
                try:
                    data = response.css('.Publication')

                    for link in data:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": base_url if not base_url.endswith('/') else base_url[:-1],
                                       "title": 'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": link.css("a").attrib["href"],
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": base_url if not base_url.endswith('/') else base_url[:-1],
                                       "title": 'na',
                                       "excerpt": 'na',
                                       "published_date":'na',
                                       "pdf_url": "https://www.bundesfinanzministerium.de" + link.css("a").attrib["href"],
                                       }

                except:
                        pass
        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1
#5
class GesetzeImInternetPdfSpider(scrapy.Spider):
    name = "GesetzeImInternetPdfSpider"
    allowed_domains = ["gesetze-im-internet.de"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css("p+ p")
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:

                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title": link.css("p::text").get().strip(),
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": link.css("a").attrib["href"].replace('..', '')
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title":  link.css("p::text").get().strip(),
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": "" + link.css("a").attrib["href"].replace('..', '')
                                       }
                    except:
                        pass

        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1
#6
class BundesverfassungsgerichtArticleSpider(scrapy.Spider):
    name = "BundesverfassungsgerichtArticleSpider"
    allowed_domains = ["bundesverfassungsgericht.de"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):

                data = response.css(".links li a")


                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title":link.css('a::text').get() if link.css('a::text') else 'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url":  link.css("a").attrib["href"]
                                       }


                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                        "title":link.css('a::text').get() if link.css('a::text') else 'na',
                                        "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": "https://www.bundesverfassungsgericht.de/" + link.css("a").attrib["href"],
                                       }

                    except:
                        pass


        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1
#7
class BundeskartellamtPdfSpider(scrapy.Spider):
    name = "BundeskartellamtPdfSpider"
    allowed_domains = ["bundeskartellamt.de"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):

                categories_links = []
                res = response.css('td a')
                for r in res:
                    try:

                        if "https://" in str(r.css("a").attrib["href"]) or "http://" in str(r.css("a").attrib["href"]):
                            categories_links.append(r.css("a").attrib["href"])

                        else:
                            categories_links.append("https://www.bundeskartellamt.de/" + r.css("a").attrib["href"])
                    except:
                        pass



                for i in set(categories_links):
                    try:
                        yield scrapy.Request(
                            i,
                            callback=self.article_links,
                            meta={"dont_retry": True, "download_timeout": cat_timeout, "base_url": self.start_urls[0]},
                            errback=handle_error,
                        )


                    except:
                        pass

                try:
                    next_page = response.css('.forward a')
                    if next_page:
                        for i in next_page:
                            if 'http' in i.css('a').attrib['href']:
                                yield scrapy.Request(
                                    i.css('a').attrib['href'],
                                    callback=self.parse,
                                    meta={"dont_retry": True, "download_timeout": cat_timeout,
                                          "base_url": response.url},
                                    errback=handle_error,
                                )
                            else:
                                yield scrapy.Request(
                                    "https://www.bundeskartellamt.de/" + i.css('a').attrib['href'],
                                    callback=self.parse,
                                    meta={"dont_retry": True, "download_timeout": cat_timeout,
                                          "base_url": response.url},
                                    errback=handle_error,
                                )
                except:
                    pass

    def article_links(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                base_url = response.meta.get('base_url')
                try:
                    data = response.css('.FTpdf')

                    for link in data:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": base_url if not base_url.endswith('/') else base_url[:-1],
                                       "title": 'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": link.css("a").attrib["href"],
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": base_url if not base_url.endswith('/') else base_url[:-1],
                                       "title": 'na',
                                       "excerpt": 'na',
                                       "published_date":'na',
                                       "pdf_url": "https://www.bundeskartellamt.de" + link.css("a").attrib["href"],
                                       }

                except:
                    pass
        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1
#8
class DpmaPdfSpider(scrapy.Spider):
    name = "DpmaPdfSpider"
    allowed_domains = ["dpma.de"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0


    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):

                data = response.css(".textbereich a")
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if ".pdf" in str(link.css('a').attrib['href']):
                                if "http" in str(link.css("a").attrib["href"]):
                                    yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title": link.css("a::text").get() if link.css("a::text") else "na",
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": link.css("a").attrib["href"].replace('..', '')
                                       }
                                else:
                                    yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title": link.css("a::text").get() if link.css("a::text") else "na",
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": "https://www.dpma.de" + link.css("a").attrib["href"].replace('..', '')
                                       }
                    except:
                        pass

        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1
#9
class BmuvPdfSpider(scrapy.Spider):
    name = "BmuvPdfSpider"
    allowed_domains = ["bmuv.de"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={ "dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):

                categories_links = []
                res = response.css('.rf-c-articles-list__anchor')
                for r in res:
                    try:

                        if "http" in str(r.css("a").attrib["href"]) or "http://" in str(r.css("a").attrib["href"]):
                            categories_links.append(r.css("a").attrib["href"])

                        else:
                            categories_links.append("https://www.bmuv.de"+ r.css("a").attrib["href"])
                    except:
                        pass



                for i in set(categories_links):
                    try:
                        yield scrapy.Request(
                            i,
                            callback=self.article_links,
                            meta={"dont_retry": True, "download_timeout": cat_timeout,"base_url": response.url},
                            errback=handle_error,
                        )


                    except:
                        pass

    def article_links(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                base_url = response.meta.get('base_url')
                try:
                    data = response.css('.rf-c-ce-download-list a')

                    for link in data:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": base_url if not base_url.endswith('/') else base_url[:-1],
                                       "title": 'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": link.css("a").attrib["href"],
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": base_url if not base_url.endswith('/') else base_url[:-1],
                                       "title": 'na',
                                       "excerpt": 'na',
                                       "published_date":'na',
                                       "pdf_url": "https://www.bmuv.de" + link.css("a").attrib["href"],
                                       }

                except:
                        pass
        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1
#11
class NsePdfSpider(scrapy.Spider):
    name = "NsePdfSpider"
    allowed_domains = ["nse.co.ke"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0


    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy,"dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):

                data = response.css(".nectar-fancy-box")
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if ".pdf" in str(link.css('a').attrib['href']):
                                if "http" in str(link.css("a").attrib["href"]):
                                    yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title": link.css('h3::text').get().strip(),
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": link.css("a").attrib["href"].replace('..', '')
                                       }
                                else:
                                    yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title": link.css('h3::text').get().strip(),
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": "" + link.css("a").attrib["href"].replace('..', '')
                                       }
                    except:
                        pass

        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1
#12
class CentralbankPdfSpider(scrapy.Spider):
    name = "CentralbankPdfSpider"
    allowed_domains = ["centralbank.go.ke"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0


    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy,"dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):

                data = response.css("#et-boc a")
                for link in data:
                    try:
                        if ".pdf" in str(link.css('a').attrib['href']):
                            if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                                pass
                            else:
                                if ".pdf" in str(link.css('a').attrib['href']):
                                    if "http" in str(link.css("a").attrib["href"]):
                                        yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title": link.css('a::text').get().strip(),
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": link.css("a").attrib["href"].replace('..', '')
                                       }
                                    else:
                                        yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title": link.css('a::text').get().strip(),
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": "https://www.centralbank.go.ke" + link.css("a").attrib["href"].replace('..', '')
                                       }
                    except:
                        pass

        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1
#13
class ParliamentPdfSpider(scrapy.Spider):
    name = "ParliamentPdfSpider"
    allowed_domains = ["parliament.go.ke"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0
    start_urlss = ['http://www.parliament.go.ke/the-senate/house-business/bills?title=%20&field_parliament_value=All&page=0',
                  'http://www.parliament.go.ke/the-senate/house-business/bills?title=%20&field_parliament_value=All&page=1',
                  'http://www.parliament.go.ke/the-senate/house-business/bills?title=%20&field_parliament_value=All&page=2',
                  'http://www.parliament.go.ke/the-senate/house-business/bills?title=%20&field_parliament_value=All&page=3',
                  'http://www.parliament.go.ke/the-senate/house-business/bills?title=%20&field_parliament_value=All&page=4',
                  'http://www.parliament.go.ke/the-senate/house-business/bills?title=%20&field_parliament_value=All&page=5',
                  'http://www.parliament.go.ke/the-senate/house-business/bills?title=%20&field_parliament_value=All&page=6',
                  'http://www.parliament.go.ke/the-senate/house-business/bills?title=%20&field_parliament_value=All&page=7',
                  'http://www.parliament.go.ke/the-senate/house-business/bills?title=%20&field_parliament_value=All&page=8',
                  'http://www.parliament.go.ke/the-senate/house-business/bills?title=%20&field_parliament_value=All&page=9',
                  'http://www.parliament.go.ke/the-senate/resources/statutory-documents?title=%20&field_parliament_value=All&page=0',
                  'http://www.parliament.go.ke/the-senate/resources/statutory-documents?title=%20&field_parliament_value=All&page=1',
                  'http://www.parliament.go.ke/the-national-assembly/resources/statutory-documents?title=%20&field_parliament_value=All&page=0',
                  'http://www.parliament.go.ke/the-national-assembly/resources/statutory-documents?title=%20&field_parliament_value=All&page=1',
                  'http://www.parliament.go.ke/legislative-proposals'
                  ]

    def start_requests(self):
        for i in self.start_urlss:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urlss:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy,"dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):

                data = response.css("span a")
                for link in data:
                    try:
                        if ".pdf" in str(link.css('a').attrib['href']):
                            if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                                pass
                            else:
                                if ".pdf" in str(link.css('a').attrib['href']):
                                    if "http" in str(link.css("a").attrib["href"]):
                                        yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url.split("?title=")[0],
                                       "title": link.css('a::text').get().strip(),
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": link.css("a").attrib["href"].replace('..', '')
                                       }
                                    else:
                                        yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url.split("?title=")[0],
                                       "title": link.css('a::text').get().strip(),
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": "https://www.centralbank.go.ke" + link.css("a").attrib["href"].replace('..', '')
                                       }
                    except:
                        pass

        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1
#14
class InteriorPdfSpider(scrapy.Spider):
    name = "InteriorPdfSpider"
    allowed_domains = ["interior.go.ke"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy,"dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):

                data = response.css("#tab-c0f285bd9bc2108331b a")
                for link in data:
                    try:
                            if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                                pass
                            else:
                                if ".pdf" in str(link.css('a').attrib['href']):
                                    if "http" in str(link.css("a").attrib["href"]):
                                        yield {"clean_url": self.allowed_domains[0],
                                       "base_url":self.start_urls[0] if not self.start_urls[0].endswith('/') else self.start_urls[0][:-1],
                                       "title": link.css('a::text').get().strip(),
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": link.css("a").attrib["href"].replace('..', '')
                                       }
                                    else:
                                        yield {"clean_url": self.allowed_domains[0],
                                       "base_url":self.start_urls[0] if not self.start_urls[0].endswith('/') else self.start_urls[0][:-1],
                                       "title": link.css('a::text').get().strip(),
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": "" + link.css("a").attrib["href"].replace('..', '')
                                       }
                    except:
                        pass

        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1
#15
class Kra1PdfSpider(scrapy.Spider):
    name = "Kra1PdfSpider"
    allowed_domains = ["kra.go.ke"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy,"dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):

                data = response.css(".doc-btn a")

                for link in data:
                    try:
                            if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                                pass
                            else:

                                    if "http" in str(link.css("a").attrib["href"]):
                                        yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title": 'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": link.css("a").attrib["href"].replace('..', '')
                                       }
                                    else:
                                        yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title": 'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": "" + link.css("a").attrib["href"].replace('..', '')
                                       }
                    except:
                        pass

        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1
#16
class Kra2ArticleSpider(scrapy.Spider):
    name = "Kra2ArticleSpider"
    allowed_domains = ["kra.go.ke"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy,"dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):

                data = response.css(".col-md-3")

                for link in data:
                    try:
                            if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                                pass
                            else:

                                    if "http" in str(link.css("a").attrib["href"]):
                                        yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title": 'na',
                                       "excerpt": 'na',
                                       "published_date":link.css(".content-wrapper p::text").get().strip(),
                                       "pdf_url": link.css("a").attrib["href"].replace('..', '')
                                       }
                                    else:
                                        yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title": 'na',
                                       "excerpt": 'na',
                                       "published_date": link.css(".content-wrapper p::text").get().strip(),
                                       "pdf_url": "https://www.kra.go.ke" + link.css("a").attrib["href"].replace('..', '')
                                       }
                    except:
                        pass

        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1
#17
class CaPdfSpider(scrapy.Spider):
    name = "CaPdfSpider"
    allowed_domains = ["ca.go.ke"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy,"dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):

                data = response.css(".repo")

                for link in data:
                    try:
                            if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                                pass
                            else:

                                    if "http" in str(link.css("a").attrib["href"]):
                                        yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title": link.css(".col.filename::text").get().strip(),
                                       "excerpt": 'na',
                                       "published_date":'na',
                                       "pdf_url": link.css("a").attrib["href"].replace('..', '')
                                       }
                                    else:
                                        yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title":  link.css(".col.filename::text").get().strip(),
                                       "excerpt": 'na',
                                       "published_date":'na',
                                       "pdf_url": "" + link.css("a").attrib["href"].replace('..', '')
                                       }
                    except:
                        pass

        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1
#18
class HealthPdfSpider(scrapy.Spider):
    name = "HealthPdfSpider"
    allowed_domains = ["health.go.ke"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy,"dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):

                data = response.css("#main-content a")

                for link in data:
                    try:
                            if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                                pass
                            else:

                                    if "http" in str(link.css("a").attrib["href"]):
                                        yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title": 'na',
                                       "excerpt": 'na',
                                       "published_date":'na',
                                       "pdf_url": link.css("a").attrib["href"].replace('..', '')
                                       }
                                    else:
                                        yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title":  'na',
                                       "excerpt": 'na',
                                       "published_date":'na',
                                       "pdf_url": "" + link.css("a").attrib["href"].replace('..', '')
                                       }
                    except:
                        pass

        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1
#19
class IndustrializationPdfSpider(scrapy.Spider):
    name = "IndustrializationPdfSpider"
    allowed_domains = ["industrialization.go.ke"]
    not_allowed_keyword = ["Template"]
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={ "dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):

                categories_links = []
                res = response.css('td a')
                for r in res:
                    try:

                        if "https://" in str(r.css("a").attrib["href"]) or "http://" in str(r.css("a").attrib["href"]):
                            categories_links.append(r.css("a").attrib["href"])

                        else:
                            categories_links.append("https://www.industrialization.go.ke"+ r.css("a").attrib["href"])
                    except:
                        pass



                for i in set(categories_links):
                    try:
                        yield scrapy.Request(
                            i,
                            callback=self.article_links,
                            meta={"dont_retry": True, "download_timeout": cat_timeout,"base_url": response.url},
                            errback=handle_error,
                        )


                    except:
                        pass
    llinks =[]
    def article_links(self, response, llinks = []):
        if str(response.status) == "200":
            if response.css("body"):
                base_url = response.meta.get('base_url')
                try:
                    data = response.css('strong a')

                    for link in data:
                        if str(link.css("a").attrib["href"]) not in llinks:
                            llinks.append(str(link.css("a").attrib["href"]))
                            if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                                pass
                            else:
                                if ".pdf" in str(link.css('a').attrib['href']):
                                    if "http" in str(link.css("a").attrib["href"]):
                                        yield {"clean_url": self.allowed_domains[0],
                                       "base_url": base_url if not base_url.endswith('/') else base_url[:-1],
                                       "title": 'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": link.css("a").attrib["href"],
                                       }
                                    else:
                                        yield {"clean_url": self.allowed_domains[0],
                                       "base_url": base_url if not base_url.endswith('/') else base_url[:-1],
                                       "title": 'na',
                                       "excerpt": 'na',
                                       "published_date":'na',
                                       "pdf_url": "https://www.industrialization.go.ke" + link.css("a").attrib["href"],
                                       }

                except:
                        pass
        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1
#20
class CmaPdfSpider(scrapy.Spider):
    name = "CmaPdfSpider"
    allowed_domains = ["cma.or.ke"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy,"dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):

                data = response.css(".pd-float")

                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:

                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[
                                                                                                       :-1],
                                       "title": link.css("a::text").get().strip(),
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": link.css("a").attrib["href"].replace('..', '')
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[
                                                                                                       :-1],
                                       "title": link.css("a::text").get().strip(),
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": "https://www.cma.or.ke" + link.css("a").attrib["href"].replace('..',
                                                                                                                 '')
                                       }
                    except:
                        pass

                try:
                    next_page = response.css('.pagenav')
                    for i in set(next_page):
                        if "http" in i.css("a").attrib["href"]:
                            res_ip = scrapy.Request(
                                i.css("a").attrib["href"],
                                meta={"dont_retry": True, "download_timeout": timeout, 'base_url': response.url},
                                callback=self.article_links,
                                errback=handle_error,
                            )
                            yield res_ip
                        else:
                            res_ip = scrapy.Request(
                                "https://www.cma.or.ke" + i.css("a").attrib["href"],
                                meta={"dont_retry": True, "download_timeout": timeout, 'base_url': response.url},
                                callback=self.article_links,
                                errback=handle_error,
                            )
                            yield res_ip


                except:
                    pass

    def article_links(self, response, llinks=[]):
        if str(response.status) == "200":
            if response.css("body"):
                base_url = response.meta.get('base_url')
                data = response.css(".pd-float")

                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:

                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": base_url if not base_url.endswith('/') else base_url[:-1],
                                       "title": link.css("a::text").get().strip(),
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": link.css("a").attrib["href"].replace('..', '')
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": base_url if not base_url.endswith('/') else base_url[:-1],
                                       "title": link.css("a::text").get().strip(),
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": "https://www.cma.or.ke" + link.css("a").attrib["href"].replace('..',
                                                                                                                 '')
                                       }
                    except:
                        pass


#21
class NemaPdfSpider(scrapy.Spider):
    name = "NemaPdfSpider"
    allowed_domains = ["nema.go.ke"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy,"dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css(".item-page p")

                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                               "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                               "title": link.css("a::text").get().strip() if link.css("a::text") else 'na',
                               "excerpt": 'na',
                               "published_date":'na',
                               "pdf_url": link.css("a").attrib["href"].replace('..', '')
                               }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                               "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                               "title": link.css("a::text").get().strip() if link.css("a::text") else 'na',
                               "excerpt": 'na',
                               "published_date":'na',
                               "pdf_url": "https://www.nema.go.ke/" + link.css("a").attrib["href"].replace('..', '')
                               }
                    except:
                        pass

        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1
#22
class EnergyPdfSpider(scrapy.Spider):
    name = "EnergyPdfSpider"
    allowed_domains = ["energy.go.ke"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy,"dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):

                llinks = []
                data = response.css(".in a")

                for link in data:
                    try:
                            if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                                pass
                            else:
                                if str(link.css("a").attrib['href']) not in llinks:
                                    llinks.append(str(link.css('a').attrib['href']))
                                    if "http" in str(link.css("a").attrib["href"]):
                                        yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title": 'na',
                                       "excerpt": 'na',
                                       "published_date":'na',
                                       "pdf_url": link.css("a").attrib["href"].replace('..', '')
                                       }
                                    else:
                                        yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title": 'na',
                                       "excerpt": 'na',
                                       "published_date":'na',
                                       "pdf_url": "" + link.css("a").attrib["href"].replace('..', '')
                                       }
                    except:
                        pass

        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1
#23
class LabourspPdfSpider(scrapy.Spider):
    name = "LabourspPdfSpider"
    allowed_domains = ["laboursp.go.ke"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy,"dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                llinks = []
                data = response.css("#post-44 a")

                for link in data:
                    try:
                            if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                                pass
                            else:
                                if str(link.css("a").attrib['href']) not in llinks:
                                    llinks.append(str(link.css('a').attrib['href']))
                                    if "http" in str(link.css("a").attrib["href"]):
                                        yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title": link.css('a::text').get().strip(),
                                       "excerpt": 'na',
                                       "published_date":'na',
                                       "pdf_url": link.css("a").attrib["href"].replace('..', '')
                                       }
                                    else:
                                        yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title": link.css('a::text').get().strip(),
                                       "excerpt": 'na',
                                       "published_date":'na',
                                       "pdf_url": "" + link.css("a").attrib["href"].replace('..', '')
                                       }
                    except:
                        pass

        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1
#24
class PharmacyboardkenyaPdfSpider(scrapy.Spider):
    name = "PharmacyboardkenyaPdfSpider"
    allowed_domains = ["pharmacyboardkenya.org"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy,"dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                llinks = []
                data = response.css(".btn-sm")

                for link in data:
                    try:
                            if any(n in str(link.css("a").attrib["data-downloadurl"]) for n in self.not_allowed_keyword):
                                pass
                            else:
                                if str(link.css("a").attrib['data-downloadurl']) not in llinks:
                                    llinks.append(str(link.css('a').attrib['data-downloadurl']))
                                    if "http" in str(link.css("a").attrib["data-downloadurl"]):
                                        yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title": 'na',
                                       "excerpt": 'na',
                                       "published_date":'na',
                                       "pdf_url": link.css("a").attrib["data-downloadurl"].replace('..', '')
                                       }
                                    else:
                                        yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title": 'na',
                                       "excerpt": 'na',
                                       "published_date":'na',
                                       "pdf_url": "" + link.css("a").attrib["data-downloadurl"].replace('..', '')
                                       }
                    except:
                        pass

        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1
#25
class AgricultureauthorityPdfSpider(scrapy.Spider):
    name = "AgricultureauthorityPdfSpider"
    allowed_domains = ["agricultureauthority.go.ke"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy,"dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                llinks = []
                data = response.css(".pd-float a")

                for link in data:
                    try:
                            if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                                pass
                            else:
                                if str(link.css("a").attrib['href']) not in llinks:
                                    llinks.append(str(link.css('a').attrib['href']))
                                    if "http" in str(link.css("a").attrib["href"]):
                                        yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title": link.css('a::text').get().strip(),
                                       "excerpt": 'na',
                                       "published_date":'na',
                                       "pdf_url": link.css("a").attrib["href"].replace('..', '')
                                       }
                                    else:
                                        yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title": link.css('a::text').get().strip(),
                                       "excerpt": 'na',
                                       "published_date":'na',
                                       "pdf_url": "http://www.agricultureauthority.go.ke" + link.css("a").attrib["href"].replace('..', '')
                                       }
                    except:
                        pass

        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1
#26

#27
class BsiBundPdfSpider(scrapy.Spider):
    name = "BsiBundPdfSpider"
    allowed_domains = ["bsi.bund.de"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                categories_links = []

                res = response.css('.links li a')
                for r in res:
                    try:

                        if "http" in str(r.css("a").attrib["href"]) or "http://" in str(r.css("a").attrib["href"]):
                            categories_links.append(r.css("a").attrib["href"])

                        else:
                            categories_links.append("https://www.bsi.bund.de/"+ r.css("a").attrib["href"])
                    except:
                        pass



                for i in set(categories_links):
                    try:
                        yield scrapy.Request(
                            i,
                            callback=self.article_links,
                            meta={"dont_retry": True, "download_timeout": cat_timeout,"base_url": response.url},
                            errback=handle_error,
                        )


                    except:
                        pass
    llinks =[]
    def article_links(self, response, llinks = []):
        if str(response.status) == "200":
            if response.css("body"):
                base_url = response.meta.get('base_url')
                try:
                    data = response.css('.c-link--download')

                    for link in data:
                        if str(link.css("a").attrib["href"]) not in llinks:
                            llinks.append(str(link.css("a").attrib["href"]))
                            if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                                pass
                            else:
                                if ".pdf" in str(link.css('a').attrib['href']):
                                    if "http" in str(link.css("a").attrib["href"]):
                                        yield {"clean_url": self.allowed_domains[0],
                                       "base_url": base_url if not base_url.endswith('/') else base_url[:-1],
                                       "title": link.css('a::text').get().strip(),
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": link.css("a").attrib["href"],
                                       }
                                    else:
                                        yield {"clean_url": self.allowed_domains[0],
                                       "base_url": base_url if not base_url.endswith('/') else base_url[:-1],
                                       "title":link.css('a::text').get().strip(),
                                       "excerpt": 'na',
                                       "published_date":'na',
                                       "pdf_url": "https://www.bsi.bund.de" + link.css("a").attrib["href"],
                                       }

                except:
                        pass
        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1
#28
class BmelPdfSpider(scrapy.Spider):
    name = "BmelPdfSpider"
    allowed_domains = ["bmel.de"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i + "?resultsPerPage=50",
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                llinks = []

                try:
                    data = response.css('.c-searchteaser__h a')

                    for link in data:
                        if str(link.css("a").attrib["href"]) not in llinks:
                            llinks.append(str(link.css("a").attrib["href"]))
                            if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                                pass
                            else:

                                if "https://" in str(link.css("a").attrib["href"]):
                                    yield {"clean_url": self.allowed_domains[0],
                                           "base_url": self.start_urls[0] if not self.start_urls[0].endswith('/') else
                                           self.start_urls[0][
                                           :-1],
                                           "title": link.css('a::text').get().strip() if link.css('a::text') else 'na',
                                           "excerpt": 'na',
                                           "published_date": 'na',
                                           "pdf_url": link.css("a").attrib["href"],
                                           }
                                else:
                                    yield {"clean_url": self.allowed_domains[0],
                                           "base_url": self.start_urls[0] if not self.start_urls[0].endswith('/') else
                                           self.start_urls[0][
                                           :-1],
                                           "title": link.css('a::text').get().strip() if link.css('a::text') else 'na',
                                           "excerpt": 'na',
                                           "published_date": 'na',
                                           "pdf_url": "https://www.bmel.de" + link.css("a").attrib["href"],
                                           }

                    try:
                        next_page = response.css('.c-pagination__item a')
                        if next_page:
                            for i in next_page:
                                if 'http' in i.css('a').attrib['href']:
                                    yield scrapy.Request(
                                        i.css('a').attrib['href'],
                                        callback=self.parse,
                                        meta={"dont_retry": True, "download_timeout": cat_timeout,
                                              "base_url": response.url},
                                        errback=handle_error,
                                    )
                                else:
                                    yield scrapy.Request(
                                        "https://www.bmel.de/" + i.css('a').attrib['href'],
                                        callback=self.parse,
                                        meta={"dont_retry": True, "download_timeout": cat_timeout,
                                              "base_url": response.url},
                                        errback=handle_error,
                                    )
                    except:
                        pass

                except:
                    pass


class TreasuryPdfSpider(scrapy.Spider):
    name = "TreasuryPdfSpider"
    allowed_domains = ["treasury.go.ke"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):

                data = response.css(".wpb_content_element a")
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if ".pdf" in str(link.css('a').attrib['href']):
                                if "http" in str(link.css("a").attrib["href"]):
                                    yield {"clean_url": self.allowed_domains[0],
                                           "base_url": response.url if not response.url.endswith('/') else response.url[
                                                                                                           :-1],
                                           "title": link.css('a::text').get().strip(),
                                           "excerpt": 'na',
                                           "published_date": 'na',
                                           "pdf_url": link.css("a").attrib["href"].replace('..', '')
                                           }
                                else:
                                    yield {"clean_url": self.allowed_domains[0],
                                           "base_url": response.url if not response.url.endswith('/') else response.url[
                                                                                                           :-1],
                                           "title": 'na',
                                           "excerpt": 'na',
                                           "published_date": 'na',
                                           "pdf_url": "https://treasury.go.ke" + link.css("a").attrib["href"].replace('..',
                                                                                                                   '')
                                           }
                    except:
                        pass

        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


#29
class BmwkPdfSpider(scrapy.Spider):
    name = "BmwkPdfSpider"
    allowed_domains = ["bmwk.de"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={ "dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                categories_links = []

                res = response.css('.card-link-overlay')
                for r in res:
                    try:

                        if "https://" in str(r.css("a").attrib["href"]) or "http://" in str(r.css("a").attrib["href"]):
                            categories_links.append(r.css("a").attrib["href"])

                        else:
                            categories_links.append("https://www.bmwk.de/"+ r.css("a").attrib["href"])
                    except:
                        pass



                for i in set(categories_links):
                    try:
                        yield scrapy.Request(
                            i,
                            callback=self.article_links,
                            meta={"dont_retry": True, "download_timeout": cat_timeout,"base_url": response.url},
                            errback=handle_error,
                        )


                    except:
                        pass
    llinks =[]
    def article_links(self, response, llinks = []):
        if str(response.status) == "200":
            if response.css("body"):
                base_url = response.meta.get('base_url')
                try:
                    data = response.css('.link.link-download')

                    for link in data:
                        if str(link.css("a").attrib["href"]) not in llinks:
                            llinks.append(str(link.css("a").attrib["href"]))
                            if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                                pass
                            else:

                                    if "http" in str(link.css("a").attrib["href"]):
                                        yield {"clean_url": self.allowed_domains[0],
                                       "base_url": base_url if not base_url.endswith('/') else base_url[:-1],
                                       "title": 'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": link.css("a").attrib["href"],
                                       }
                                    else:
                                        yield {"clean_url": self.allowed_domains[0],
                                       "base_url": base_url if not base_url.endswith('/') else base_url[:-1],
                                       "title":'na',
                                       "excerpt": 'na',
                                       "published_date":'na',
                                       "pdf_url": "" + link.css("a").attrib["href"],
                                       }

                except:
                        pass
        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1
#30
class Bmbf1PdfSpider(scrapy.Spider):
    name = "Bmbf1PdfSpider"
    allowed_domains = ["bmbf.de"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy,"dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                llinks = []
                data = response.css(".FTpdf")

                for link in data:
                    try:
                            if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                                pass
                            else:
                                if str(link.css("a").attrib['href']) not in llinks:
                                    llinks.append(str(link.css('a').attrib['href']))
                                    if "http" in str(link.css("a").attrib["href"]):
                                        yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title": link.css('a::text').get().strip(),
                                       "excerpt": 'na',
                                       "published_date":'na',
                                       "pdf_url": link.css("a").attrib["href"].replace('..', '')
                                       }
                                    else:
                                        yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title": link.css('a::text').get().strip(),
                                       "excerpt": 'na',
                                       "published_date":'na',
                                       "pdf_url": "https://www.bmbf.de" + link.css("a").attrib["href"].replace('..', '')
                                       }
                    except:
                        pass

        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1
#31
class Bmbf2PdfSpider(scrapy.Spider):
    name = "Bmbf2PdfSpider"
    allowed_domains = ["bmbf.de"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy,"dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                llinks = []
                data = response.css(".c-download-teaser__link")

                for link in data:
                    try:
                            if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                                pass
                            else:
                                if str(link.css("a").attrib['href']) not in llinks:
                                    llinks.append(str(link.css('a').attrib['href']))
                                    if "http" in str(link.css("a").attrib["href"]):
                                        yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title": link.css('a::text').get().strip(),
                                       "excerpt": 'na',
                                       "published_date":'na',
                                       "pdf_url": link.css("a").attrib["href"].replace('..', '')
                                       }
                                    else:
                                        yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title": link.css('a::text').get().strip(),
                                       "excerpt": 'na',
                                       "published_date":'na',
                                       "pdf_url": "https://www.bmbf.de" + link.css("a").attrib["href"].replace('..', '')
                                       }
                    except:
                        pass

        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1
#32
class BmiBundPdfSpider(scrapy.Spider):
    name = "BmiBundPdfSpider"
    allowed_domains = ["bmi.bund.de"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={ "dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                categories_links = []

                res = response.css('.c-search-teaser__link')
                for r in res:
                    try:

                        if "https://" in str(r.css("a").attrib["href"]) or "http://" in str(r.css("a").attrib["href"]):
                            categories_links.append(r.css("a").attrib["href"])

                        else:
                            categories_links.append("https://www.bmi.bund.de/"+ r.css("a").attrib["href"])
                    except:
                        pass



                for i in set(categories_links):
                    try:
                        yield scrapy.Request(
                            i,
                            callback=self.article_links,
                            meta={"dont_retry": True, "download_timeout": cat_timeout,"base_url": response.url},
                            errback=handle_error,
                        )


                    except:
                        pass
    llinks =[]
    def article_links(self, response, llinks = []):
        if str(response.status) == "200":
            if response.css("body"):
                base_url = response.meta.get('base_url')
                try:
                    data = response.css('.c-content-linklist__l--Publication')

                    for link in data:
                        if str(link.css("a").attrib["href"]) not in llinks:
                            llinks.append(str(link.css("a").attrib["href"]))
                            if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                                pass
                            else:

                                    if "http" in str(link.css("a").attrib["href"]):
                                        yield {"clean_url": self.allowed_domains[0],
                                       "base_url": base_url if not base_url.endswith('/') else base_url[:-1],
                                       "title": 'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": link.css("a").attrib["href"],
                                       }
                                    else:
                                        yield {"clean_url": self.allowed_domains[0],
                                       "base_url": base_url if not base_url.endswith('/') else base_url[:-1],
                                       "title":'na',
                                       "excerpt": 'na',
                                       "published_date":'na',
                                       "pdf_url": "https://www.bmi.bund.de" + link.css("a").attrib["href"],
                                       }

                except:
                        pass
        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1
#33
class WaterPdfSpider(scrapy.Spider):
    name = "WaterPdfSpider"
    allowed_domains = ["water.go.ke"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy,"dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                llinks = []
                data = response.css("strong a")
                for link in data:
                    try:
                            if any(n in str(link.css("a").attrib["onclick"]) for n in self.not_allowed_keyword):
                                pass
                            else:
                                if str(link.css("a").attrib['onclick']) not in llinks:
                                    llinks.append(str(link.css('a').attrib['onclick']))
                                    if "http" in str(link.css("a").attrib["onclick"]):
                                        yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title": 'na',
                                       "excerpt": 'na',
                                       "published_date":'na',
                                       "pdf_url": link.css("a").attrib["onclick"].replace("location.href='", "").replace("';return false;", "")
                                       }
                                    else:
                                        yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title": 'na',
                                       "excerpt": 'na',
                                       "published_date":'na',
                                       "pdf_url": "" + link.css("a").attrib["onclick"].replace("location.href='", "").replace("';return false;", "")
                                       }
                    except:
                        pass

        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class BusinessStandardSpider(scrapy.Spider):
    name = "BusinessStandardSpider"
    allowed_domains = ["business-standard.com"]
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def parse(self, response):
        categories_links = []

        data = response.css(
            "#nifty-tab3 a , .listing-txt h2 a,.aticle-list h2,.aticle-txt h2 a , .top-section h2 , .webexclu1 , .webexclu0,.size16 span , #bs-new-top-story-listing-ajax-block a"
        )
        for link in data:
            try:
                yield {"clean_url": self.allowed_domains[0],
                       "base_url": response.url,
                       "link": "https://www.business-standard.com"
                               + link.css("a").attrib["href"]
                       }
            except:
                pass

class businesstodaySpider(scrapy.Spider):
    name = "businesstodaySpider"
    allowed_domains = ["businesstoday.in"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.bs_title,h3 a,.widget-listing-content-section a,.bse_blt_li a,.crp_tdy_rhs a')
                for link in set(data):
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]}
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www.businesstoday.in" + link.css("a").attrib["href"],
                                       "type": 'article'}
                    except:
                        pass




class CnbcTv18Spider(scrapy.Spider):
    name = "CnbcTv18Spider"
    allowed_domains = ["cnbctv18.com"]
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res


    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.jsx-7bef0b538bcdb25b a')
                for link in set(data):
                    try:
                        if "http" in str(link.css("a").attrib["href"]):
                            yield {"clean_url": self.allowed_domains[0],
                                    "base_url": response.url,
                                    "link":link.css("a").attrib["href"]}

                        else:
                            yield{"clean_url": self.allowed_domains[0],
                                    "base_url": response.url,
                                "link": "https://www.cnbctv18.com"
                                + link.css("a").attrib["href"]
                            }
                    except:
                        pass



class dnaindiaSpider(scrapy.Spider):
    name = "dnaindiaSpider"
    allowed_domains = ["dnaindia.com"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res



    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.explainer-subtext a,.list-news a')
                for link in set(data):
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"].replace(' ',""),

                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www.dnaindia.com" + link.css("a").attrib["href"].replace(' ',""),

                                       }

                    except:
                        pass



class HindustanTimesSpider(scrapy.Spider):
    name = "HindustanTimesSpider"
    allowed_domains = ["hindustantimes.com"]
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css(".hdg3 a")
                for link in data:
                    try:
                        if "http" in str(link.css("a").attrib["href"]):
                            yield {"clean_url": self.allowed_domains[0],
                                   "base_url": response.url,
                                   "link": link.css("a").attrib["href"]
                                   }
                        else:
                            yield {"clean_url": self.allowed_domains[0],
                                   "base_url": response.url,
                                   "link": "https://www.hindustantimes.com"
                                           + link.css("a").attrib["href"]
                                   }
                    except:
                        pass



class IndiaTodaySpider(scrapy.Spider):
    name = "IndiaTodaySpider"
    allowed_domains = ["indiatoday.in"]
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res


    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('h3 a,h2 a')
                for link in data:
                    try:
                        if "http" in str(link.css("a").attrib["href"]):
                            yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                "link": link.css("a").attrib["href"]}
                        else:
                            yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                "link": "https://www.indiatoday.in"
                                + link.css("a").attrib["href"]
                            }

                    except:
                        pass


class IndiaTVSpider(scrapy.Spider):
    name = "IndiaTVSpider"
    allowed_domains = ["indiatvnews.com"]
    not_allowed_keyword =['javascript:void']
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        res_ip = scrapy.Request(
            self.start_urls,
            meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
            callback=self.parse,
            dont_filter=True,
            errback=handle_error,
        )
        logging.info(
            {
                "proxy": "1",
                "clean_url": self.allowed_domains[0],
                "link": self.start_urls,
            }
        )
        yield res_ip

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css(".two_column a")
                for link in set(data):
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if 'http' in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                    "base_url": response.url,
                                    "link": link.css("a").attrib["href"]}
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                    "base_url": response.url,
                                    "link": "https://www.indiatvnews.com" + link.css("a").attrib["href"]}
                    except:
                        pass


class midDaySpider(scrapy.Spider):
    name = "midDaySpider"
    allowed_domains = ["mid-day.com"]
    not_allowed_keyword = ['{{home', '{{lifestyl','{{subcatdata','{{mumbaiguide','{{entertainment','{{moviesreview','{photogallery','{{news']
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res


    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.right16 , .shortbox , .pr-3 , .list-item li, .title-news-heading a')
                for link in set(data):
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"],

                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www.mid-day.com" + link.css("a").attrib["href"],

                                       }

                    except:
                        pass


class MoneyControlSpider(scrapy.Spider):
    name = "MoneyControlSpider"
    allowed_domains = ["moneycontrol.com"]
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res


    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css(".main-left .mob-hide .clearfix div :nth-child(1), h2 a")
                for link in set(data):
                    try:
                        if "http" in str(link.css("a").attrib["href"]):
                            yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                "link": link.css("a").attrib["href"]}
                        else:
                            yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                "link": "https://www.moneycontrol.com"
                                + link.css("a").attrib["href"]
                            }
                    except:
                        pass


class NDTVSpider(scrapy.Spider):
    name = "NDTVSpider"
    allowed_domains = ["ndtv.com"]
    start_urls = ["https://www.ndtv.com/", "https://www.ndtv.com/latest", 'https://www.ndtv.com/trends']
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css(".item-title , h3 a,.newsHdng a")
                for link in set(data):
                    try:
                        if "https" in str(link.css("a").attrib["href"]):
                            yield {"clean_url": self.allowed_domains[0],
                                   "base_url": response.url,
                                   "link": link.css("a").attrib["href"]}
                        else:
                            yield {"clean_url": self.allowed_domains[0],
                                   "base_url": response.url,
                                   "link": "https://www.ndtv.com"
                                           + link.css("a").attrib["href"]
                                   }
                    except:
                        pass

        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_category += 1



class newsNINEliveSpider(scrapy.Spider):
    name = "newsNINEliveSpider"
    allowed_domains = ["news9live.com"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.flexBox+ a , h3 a')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "https://" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"],

                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www.news9live.com" + link.css("a").attrib["href"],

                                       }

                    except:
                        pass


class PoliticoSpider(scrapy.Spider):
    name = "PoliticoSpider"
    allowed_domains = ["politico.com"]
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res


    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css(".is-standard-typeface .js-tealium-tracking,h3 a , .size-l, #main h1 a")
                for link in data:
                    try:
                        if "http" in str(link.css("a").attrib["href"]):
                            yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                "link": link.css("a").attrib["href"]}
                        else:
                            yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                "link": "https://www.politico.com"
                                        + link.css("a").attrib["href"]
                            }
                    except:
                        pass


class TheHinduSpider(scrapy.Spider):
    name = "TheHinduSpider"
    allowed_domains = ["thehindu.com"]
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css(
                    ".story1-3x100-heading , h3 a , .story-card-news h2 a"
                )
                for link in data:
                    try:
                        yield {"clean_url": self.allowed_domains[0],
                               "base_url": response.url,
                               "link": link.css("a").attrib["href"]}
                    except:
                        pass



class IndianExpressSpider(scrapy.Spider):
    name = "IndianExpressSpider"
    allowed_domains = ["indianexpress.com"]
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res


    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css(
                    "#HP_LATEST_NEWS a , .top-news a , .ie-first-story a,.title a"
                )
                for link in set(data):
                    try:
                        if "https" in str(link.css("a").attrib["href"]):
                            yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                "link": link.css("a").attrib["href"]}
                        else:
                            yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                "link": "https://indianexpress.com"
                                + link.css("a").attrib["href"]
                            }
                    except:
                        pass


class tribuneindiaSpider(scrapy.Spider):
    name = "tribuneindiaSpider"
    allowed_domains = ["tribuneindia.com"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.card-top-align')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "https" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]}
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www.tribuneindia.com" + link.css("a").attrib["href"]}
                    except:
                        pass


class WSJSpider(scrapy.Spider):
    name = "WSJSpider"
    allowed_domains = ["wsj.com"]
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css(".typography--serif-display--ZXeuhS5E,.reset")
                for link in data:
                    try:
                        if "https" in str(link.css("a").attrib["href"]):
                            yield {"clean_url": self.allowed_domains[0],
                                   "base_url": response.url,
                                   "link": link.css("a").attrib["href"]}
                        else:
                            yield {"clean_url": self.allowed_domains[0],
                                   "base_url": response.url,
                                   "link": "https://www.wsj.com"
                                           + link.css("a").attrib["href"]
                                   }
                    except:
                        pass



class TimesNowNewsSpider(scrapy.Spider):
    name = "TimesNowNewsSpider"
    allowed_domains = ["timesnownews.com"]
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res


    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('._2LXp3._2jF__ a,._1W5s9 a')
                for link in data:
                    try:
                        if "http" in str(link.css('a').attrib['href']):
                            yield {"clean_url": self.allowed_domains[0],
                                        "base_url": response.url,
                                "link": link.css("a").attrib["href"]
                            }
                        else:
                            yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                            "link": "https://www.timesnownews.com"
                            + link.css("a").attrib["href"]
                        }
                    except:
                        pass


class timeforkidsSpider(scrapy.Spider):
    name = "timeforkidsSpider"
    allowed_domains = ["timeforkids.com"]
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def parse(self, response):

        if str(response.status) == "200":
            if response.css("body"):
                data = response.css(".c-article-preview__title")

                for link in data:
                    try:
                        if "http" in str(link.css("a").attrib["href"]):
                            yield {"clean_url": self.allowed_domains[0],
                                   "base_url": response.url,
                                   "link": link.css("a").attrib["href"]
                                   }
                        else:
                            yield {"clean_url": self.allowed_domains[0],
                                   "base_url": response.url,
                                   "link": "https://www.timeforkids.com" + link.css("a").attrib["href"]
                                   }

                    except:
                        pass


class youngzineSpider(scrapy.Spider):
    name = "youngzineSpider"
    allowed_domains = ["youngzine.org"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res


    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.views-column')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"],

                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://youngzine.org" + link.css("a").attrib["href"],

                                       }
                    except:
                        pass


class mashableSpider(scrapy.Spider):
    name = "mashableSpider"
    allowed_domains = ["mashable.com"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res


    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.justify-center, .flex-1,.xl\:relative , .flex-1,.hero_mob_box a,.box_title a')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]}
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://mashable.com" + link.css("a").attrib["href"]}
                    except:
                        pass


class AbpliveSpider(scrapy.Spider):
    name = 'AbpliveSpider'
    allowed_domains = ['news.abplive.com']
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            print(i)
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            print(i)
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )
            yield res_ip

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):

                data = response.css(
                    '.news_featured,.home-hero-news,a.other_news,.el-hero-news,.el-subnews,a.news_content,.first-news-item')
                for link in data:
                    try:
                        if any(n in str(link.attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if str(link.attrib["href"]).count('/') >= 1:
                                if "http" in str(link.attrib["href"]):
                                    yield {
                                        "base_url": response.url,
                                        "link": link.attrib["href"],
                                    }
                                else:
                                    yield {
                                        "base_url": response.url,
                                        "link": "https://news.abplive.com" + link.attrib["href"],
                                    }

                    except Exception as e:
                        print(link.text)
                        print(link)
                        print(e)
                data_div = response.css('div.news_content, div.other_news')
                for link in data_div:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if str(link.css("a").attrib["href"]).count('/') >= 1:
                                if "http" in str(link.css("a").attrib["href"]):
                                    yield {
                                        "base_url": response.url,
                                        "link": link.css("a").attrib["href"],
                                    }
                                else:
                                    yield {
                                        "base_url": self.start_urls,
                                        "link": "https://news.abplive.com" + link.css("a").attrib["href"],
                                    }


                    except Exception as e:
                        print(link.text)
                        print(link)
                        print(e)
        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_category += 1


class AljazeeraSpider(scrapy.Spider):
    name = 'AljazeeraSpider'
    allowed_domains = ['aljazeera.com']
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )
            yield res_ip

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):

                data = response.css('article')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if str(link.css("a").attrib["href"]).count('/') >= 1:
                                if "http" in str(link.css("a").attrib["href"]):
                                    yield {
                                        "base_url": response.url,
                                        "link": link.css("a").attrib["href"],
                                    }
                                else:
                                    yield {
                                        "base_url": response.url,
                                        "link": "http://www.aljazeera.com" + link.css("a").attrib["href"],
                                    }

                    except:
                        pass
        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_category += 1


class AniSpider(scrapy.Spider):
    name = 'AniSpider'
    allowed_domains = ['aninews.in']
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )
            yield res_ip

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):

                data = response.css('a.link-data')
                for link in data:
                    try:
                        if any(n in str(link.attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if str(link.attrib["href"]).count('/') >= 1:
                                if "http" in str(link.attrib["href"]):
                                    yield {
                                        "base_url": response.url,
                                        "link": link.attrib["href"],
                                    }
                                else:
                                    yield {
                                        "base_url": response.url,
                                        "link": "https://aninews.in/" + link.attrib["href"],
                                    }

                    except:
                        pass
                data_div = response.css('.bottom')
                for link in data_div:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if str(link.css("a").attrib["href"]).count('/') >= 1:
                                if "http" in str(link.css("a").attrib["href"]):
                                    yield {
                                        "base_url": response.url,
                                        "link": link.css("a").attrib["href"],
                                    }
                                else:
                                    yield {
                                        "base_url": response.url,
                                        "link": "https://aninews.in/" + link.css("a").attrib["href"],
                                    }

                    except:
                        pass
        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_category += 1


class AsianageSpider(scrapy.Spider):
    name = 'AsianageSpider'
    allowed_domains = ['asianage.com']
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )
            yield res_ip

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):

                data = response.css('h3 a,h2 a')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if str(link.css("a").attrib["href"]).count('/') >= 1:
                                if "http" in str(link.css("a").attrib["href"]):
                                    yield {
                                        "base_url": response.url,
                                        "link": link.css("a").attrib["href"],
                                    }
                                else:
                                    yield {
                                        "base_url": response.url,
                                        "link": "http://www.asianage.com" + link.css("a").attrib["href"],
                                    }

                    except:
                        pass
        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_category += 1


class AsianetnewsSpider(scrapy.Spider):
    name = 'AsianetnewsSpider'
    allowed_domains = ['newsable.asianetnews.com']
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )
            yield res_ip

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):

                data = response.css(
                    '.widget01-post, .widget0_1-three-txt, .t-small-txt, .post-content, .wid11-main-post, ul.sports-rel-post>li, .video-l-item, div.video-r-item>ul>li, .owl-item, .related-posts>ul>li, .widget04-post, ul.wid12-post-listing>li, .widget05-post, .related-post-txt, .category-txt-content, .title-p')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if str(link.css("a").attrib["href"]).count('/') >= 1:
                                if "http" in str(link.css("a").attrib["href"]):
                                    yield {
                                        "base_url": response.url,
                                        "link": link.css("a").attrib["href"],
                                    }
                                else:
                                    yield {
                                        "base_url": response.url,
                                        "link": "http://newsable.asianetnews.com" + link.css("a").attrib["href"],
                                    }

                    except:
                        pass
        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_category += 1


class BqprimeSpider(scrapy.Spider):
    name = 'BqprimeSpider'
    allowed_domains = ['bqprime.com']
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )
            yield res_ip

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):

                data = response.css(
                    '.stories-eight-2lx6s-m__shimmer-card__1HL52, .list-title, .card-with-reaction-title, .card-image-headline-horizontal, .marketsResearchReportsCollection-m__research-reports-list__JNpVJ, .story-card, .card-image-reaction-headline, .cardHeadlineAuthorVertical-m__wrapper__esCCA, .story-card-vertical-with-image-content-m__story-content__1XhUb, .story-card-vertical-with-image-all-content-m__story-card__MnGIx, .story-card-with-large-image-all-content-m__story-card__1GTZb, .story-card-img-title, .image-and-title-m__story-details__2dlVe')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if str(link.css("a").attrib["href"]).count('/') >= 1:
                                if "http" in str(link.css("a").attrib["href"]):
                                    yield {
                                        "base_url": response.url,
                                        "link": link.css("a").attrib["href"],
                                    }
                                else:
                                    yield {
                                        "base_url": response.url,
                                        "link": "http://www.bqprime.com" + link.css("a").attrib["href"],
                                    }

                    except:
                        pass
        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_category += 1


class CbccaSpider(scrapy.Spider):
    name = 'CbccaSpider'
    allowed_domains = ['cbc.ca']
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )
            yield res_ip

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):

                data = response.css('a.contentWrapper, a.card')
                for link in data:
                    try:
                        if any(n in str(link.attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if str(link.attrib["href"]).count('/') >= 1:
                                if "http" in str(link.attrib["href"]):
                                    yield {
                                        "base_url": response.url,
                                        "link": link.attrib["href"],
                                    }
                                else:
                                    yield {
                                        "base_url": response.url,
                                        "link": "https://www.cbc.ca" + link.attrib["href"],
                                    }

                    except:
                        pass
        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_category += 1


class Cnn10Spider(scrapy.Spider):
    name = 'Cnn10Spider'
    allowed_domains = ['edition.cnn.com']
    not_allowed_keyword = ['/specials/cnn10-newsletter', 'youtube.com', '/videos/']
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )
            yield res_ip

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):

                data = response.css('h3')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if str(link.css("a").attrib["href"]).count('/') >= 1:
                                if "http" in str(link.css("a").attrib["href"]):
                                    yield {
                                        "base_url": response.url,
                                        "link": link.css("a").attrib["href"],
                                    }
                                else:
                                    yield {
                                        "base_url": response.url,
                                        "link": "https://edition.cnn.com/cnn10" + link.css("a").attrib["href"],
                                    }

                    except:
                        pass
        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_category += 1


class DeccanchronicleSpider(scrapy.Spider):
    name = 'DeccanchronicleSpider'
    allowed_domains = ['deccanchronicle.com']
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )
            yield res_ip

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):

                data = response.css(
                    '.stry-top-big-a, .tstry-feed-sml-a, .stry-feed-sml-a, .cmt-top-sml-a,.col-xs-12, .col-sm-8')
                data = response.css('.col-xs-12')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if str(link.css("a").attrib["href"]).count('/') >= 4:
                                if "http" in str(link.css("a").attrib["href"]):
                                    yield {
                                        "base_url": response.url,
                                        "link": link.css("a").attrib["href"],
                                    }
                                else:
                                    yield {
                                        "base_url": response.url,
                                        "link": "https://www.deccanchronicle.com" + link.css("a").attrib["href"],
                                    }

                    except:
                        pass
        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_category += 1


class EfeSpider(scrapy.Spider):
    name = 'EfeSpider'
    allowed_domains = ['efe.com']
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )
            yield res_ip

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):

                data = response.css('.anwp-pg-post-teaser__title, .elementor-heading-title, .entry-title')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if str(link.css("a").attrib["href"]).count('/') >= 1:
                                if "http" in str(link.css("a").attrib["href"]):
                                    yield {
                                        "base_url": response.url,
                                        "link": link.css("a").attrib["href"],
                                    }
                                else:
                                    yield {
                                        "base_url": response.url,
                                        "link": "https://efe.com/" + link.css("a").attrib["href"],
                                    }

                    except:
                        pass
        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_category += 1


class GulftodaySpider(scrapy.Spider):
    name = 'GulftodaySpider'
    allowed_domains = ['gulftoday.ae']
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )
            yield res_ip

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):

                data = response.css('a.tile-card')
                for link in data:
                    try:
                        if any(n in str(link.attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if str(link.attrib["href"]).count('/') >= 1:
                                if "http" in str(link.attrib["href"]):
                                    yield {
                                        "base_url": response.url,
                                        "link": link.attrib["href"],
                                    }
                                else:
                                    yield {
                                        "base_url": response.url,
                                        "link": "https://www.gulftoday.ae" + link.attrib["href"],
                                    }

                    except:
                        pass
        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_category += 1


class IndiaSpider(scrapy.Spider):
    name = 'IndiaSpider'
    allowed_domains = ['india.com']
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )
            yield res_ip

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):

                data = response.css('article')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if str(link.css("a").attrib["href"]).count('/') >= 1:
                                if "http" in str(link.css("a").attrib["href"]):
                                    yield {
                                        "base_url": response.url,
                                        "link": link.css("a").attrib["href"],
                                    }
                                else:
                                    yield {
                                        "base_url": response.url,
                                        "link": "https://www.india.com" + link.css("a").attrib["href"],
                                    }

                    except:
                        pass
        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_category += 1


class KidsnatgeoSpider(scrapy.Spider):
    name = 'KidsnatgeoSpider'
    allowed_domains = ['kids.nationalgeographic.com']
    not_allowed_keyword = ['/topic/']
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )
            yield res_ip

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):

                data = response.css('.ListItem, .RegularStandardPrismTile')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if str(link.css("a").attrib["href"]).count('/') >= 4:
                                if "http" in str(link.css("a").attrib["href"]):
                                    yield {
                                        "base_url": response.url,
                                        "link": link.css("a").attrib["href"],
                                    }
                                else:
                                    yield {
                                        "base_url": response.url,
                                        "link": "https://kids.nationalgeographic.com/" + link.css("a").attrib["href"],
                                    }

                    except:
                        pass

        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_category += 1


class KnnindiaSpider(scrapy.Spider):
    name = 'KnnindiaSpider'
    allowed_domains = ['knnindia.co.in']
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )
            yield res_ip

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):

                data = response.css('ul.article-array>li, .article-header, .title, .user-nick')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if str(link.css("a").attrib["href"]).count('/') >= 1:
                                if "http" in str(link.css("a").attrib["href"]):
                                    yield {
                                        "base_url": response.url,
                                        "link": link.css("a").attrib["href"],
                                    }
                                else:
                                    yield {
                                        "base_url": response.url,
                                        "link": "https://knnindia.co.in" + link.css("a").attrib["href"],
                                    }

                    except:
                        pass
        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_category += 1


class LatestlySpider(scrapy.Spider):
    name = 'LatestlySpider'
    allowed_domains = ['latestly.com']
    not_allowed_keyword = ['/videos/']
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )
            yield res_ip

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):

                data = response.css('a.story_title_alink')
                for link in data:
                    try:
                        if any(n in str(link.attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if str(link.attrib["href"]).count('/') >= 1:
                                if "http" in str(link.attrib["href"]):
                                    yield {
                                        "base_url": response.url,
                                        "link": link.attrib["href"],
                                    }
                                else:
                                    yield {
                                        "base_url": response.url,
                                        "link": "https://www.latestly.com/" + link.attrib["href"],
                                    }

                    except:
                        pass
        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_category += 1


class MintSpider(scrapy.Spider):
    name = 'MintSpider'
    allowed_domains = ['livemint.com']
    not_allowed_keyword = ['podcast/']
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )
            yield res_ip

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):

                data = response.css('h2,h3')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if str(link.css("a").attrib["href"]).count('/') >= 1:
                                if "http" in str(link.css("a").attrib["href"]):
                                    yield {
                                        "base_url": response.url,
                                        "link": link.css("a").attrib["href"],
                                    }
                                else:
                                    yield {
                                        "base_url": response.url,
                                        "link": "https://www.livemint.com" + link.css("a").attrib["href"],
                                    }

                    except:
                        pass
        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_category += 1


class MintgenieSpider(scrapy.Spider):
    name = 'MintgenieSpider'
    allowed_domains = ['mintgenie.livemint.com']
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )
            yield res_ip

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):

                data = response.css(
                    '.col-lg-7, .top-news-list-container__item, .latestSmallNewsContainer, .latest-stories-slider__item, .more-news-container__item')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if str(link.css("a").attrib["href"]).count('/') >= 1:
                                if "http" in str(link.css("a").attrib["href"]):
                                    yield {
                                        "base_url": response.url,
                                        "link": link.css("a").attrib["href"],
                                    }
                                else:
                                    yield {
                                        "base_url": response.url,
                                        "link": "https://mintgenie.livemint.com" + link.css("a").attrib["href"],
                                    }

                    except:
                        pass
        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_category += 1


class MykhelSpider(scrapy.Spider):
    name = 'MykhelSpider'
    allowed_domains = ['mykhel.com']
    not_allowed_keyword = ['/photos/']
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )
            yield res_ip

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):

                data = response.css('a.col-block, a.item-thumbnail-href ')
                for link in data:
                    try:
                        if any(n in str(link.attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if str(link.attrib["href"]).count('/') >= 1:
                                if "http" in str(link.attrib["href"]):
                                    yield {
                                        "base_url": response.url,
                                        "link": link.attrib["href"],
                                    }
                                else:
                                    yield {
                                        "base_url": response.url,
                                        "link": "https://www.mykhel.com/" + link.attrib["href"],
                                    }

                    except:
                        pass
        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_category += 1


class News18Spider(scrapy.Spider):
    name = 'News18Spider'
    allowed_domains = ['news18.com']
    not_allowed_keyword = ['photogallery/']
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )
            yield res_ip

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):

                data = response.css('figure, .sub_headstory_title, div.home_section_row>ul>li')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if str(link.css("a").attrib["href"]).count('/') >= 1:
                                if "http" in str(link.css("a").attrib["href"]):
                                    yield {
                                        "base_url": response.url,
                                        "link": link.css("a").attrib["href"],
                                    }
                                else:
                                    yield {
                                        "base_url": response.url,
                                        "link": "https://www.news18.com/" + link.css("a").attrib["href"],
                                    }

                    except:
                        pass

                data_a = response.css(
                    'a.jsx-679621898, a.jsx-2829712739, a.jsx-3826095802, a.jsx-2829712739, a.jsx-2593873025, a.jsx-1813205701, a.jsx-542020546, a.jsx-3621759782')
                for link in data_a:
                    try:
                        if any(n in str(link.attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if str(link.attrib["href"]).count('/') >= 1:
                                if "http" in str(link.attrib["href"]):
                                    yield {
                                        "base_url": response.url,
                                        "link": link.attrib["href"],
                                    }
                                else:
                                    yield {
                                        "base_url": self.start_urls,
                                        "link": "https://www.news18.com/" + link.attrib["href"],
                                    }

                    except:
                        pass
        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_category += 1


class News24Spider(scrapy.Spider):
    name = 'News24Spider'
    allowed_domains = ['news24online.com']
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )
            yield res_ip

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):

                data = response.css('.entry-title')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if str(link.css("a").attrib["href"]).count('/') >= 1:
                                if "http" in str(link.css("a").attrib["href"]):
                                    yield {
                                        "base_url": response.url,
                                        "link": link.css("a").attrib["href"],
                                    }
                                else:
                                    yield {
                                        "base_url": response.url,
                                        "link": "https://news24online.com" + link.css("a").attrib["href"],
                                    }

                    except:
                        pass
        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_category += 1


class PbsnewshourSpider(scrapy.Spider):
    name = 'PbsnewshourSpider'
    allowed_domains = ['pbs.org']
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )
            yield res_ip

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):

                data = response.css(
                    'a.card-md__title, a.card-hp__title, a.series-tease__feature-title, a.card-sm__title, a.card-thumb__link, a.card-lg__title, a.card-timeline__title, a.card-xl__title, a.card-horiz__title')
                for link in data:
                    try:
                        if any(n in str(link.attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if str(link.attrib["href"]).count('/') >= 1:
                                if "http" in str(link.attrib["href"]):
                                    yield {
                                        "base_url": response.url,
                                        "link": link.attrib["href"],
                                    }
                                else:
                                    yield {
                                        "base_url": response.url,
                                        "link": "https://www.pbs.org/newshour" + link.attrib["href"],
                                    }

                    except:
                        pass

                data_div = response.css('.home-hero__title')
                for link in data_div:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if str(link.css("a").attrib["href"]).count('/') >= 1:
                                if "http" in str(link.css("a").attrib["href"]):
                                    yield {
                                        "base_url": response.url,
                                        "link": link.css("a").attrib["href"],
                                    }
                                else:
                                    yield {
                                        "base_url": response.url,
                                        "link": "https://www.pbs.org/newshour" + link.css("a").attrib["href"],
                                    }

                    except:
                        pass
        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_category += 1


class PhysSpider(scrapy.Spider):
    name = 'PhysSpider'
    allowed_domains = ['phys.org']
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )
            yield res_ip

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):

                data = response.css('article')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if str(link.css("a").attrib["href"]).count('/') >= 1:
                                if "http" in str(link.css("a").attrib["href"]):
                                    yield {
                                        "base_url": response.url,
                                        "link": link.css("a").attrib["href"],
                                    }
                                else:
                                    yield {
                                        "base_url": response.url,
                                        "link": "https://phys.org/" + link.css("a").attrib["href"],
                                    }

                    except:
                        pass
        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_category += 1


class PinkvillaSpider(scrapy.Spider):
    name = 'PinkvillaSpider'
    allowed_domains = ['pinkvilla.com']
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )
            yield res_ip

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):

                data = response.css('.article-description, .views-row')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if str(link.css("a").attrib["href"]).count('/') >= 1:
                                if "http" in str(link.css("a").attrib["href"]):
                                    yield {
                                        "base_url": response.url,
                                        "link": link.css("a").attrib["href"],
                                    }
                                else:
                                    yield {
                                        "base_url": response.url,
                                        "link": "https://www.pinkvilla.com" + link.css("a").attrib["href"],
                                    }

                    except:
                        pass
        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_category += 1


class ScrollinSpider(scrapy.Spider):
    name = 'ScrollinSpider'
    allowed_domains = ['scroll.in']
    not_allowed_keyword = ['/video/']
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )
            yield res_ip

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):

                data = response.css('.column, .row-story')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if str(link.css("a").attrib["href"]).count('/') >= 4:
                                if "http" in str(link.css("a").attrib["href"]):
                                    yield {
                                        "base_url": response.url,
                                        "link": link.css("a").attrib["href"],
                                    }
                                else:
                                    yield {
                                        "base_url": response.url,
                                        "link": "https://scroll.in/" + link.css("a").attrib["href"],
                                    }

                    except:
                        pass
        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_category += 1


class ThehansindiaspiderSpider(scrapy.Spider):
    name = 'thehansindiaSpider'
    allowed_domains = ['thehansindia.com']
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )
            yield res_ip

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):

                data = response.css('.ban-inner-content')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if str(link.css("a").attrib["href"]).count('/') >= 1:
                                if "http" in str(link.css("a").attrib["href"]):
                                    yield {
                                        "base_url": response.url,
                                        "link": link.css("a").attrib["href"],
                                    }
                                else:
                                    yield {
                                        "base_url": response.url,
                                        "link": "https://www.thehansindia.com" + link.css("a").attrib["href"],
                                    }

                    except:
                        pass
        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_category += 1


class ThewireinSpider(scrapy.Spider):
    name = 'ThewireinSpider'
    allowed_domains = ['thewire.in']
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )
            yield res_ip

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):

                data = response.css('.card__title')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if str(link.css("a").attrib["href"]).count('/') >= 1:
                                if "http" in str(link.css("a").attrib["href"]):
                                    yield {
                                        "base_url": response.url,
                                        "link": link.css("a").attrib["href"],
                                    }
                                else:
                                    yield {
                                        "base_url": response.url,
                                        "link": "https://thewire.in" + link.css("a").attrib["href"],
                                    }

                    except:
                        pass
        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_category += 1


class TranscontinentaltimesScienceSpider(scrapy.Spider):
    name = 'TranscontinentaltimesScienceSpider'
    allowed_domains = ['transcontinentaltimes.com']
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )
            yield res_ip

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):

                data = response.css('.entry-title')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if str(link.css("a").attrib["href"]).count('/') >= 1:
                                if "http" in str(link.css("a").attrib["href"]):
                                    yield {
                                        "base_url": response.url,
                                        "link": link.css("a").attrib["href"],
                                    }
                                else:
                                    yield {
                                        "base_url": response.url,
                                        "link": "https://www.transcontinentaltimes.com" + link.css("a").attrib["href"],
                                    }

                    except:
                        pass
        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_category += 1


class TwinklSpider(scrapy.Spider):
    name = 'TwinklSpider'
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )
            yield res_ip

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):

                data = response.css('a.newsroom-item')
                for link in data:
                    try:
                        if any(n in str(link.attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if str(link.attrib["href"]).count('/') >= 1:
                                if "http" in str(link.attrib["href"]):
                                    yield {
                                        "base_url": response.url,
                                        "link": link.attrib["href"],
                                    }
                                else:
                                    yield {
                                        "base_url": response.url,
                                        "link": "https://www.twinkl.co.in" + link.attrib["href"],
                                    }

                    except:
                        pass
        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_category += 1


class ZeenewsrSpider(scrapy.Spider):
    name = 'ZeenewsrSpider'
    allowed_domains = ['zeenews.india.com']
    not_allowed_keyword = ['/video/']
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )
            yield res_ip

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):

                data = response.css(
                    '.news_description, .lead_news_title, .story_news_description, .section-thumbnail-title')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if str(link.css("a").attrib["href"]).count('/') >= 1:
                                if "http" in str(link.css("a").attrib["href"]):
                                    yield {
                                        "base_url": response.url,
                                        "link": link.css("a").attrib["href"],
                                    }
                                else:
                                    yield {
                                        "base_url": response.url,
                                        "link": "https://zeenews.india.com" + link.css("a").attrib["href"],
                                    }

                    except:
                        pass
        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_category += 1



class FrontlineSpider(scrapy.Spider):
    name = 'FrontlineSpider'
    allowed_domains = ['frontline.thehindu.com']
    not_allowed_keyword = ['/aboutus/','/contacts/','roofandfloor.thehindu.com/']
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )
            yield res_ip

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):

                data = response.css('.title')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if str(link.css("a").attrib["href"]).count('/') >= 4:
                                if "http" in str(link.css("a").attrib["href"]):
                                    yield {
                                        "base_url": response.url,
                                        "link": link.css("a").attrib["href"],
                                        }
                                else:
                                    yield {
                                        "base_url": self.start_urls,
                                        "link": "https://frontline.thehindu.com" + link.css("a").attrib["href"],
                                        }

                    except:
                        pass
        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_category += 1



class HttechSpider(scrapy.Spider):
    name = 'HttechSpider'
    allowed_domains = ['tech.hindustantimes.com']
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )
            yield res_ip

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):

                data = response.css('a.firstArticle, a.title-link, a.parentWrapper, a.sliderBox')
                for link in data:
                    try:
                        if any(n in str(link.attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if str(link.attrib["href"]).count('/') >= 1:
                                if "http" in str(link.attrib["href"]):
                                    yield {
                                        "base_url": response.url,
                                        "link": link.attrib["href"],
                                        }
                                else:
                                    yield {
                                        "base_url": response.url,
                                        "link": "https://tech.hindustantimes.com" + link.attrib["href"],
                                        }

                    except:
                        pass
        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_category += 1




class BostonGlobeSpider(scrapy.Spider):
    name = "BostonGlobeSpider"
    allowed_domains = ["bostonglobe.com"]
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                categories_links = []
                data = response.css(".card.color_inherit, div.text")
                for link in data:
                    try:
                        if "http" in str(link.css("a").attrib["href"]):
                            yield {"base_url": response.url,
                                "link": link.css("a").attrib["href"]}

                        else:
                            yield {"base_url": response.url,
                                "link": "https://www.bostonglobe.com"
                                + link.css("a").attrib["href"]
                            }

                    except:
                        pass




class espncricinfoCSpider(scrapy.Spider):
    name = "espncricinfoCSpider"
    allowed_domains = ["espncricinfo.com"]
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res


    def parse(self, response):

        if str(response.status) == "200":
            if response.css("body"):
                data = response.css(".ds-border-line.ds-p-4")

                for link in data:
                    try:
                        if "http" in str(link.css("a").attrib["href"]):
                            yield {"clean_url": self.allowed_domains[0],
                                   "base_url": response.url,
                                   "link": link.css("a").attrib["href"]
                                   }
                        else:
                            yield {"clean_url": self.allowed_domains[0],
                                   "base_url": response.url,
                                   "link": "https://www.espncricinfo.com" + link.css("a").attrib["href"]
                                   }

                    except:
                        pass



class firstpostSpider(scrapy.Spider):
    name = "firstpostSpider"
    allowed_domains = ["firstpost.com"]
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"proxy": proxy, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res


    def parse(self, response):

        if str(response.status) == "200":
            if response.css("body"):
                data = response.css(".main-title a")

                for link in data:
                    try:
                        if "http" in str(link.css("a").attrib["href"]):
                            yield {"clean_url": self.allowed_domains[0],
                                   "base_url": response.url,
                                   "link": link.css("a").attrib["href"]
                                   }
                        else:
                            yield {"clean_url": self.allowed_domains[0],
                                   "base_url": response.url,
                                   "link": "https://www.firstpost.com" + link.css("a").attrib["href"]
                                   }

                    except:
                        pass



class FreePressJournalSpider(scrapy.Spider):
    name = "FreePressJournalSpider"
    allowed_domains = ["freepressjournal.in"]
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res



    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.fpj_bignews a,.fpj_newList a,.fpj_cityBigNews a,.fpj_cityNewsList a,#storyList li')
                for link in data:
                    try:
                        if "http" in str(link.css("a").attrib["href"]):
                            yield {"clean_url": self.allowed_domains[0],
                                   "base_url": response.url,
                                   "link": link.css("a").attrib["href"]
                                   }
                        else:
                            yield {"clean_url": self.allowed_domains[0],
                                   "base_url": response.url,
                                   "link": "https://www.firstpost.com" + link.css("a").attrib["href"]
                                   }

                    except:
                        pass


class hwnewsInSpider(scrapy.Spider):
    name = "hwnewsInSpider"
    allowed_domains = ["hwnews.in"]
    not_allowed_keyword = []

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res


    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.entry-title a')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "https://" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"],

                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://hwnews.in" + link.css("a").attrib["href"],

                                       }

                    except:
                        pass


class insidescienceSpider(scrapy.Spider):
    name = "insidescienceSpider"
    allowed_domains = ["insidescience.org"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.field--type-ds a')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "https://" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"],

                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www.insidescience.org" + link.css("a").attrib["href"],

                                       }

                    except:
                        pass



class NewsWeekSpider(scrapy.Spider):
    name = "NewsWeekSpider"
    allowed_domains = ["newsweek.com"]
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res



    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css("article :nth-child(2)")
                for link in data:
                    try:
                        if "https" in str(link.css("a").attrib["href"]):
                            yield {"base_url": response.url,
                                "link": link.css("a").attrib["href"]}
                        else:
                            yield {"base_url": response.url,
                                "link": "https://www.newsweek.com"
                                + link.css("a").attrib["href"]
                            }
                    except:
                        pass


class RepublicWorldSpider(scrapy.Spider):
    name = "RepublicWorldSpider"
    allowed_domains = ["republicworld.com"]
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css("article a")
                for link in data:
                    try:
                        if "https" in str(link.css("a").attrib["href"]):
                            yield {"base_url": response.url,
                                   "link": link.css("a").attrib["href"]}
                        else:
                            yield {"base_url": response.url,
                                   "link": "https://www.republicworld.com"
                                           + link.css("a").attrib["href"]
                                   }
                    except:
                        pass



class TechCrunchSpider(scrapy.Spider):
    name = "TechCrunchSpider"
    allowed_domains = ["techcrunch.com"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"proxy": proxy,"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res


    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('h2 a,h3 a')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]
                                       }


                            else:
                                yield {
                                       "base_url": response.url,
                                       "link": "https://techcrunch.com" + str(link.css("a").attrib["href"]),
                                       }

                    except:
                        pass


class economistSpider(scrapy.Spider):
    name = "economistSpider"
    allowed_domains = ["economist.com"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res



    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.eifj80y0 a,section .e16rqvvr0 a')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]}
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www.economist.com" + link.css("a").attrib["href"]}
                    except:
                        pass


class huffpostSpider(scrapy.Spider):
    name = "huffpostSpider"
    allowed_domains = ["huffpost.com"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('a.card__image__link')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"]}
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www.huffpost.com" + link.css("a").attrib["href"]}
                    except:
                        pass



class theprintSpider(scrapy.Spider):
    name = "theprintSpider"
    allowed_domains = ["theprint.in"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res



    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.td-module-title a')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "https://" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"],

                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://theprint.in" + link.css("a").attrib["href"],

                                       }

                    except:
                        pass


class thestatesmanSpider(scrapy.Spider):
    name = "thestatesmanSpider"
    allowed_domains = ["thestatesman.com"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res


    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.card__title a,.u-listing a')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"],

                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www.thestatesman.com" + link.css("a").attrib["href"],

                                       }

                    except:
                        pass



class newsforkidsSpider(scrapy.Spider):
    name = "newsforkidsSpider"
    allowed_domains = ["newsforkids.net"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res



    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.post-title')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "https://" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"],

                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://newsforkids.net" + link.css("a").attrib["href"],

                                       }

                    except:
                        pass



class AbcnewsArticleSpider(scrapy.Spider):
    name = "AbcnewsArticleSpider"
    allowed_domains = ["abcnews.go.com"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res


    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                llinks = []
                data = response.css(".VideoTile")
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        elif link.css("a").attrib["href"] not in llinks:
                            llinks.append(link.css("a").attrib["href"])
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"],
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://abcnews.go.com" + link.css("a").attrib["href"],
                                       }
                    except:
                        pass



class AbcrnewsArticleSpider(scrapy.Spider):
    name = "AbcrnewsArticleSpider"
    allowed_domains = ["abcrnews.com"]
    not_allowed_keyword = ['category/','/author']
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                llinks = []
                data = response.css(".grippy-host , #tdi_78 .td-module-title a,.td-module-meta-info a")
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        elif link.css("a").attrib["href"] not in llinks:
                            llinks.append(link.css("a").attrib["href"])
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"],
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://abcrnews.com" + link.css("a").attrib["href"],
                                       }
                    except:
                        pass

        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class EtvbharatArticleSpider(scrapy.Spider):
    name = "EtvbharatArticleSpider"
    allowed_domains = ["etvbharat.com"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                llinks = []
                data = response.css(".flex.justify-between")
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        elif link.css("a").attrib["href"] not in llinks:
                            llinks.append(link.css("a").attrib["href"])
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"],
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www.etvbharat.com" + link.css("a").attrib["href"],
                                       }
                    except:
                        pass

        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1



class OutlookindiaArticleSpider(scrapy.Spider):
    name = "OutlookindiaArticleSpider"
    allowed_domains = ["outlookindia.com"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"proxy": proxy,"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                llinks = []
                data = response.css("h2 a")
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        elif link.css("a").attrib["href"] not in llinks:
                            llinks.append(link.css("a").attrib["href"])
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"],
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www.outlookindia.com" + link.css("a").attrib["href"],
                                       }
                    except:
                        pass

        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1



class PtinewsArticleSpider(scrapy.Spider):
    name = "PtinewsArticleSpider"
    allowed_domains = ["ptinews.com"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                llinks = []
                data = response.css(".section-news-list a")
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        elif link.css("a").attrib["href"] not in llinks:
                            llinks.append(link.css("a").attrib["href"])
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"],
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www.ptinews.com" + link.css("a").attrib["href"],
                                       }
                    except:
                        pass

        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1




class FinancialexpressArticleSpider(scrapy.Spider):
    name = "FinancialexpressArticleSpider"
    allowed_domains = ["financialexpress.com"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                llinks = []
                data = response.css(".entry-title a")
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        elif link.css("a").attrib["href"] not in llinks:
                            llinks.append(link.css("a").attrib["href"])
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"],
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www.financialexpress.com" + link.css("a").attrib["href"],
                                       }
                    except:
                        pass

        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1
#6
class TelegraphindiaArticleSpider(scrapy.Spider):
    name = "TelegraphindiaArticleSpider"
    allowed_domains = ["telegraphindia.com"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                llinks = []
                data = response.css(".ellipsis_data_2")
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        elif link.css("a").attrib["href"] not in llinks:
                            llinks.append(link.css("a").attrib["href"])
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"],
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www.telegraphindia.com" + link.css("a").attrib["href"],
                                       }
                    except:
                        pass

        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1
#7
class ZeebizArticleSpider(scrapy.Spider):
    name = "ZeebizArticleSpider"
    allowed_domains = ["zeebiz.com"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                llinks = []
                data = response.css("#mostread a")
                data += response.css("#mostvideo a")
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        elif link.css("a").attrib["href"] not in llinks:
                            llinks.append(link.css("a").attrib["href"])
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"],
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www.zeebiz.com" + link.css("a").attrib["href"],
                                       }
                    except:
                        pass

        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1
#8
class MedicalxpressArticleSpider(scrapy.Spider):
    name = "MedicalxpressArticleSpider"
    allowed_domains = ["medicalxpress.com"]
    not_allowed_keyword = []

    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                llinks = []
                data = response.css(".news-link")

                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        elif link.css("a").attrib["href"] not in llinks:
                            llinks.append(link.css("a").attrib["href"])
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"],
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://medicalxpress.com" + link.css("a").attrib["href"],
                                       }
                    except:
                        pass

        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1
#9
class EntrackrArticleSpider(scrapy.Spider):
    name = "EntrackrArticleSpider"
    allowed_domains = ["entrackr.com"]
    not_allowed_keyword = []

    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                llinks = []
                data = response.css(".elementor-page-title a")

                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        elif link.css("a").attrib["href"] not in llinks:
                            llinks.append(link.css("a").attrib["href"])
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"],
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://entrackr.com" + link.css("a").attrib["href"],
                                       }
                    except:
                        pass

        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1
#10
class IndependentArticleSpider(scrapy.Spider):
    name = "IndependentArticleSpider"
    allowed_domains = ["independent.co.uk"]
    not_allowed_keyword = []

    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                llinks = []
                data = response.css(".jRhQrU .title")

                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        elif link.css("a").attrib["href"] not in llinks:
                            llinks.append(link.css("a").attrib["href"])
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"],
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www.independent.co.uk" + link.css("a").attrib["href"],
                                       }
                    except:
                        pass

        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1
#11
class ScienceThewireArticleSpider(scrapy.Spider):
    name = "ScienceThewireArticleSpider"
    allowed_domains = ["science.thewire.in"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                llinks = []
                data = response.css("h3 a")

                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        elif link.css("a").attrib["href"] not in llinks:
                            llinks.append(link.css("a").attrib["href"])
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"],
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://science.thewire.in" + link.css("a").attrib["href"],
                                       }
                    except:
                        pass

        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1
#12
class ThewireArticleSpider(scrapy.Spider):
    name = "ThewireArticleSpider"
    allowed_domains = ["thewire.in"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                llinks = []
                data = response.css(".card__title a")

                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        elif link.css("a").attrib["href"] not in llinks:
                            llinks.append(link.css("a").attrib["href"])
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"],
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://thewire.in" + link.css("a").attrib["href"],
                                       }
                    except:
                        pass

        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1
#13
class YourstoryArticleSpider(scrapy.Spider):
    name = "YourstoryArticleSpider"
    allowed_domains = ["yourstory.com"]
    not_allowed_keyword = []

    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                llinks = []
                data = response.css(".sc-kTvvXX.hQFavZ a,.sc-hAQHyU.eWUire a, .sc-kDVBPB.eCGvYH a")

                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        elif link.css("a").attrib["href"] not in llinks:
                            llinks.append(link.css("a").attrib["href"])
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"],
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://yourstory.com" + link.css("a").attrib["href"],
                                       }
                    except:
                        pass

        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1



class NYtimesSpider(scrapy.Spider):
    name = "NYtimesSpider"
    allowed_domains = ["nytimes.com"]
    check_ip_category = 0
    check_ip_article_links = 0
    check_ip_sub_category = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        res_ip = scrapy.Request(
            self.start_urls,
            meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
            callback=self.parse,
            dont_filter=True,
            errback=handle_error,
        )
        logging.info(
            {
                "proxy": "1",
                "clean_url": self.allowed_domains[0],
                "link": self.start_urls,
            }
        )
        yield res_ip

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):

                data = response.css(".e15t083i0 , .e1y0a3kv0 , .e1hr934v1,.css-9mylee,.ef62v182 a,.e27kk5e2 a,.e27kk5e0 a")
                for link in data:
                    try:
                        if "http" in str(
                            link.css("a").attrib["href"]
                        ):
                            yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                "link": link.css("a").attrib["href"]}
                        else:
                            yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                "link": "https://www.nytimes.com"
                                + link.css("a").attrib["href"]
                            }

                    except:
                        pass

        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls, callback=self.start_requests_ip)
                self.check_ip_category += 1



class washingtonpostSpider(scrapy.Spider):
    name = "washingtonpostSpider"
    allowed_domains = ["washingtonpost.com"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"proxy": proxy, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        res_ip = scrapy.Request(
            self.start_urls,
            meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
            callback=self.parse,
            dont_filter=True,
            errback=handle_error,
        )
        logging.info(
            {
                "proxy": "1",
                "clean_url": self.allowed_domains[0],
                "link": self.start_urls,
            }
        )
        yield res_ip

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.font--headline ,.font-size-sm')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "https://" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"],
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www.washingtonpost.com" + link.css("a").attrib["href"],
                                       }
                    except:
                        pass


class NbcNewsSpider(scrapy.Spider):
    name = "NbcNewsSpider"
    allowed_domains = ["nbcnews.com"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        res_ip = scrapy.Request(
            self.start_urls,
            meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
            callback=self.parse,
            dont_filter=True,
            errback=handle_error,
        )
        logging.info(
            {
                "proxy": "1",
                "clean_url": self.allowed_domains[0],
                "link": self.start_urls,
            }
        )
        yield res_ip

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.f6-m a , .styles_headline__ice3t a , .f8-m a,.wide-tease-item__info-wrapper a')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": link.css("a").attrib["href"],
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "link": "https://www.nbcnews.com" + link.css("a").attrib["href"],
                                       }
                    except:
                        pass


class EditionSpider(scrapy.Spider):
    name = "EditionSpider"
    allowed_domains = ["edition.cnn.com"]
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        res_ip = scrapy.Request(
            self.start_urls,
            meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
            callback=self.parse,
            dont_filter=True,
            errback=handle_error,
        )
        logging.info(
            {
                "proxy": "1",
                "clean_url": self.allowed_domains[0],
                "link": self.start_urls,
            }
        )
        yield res_ip

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):

                data = response.css(
                    '.cd__headline a,.container__link.container_lead-plus-headlines__link,.container__link.container__link')
                for link in data:
                    try:
                        if "https" in str(link.css("a").attrib["href"]):
                            yield {"clean_url": self.allowed_domains[0],
                                   "base_url": response.url,
                                   "link": link.css("a").attrib["href"]}
                        else:
                            yield {"clean_url": self.allowed_domains[0],
                                   "base_url": response.url,
                                   "link": "https://edition.cnn.com"
                                           + link.css("a").attrib["href"]
                                   }
                    except:
                        pass




class BbcKidsSpider(scrapy.Spider):
    name = "BbcKidsSpider"
    allowed_domains = ["bbc.co.uk"]
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        res_ip = scrapy.Request(
            self.start_urls,
            meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
            callback=self.parse,
            dont_filter=True,
            errback=handle_error,
        )
        logging.info(
            {
                "proxy": "1",
                "clean_url": self.allowed_domains[0],
                "link": self.start_urls,
            }
        )
        yield res_ip

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):

                data = response.css('.e1f5wbog1')
                for link in data:
                    try:
                        if "https" in str(link.css("a").attrib["href"]):
                            yield {"clean_url": self.allowed_domains[0],
                                   "base_url": response.url,
                                   "link": link.css("a").attrib["href"]}
                        else:
                            yield {"clean_url": self.allowed_domains[0],
                                   "base_url": response.url,
                                   "link": "https://www.bbc.co.uk"
                                           + link.css("a").attrib["href"]
                                   }
                    except:
                        pass



class FtNdSpider(scrapy.Spider):
    name = "FtNdSpider"
    allowed_domains = ["ft.com"]
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        res_ip = scrapy.Request(
            self.start_urls,
            meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
            callback=self.parse,
            dont_filter=True,
            errback=handle_error,
        )
        logging.info(
            {
                "proxy": "1",
                "clean_url": self.allowed_domains[0],
                "link": self.start_urls,
            }
        )
        yield res_ip

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):

                data = response.css(".o-teaser__heading,.headline--color-black a")
                for link in data:
                    try:
                        if "https" in str(link.css("a").attrib["href"]):
                            yield {"clean_url": self.allowed_domains[0],
                                   "base_url": response.url,
                                   "link": link.css("a").attrib["href"]}
                        else:
                            yield {"clean_url": self.allowed_domains[0],
                                   "base_url": response.url,
                                   "link": "https://www.ft.com"
                                           + link.css("a").attrib["href"]
                                   }
                    except:
                        pass




class AssembleeNationalePdfSpider(scrapy.Spider):
    name = "AssembleeNationalePdfSpider"
    allowed_domains = ["assemblee-nationale.fr"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0
    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy,"dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                llinks = []
                data = response.css('.liens-liste li')
                # data = response.css("ul li:nth-child(3) a")

                for link in data:
                    try:
                        if any(n in str(link.css("ul li:nth-child(3) a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("ul li:nth-child(3) a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                "title": link.css('h3::text').get() if link.css('h3::text') else 'na',
                                "excerpt": 'na',
                                "published_date":'na',
                                "pdf_url": link.css("ul li:nth-child(3) a").attrib["href"].replace("..", "")
                                }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                "title": link.css('h3::text').get() if link.css('h3::text') else 'na',
                                "excerpt": 'na',
                                "published_date":'na',
                                "pdf_url": "" + link.css("ul li:nth-child(3) a").attrib["href"].replace("..", "")
                                }
                    except:
                        pass



class AnsmSanteArticleSpider(scrapy.Spider):
    name = "AnsmSanteArticleSpider"
    allowed_domains = ["ansm.sante.fr"]
    not_allowed_keyword = ["informations-de-securite"]
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css(".col-sm-6 a")
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": self.start_urls[0] if not self.start_urls[0].endswith('/') else self.start_urls[0][:-1],
                                       "title": link.css('a').attrib['title'],
                                        "excerpt": 'na',
                                        "published_date":'na',
                                        "pdf_url": link.css("a").attrib["href"].replace("..", "")
                                       }

                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": self.start_urls[0] if not self.start_urls[0].endswith('/') else self.start_urls[0][:-1],
                                       "title": link.css('a').attrib['title'],
                                        "excerpt": 'na',
                                        "published_date":'na',
                                        "pdf_url": "https://ansm.sante.fr" + link.css("a").attrib["href"].replace("..", "")
                                       }

                    except:
                        pass

                    if response.css('.next a'):
                        next_page = response.css('.next a').attrib['href']
                        if 'http' in next_page:
                            yield(scrapy.Request(
                                        next_page,
                                        callback=self.parse,
                                        meta={"dont_retry": True, "download_timeout": timeout},
                                        errback=handle_error,
                                    ))
                        else:
                            yield(scrapy.Request(
                                    "https://ansm.sante.fr"+next_page,
                                    callback=self.parse,
                                    meta={"dont_retry": True, "download_timeout": timeout},
                                    errback=handle_error,
                                ))






        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1




class TextesJusticePdfSpider(scrapy.Spider):
    name = "TextesJusticePdfSpider"
    allowed_domains = ["textes.justice.gouv.fr"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={ "dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                categories_links = []

                res = response.css('.une_blck a')
                for r in res:
                    try:
                        if "http://www.textes.justice.gouv.fr/bulletin-officiel-10085/bulletin-officiel-du-31-decembre-2021-34227.html" in str(r.css("a").attrib["href"]):
                            break;
                        if "https://" in str(r.css("a").attrib["href"]) or "http://" in str(r.css("a").attrib["href"]):
                            categories_links.append(r.css("a").attrib["href"])

                        else:
                            categories_links.append(""+ r.css("a").attrib["href"])
                    except:
                        pass



                for i in set(categories_links):
                    try:
                        yield scrapy.Request(
                            i,
                            callback=self.article_links,
                            meta={"dont_retry": True, "download_timeout": cat_timeout,"base_url": response.url},
                            errback=handle_error,
                        )


                    except:
                        pass


    def article_links(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                base_url = response.meta.get('base_url')
                try:
                    data = response.css('p a')

                    for link in data:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if ".pdf" in str(link.css("a").attrib["href"]):
                                if "https://" in str(link.css("a").attrib["href"]):
                                    yield {"clean_url": self.allowed_domains[0],
                                       "base_url": base_url if not base_url.endswith('/') else base_url[:-1],
                                       "title": 'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": link.css("a").attrib["href"],
                                       }
                                else:
                                    yield {"clean_url": self.allowed_domains[0],
                                       "base_url": base_url if not base_url.endswith('/') else base_url[:-1],
                                       "title": 'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": "" + link.css("a").attrib["href"],
                                       }

                except:
                    pass




class CulturePdfSpider(scrapy.Spider):
    name = "CulturePdfSpider"
    allowed_domains = ["culture.gouv.fr"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                categories_links = []
                res = response.css('.doc-item-link')
                for r in res:
                    try:
                        if "https://" in str(r.css("a").attrib["href"]) or "http://" in str(r.css("a").attrib["href"]):
                            categories_links.append(r.css("a").attrib["href"])

                        else:
                            categories_links.append("" + r.css("a").attrib["href"])
                    except:
                        pass
                for i in set(categories_links):
                    try:
                        yield scrapy.Request(
                            i,
                            callback=self.article_links,
                            meta={"dont_retry": True, "download_timeout": cat_timeout, "base_url": self.start_urls[0]},
                            errback=handle_error,
                        )


                    except:
                        pass

                if response.css('.page-item.next  a'):
                    next_page = response.css('.page-item.next  a').attrib['href']
                    if 'http' in response.css('.page-item.next  a').attrib['href']:
                        yield scrapy.Request(
                            next_page,
                            callback=self.parse,
                            meta={"dont_retry": True, "download_timeout": cat_timeout, "base_url": response.url},
                            errback=handle_error,
                        )
                    else:
                        yield scrapy.Request(
                            "https://www.culture.gouv.fr" + next_page,
                            callback=self.parse,
                            meta={"dont_retry": True, "download_timeout": cat_timeout, "base_url": response.url},
                            errback=handle_error,
                        )

    def article_links(self, response):
        if str(response.status) == "200":
            if response.css("body"):

                base_url = response.meta.get('base_url')
                try:
                    data = response.css('.file-link a')

                    for link in data:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "https://" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": base_url if not base_url.endswith('/') else base_url[:-1],
                                       "title": link.css('a').attrib['title'],
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": link.css("a").attrib["href"],
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": base_url if not base_url.endswith('/') else base_url[:-1],
                                       "title": link.css('a').attrib['title'],
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": "https://www.culture.gouv.fr" + link.css("a").attrib["href"],
                                       }

                except:
                    pass




class AutoriteTransportsPdfSpider(scrapy.Spider):
    name = "AutoriteTransportsPdfSpider"
    allowed_domains = ["autorite-transports.fr"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                llinks = []

                data = response.css("h1 a")
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if str(link.css("a").attrib['href']) not in llinks:
                                llinks.append(str(link.css('a').attrib['href']))
                                if "http" in str(link.css("a").attrib["href"]):
                                    yield {"clean_url": self.allowed_domains[0],
                                           "base_url": self.start_urls[0] if not self.start_urls[0].endswith('/') else
                                           self.start_urls[0][:-1],
                                           "title": link.css("a::text").get() if link.css("a::text") else 'na',
                                           "excerpt": 'na',
                                           "published_date": 'na',
                                           "pdf_url": link.css("a").attrib["href"].replace("..", "")
                                           }
                                else:
                                    yield {"clean_url": self.allowed_domains[0],
                                           "base_url": self.start_urls[0] if not self.start_urls[0].endswith('/') else
                                           self.start_urls[0][:-1],
                                           "title": link.css("a::text").get() if link.css("a::text") else 'na',
                                           "excerpt": 'na',
                                           "published_date": 'na',
                                           "pdf_url": "" + link.css("a").attrib["href"].replace("..", "")
                                           }
                    except:
                        pass

                for i in range(2, 62):
                    next_page = f"https://www.autorite-transports.fr/avis-et-decisions/?fwp_paged={i}"
                    yield scrapy.Request(
                        next_page,
                        callback=self.parse,
                        meta={"dont_retry": True, "download_timeout": cat_timeout, "base_url": response.url},
                        errback=handle_error,
                    )


        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1


class ArcomPdfSpider(scrapy.Spider):
    name = "ArcomPdfSpider"
    allowed_domains = ["arcom.fr"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.views-field-title a')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": self.start_urls[0] if not self.start_urls[0].endswith('/') else
                                       self.start_urls[0][:-1],
                                       "title": link.css("a::text").get() if link.css("a::text") else 'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": link.css("a").attrib["href"].replace("..", "")
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": self.start_urls[0] if not self.start_urls[0].endswith('/') else
                                       self.start_urls[0][:-1],
                                       "title": link.css("a::text").get() if link.css("a::text") else 'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": "https://www.arcom.fr" + link.css("a").attrib["href"].replace("..",
                                                                                                                "")
                                       }
                    except:
                        pass

                if response.css('.pager__item--next .page-link'):
                    next_page = response.css('.pager__item--next .page-link').attrib['href']
                    if 'http' in next_page:
                        yield scrapy.Request(
                            next_page,
                            callback=self.parse,
                            meta={"dont_retry": True, "download_timeout": cat_timeout, "base_url": response.url},
                            errback=handle_error,
                        )
                    else:
                        yield scrapy.Request(
                            "https://www.arcom.fr/nos-ressources/espace-juridique/decisions" + next_page,
                            callback=self.parse,
                            meta={"dont_retry": True, "download_timeout": cat_timeout, "base_url": response.url},
                            errback=handle_error,
                        )

        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1




class ArcomPdf2Spider(scrapy.Spider):
    name = "ArcomPdf2Spider"
    allowed_domains = ["arcom.fr"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.views-field-title a')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": self.start_urls[0] if not self.start_urls[0].endswith('/') else
                                       self.start_urls[0][:-1],
                                       "title": link.css("a::text").get() if link.css("a::text") else 'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": link.css("a").attrib["href"].replace("..", "")
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": self.start_urls[0] if not self.start_urls[0].endswith('/') else
                                       self.start_urls[0][:-1],
                                       "title": link.css("a::text").get() if link.css("a::text") else 'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": "https://www.arcom.fr" + link.css("a").attrib["href"].replace("..",
                                                                                                                "")
                                       }
                    except:
                        pass

                if response.css('.pager__item--next .page-link'):
                    next_page = response.css('.pager__item--next .page-link').attrib['href']
                    if 'http' in next_page:
                        yield scrapy.Request(
                            next_page,
                            callback=self.parse,
                            meta={"dont_retry": True, "download_timeout": cat_timeout, "base_url": response.url},
                            errback=handle_error,
                        )
                    else:
                        yield scrapy.Request(
                            "https://www.arcom.fr/nos-ressources/espace-juridique/textes-juridiques" + next_page,
                            callback=self.parse,
                            meta={"dont_retry": True, "download_timeout": cat_timeout, "base_url": response.url},
                            errback=handle_error,
                        )

        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1




class NvwaPdfSpider(scrapy.Spider):
    name = "NvwaPdfSpider"
    allowed_domains = ["nvwa.nl"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                categories_links = []
                res = response.css('.publication')
                for r in res:
                    try:
                        if "https://" in str(r.css("a").attrib["href"]) or "http://" in str(r.css("a").attrib["href"]):
                            categories_links.append(r.css("a").attrib["href"])

                        else:
                            categories_links.append("https://www.nvwa.nl" + r.css("a").attrib["href"])
                    except:
                        pass
                for i in set(categories_links):
                    try:
                        yield scrapy.Request(
                            i,
                            callback=self.article_links,
                            meta={"dont_retry": True, "download_timeout": cat_timeout, "base_url": self.start_urls[0]},
                            errback=handle_error, )
                    except:
                        pass

                if response.css('.paging__unit.paging__unit--next a'):
                    next_page = response.css('.paging__unit.paging__unit--next a').attrib['href']
                    if 'http' in next_page:
                        yield scrapy.Request(
                            next_page,
                            callback=self.parse,
                            meta={"dont_retry": True, "download_timeout": cat_timeout, "base_url": response.url},
                            errback=handle_error,
                        )
                    else:
                        yield scrapy.Request(
                            "https://www.nvwa.nl" + next_page,
                            callback=self.parse,
                            meta={"dont_retry": True, "download_timeout": cat_timeout, "base_url": response.url},
                            errback=handle_error,
                        )

    def article_links(self, response):
        if str(response.status) == "200":
            if response.css("body"):

                base_url = response.meta.get('base_url')
                try:
                    data = response.css('.download-chunk')

                    for link in data:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "https://" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": base_url if not base_url.endswith('/') else base_url[:-1],
                                       "title": link.css("h2::text").get().strip().replace("Download", ""),
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": link.css("a").attrib["href"],
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": base_url if not base_url.endswith('/') else base_url[:-1],
                                       "title": link.css("h2::text").get().strip().replace("Download", ""),
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": "https://www.nvwa.nl" + link.css("a").attrib["href"],
                                       }

                except:
                    pass





class GovernmentPdfSpider(scrapy.Spider):
    name = "GovernmentPdfSpider"
    allowed_domains = ["government.nl"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                categories_links = []
                res = response.css('.publication')
                for r in res:
                    try:
                        if "https://" in str(r.css("a").attrib["href"]) or "http://" in str(r.css("a").attrib["href"]):
                            categories_links.append(r.css("a").attrib["href"])

                        else:
                            categories_links.append("https://www.government.nl" + r.css("a").attrib["href"])
                    except:
                        pass
                for i in set(categories_links):
                    try:
                        yield scrapy.Request(
                            i,
                            callback=self.article_links,
                            meta={"dont_retry": True, "download_timeout": cat_timeout, "base_url": self.start_urls[0]},
                            errback=handle_error, )
                    except:
                        pass

                if response.css('.paging__unit.paging__unit--next a'):
                    next_page = response.css('.paging__unit.paging__unit--next a').attrib['href']
                    if 'http' in next_page:
                        yield scrapy.Request(
                            next_page,
                            callback=self.parse,
                            meta={"dont_retry": True, "download_timeout": cat_timeout, "base_url": response.url},
                            errback=handle_error,
                        )
                    else:
                        yield scrapy.Request(
                            "https://www.government.nl" + next_page,
                            callback=self.parse,
                            meta={"dont_retry": True, "download_timeout": cat_timeout, "base_url": response.url},
                            errback=handle_error,
                        )

    def article_links(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                base_url = response.meta.get('base_url')
                try:
                    data = response.css('.download-chunk')

                    for link in data:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": base_url if not base_url.endswith('/') else base_url[:-1],
                                       "title": link.css("h2::text").get().strip().replace("Download", "") if link.css(
                                           "h2::text") else 'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": link.css("a").attrib["href"],
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": base_url if not base_url.endswith('/') else base_url[:-1],
                                       "title": link.css("h2::text").get().strip().replace("Download", "") if link.css(
                                           "h2::text") else 'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": "https://www.government.nl" + link.css("a").attrib["href"],
                                       }

                except:
                    pass



class AcmPdfSpider(scrapy.Spider):
    name = "AcmPdfSpider"
    allowed_domains = ["acm.nl"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css(".search-result-item__title a")
                for link in data:
                    if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                        pass
                    else:
                        if "http" in str(link.css("a").attrib["href"]):
                            yield {"clean_url": self.allowed_domains[0],
                                   "base_url": self.start_urls[0] if not self.start_urls[0].endswith('/') else
                                   self.start_urls[0][:-1],
                                   "title": link.css("a::text").get() if link.css("a::text") else 'na',
                                   "excerpt": 'na',
                                   "published_date": 'na',
                                   "pdf_url": link.css("a").attrib["href"],
                                   }
                        else:
                            yield {"clean_url": self.allowed_domains[0],
                                   "base_url": self.start_urls[0] if not self.start_urls[0].endswith('/') else
                                   self.start_urls[0][:-1],
                                   "title": link.css("a::text").get() if link.css("a::text") else 'na',
                                   "excerpt": 'na',
                                   "published_date": 'na',
                                   "pdf_url": "https://www.acm.nl" + link.css("a").attrib["href"],
                                   }

                if response.css('.m-pager__next'):
                    next_page = response.css('.m-pager__next').attrib['href']
                    if 'http' in next_page:
                        yield scrapy.Request(
                            next_page,
                            callback=self.parse,
                            meta={"dont_retry": True, "download_timeout": cat_timeout, "base_url": response.url},
                            errback=handle_error,
                        )
                    else:
                        yield scrapy.Request(
                            "https://www.acm.nl/en/publications/search-publications" + next_page,
                            callback=self.parse,
                            meta={"dont_retry": True, "download_timeout": cat_timeout, "base_url": response.url},
                            errback=handle_error,
                        )



class AutoriteitpersoonsgegevensPdfSpider(scrapy.Spider):
    name = "AutoriteitpersoonsgegevensPdfSpider"
    allowed_domains = ["autoriteitpersoonsgegevens.nl"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy,"dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                llinks = []
                data = response.css("#main-content a")
                for link in data:
                    try:
                            if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                                pass
                            else:
                                if str(link.css("a").attrib['href']) not in llinks:
                                    llinks.append(str(link.css('a').attrib['href']))
                                    if "http" in str(link.css("a").attrib["href"]):
                                        yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title": 'na',
                                       "excerpt": 'na',
                                       "published_date":'na',
                                       "pdf_url": link.css("a").attrib["href"].replace("..", "")
                                       }
                                    else:
                                        yield {"clean_url": self.allowed_domains[0],
                                        "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title": 'na',
                                       "excerpt": 'na',
                                       "published_date":'na',
                                       "pdf_url": "https://autoriteitpersoonsgegevens.nl" + link.css("a").attrib["href"].replace("..", "")
                                       }
                    except:
                        pass

        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1




class Government2PdfSpider(scrapy.Spider):
    name = "Government2PdfSpider"
    allowed_domains = ["government.nl"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                categories_links = []
                res = response.css('.publication')
                for r in res:
                    try:
                        if "https://" in str(r.css("a").attrib["href"]) or "http://" in str(r.css("a").attrib["href"]):
                            categories_links.append(r.css("a").attrib["href"])

                        else:
                            categories_links.append("https://www.government.nl" + r.css("a").attrib["href"])
                    except:
                        pass
                for i in set(categories_links):
                    try:
                        yield scrapy.Request(
                            i,
                            callback=self.article_links,
                            meta={"dont_retry": True, "download_timeout": cat_timeout,
                                  "base_url": response.url if response.url != "https://www.government.nl/documents?type=Regulation&page=2" else "https://www.government.nl/documents?keyword=&start-date=&end-date=&issue=All+topics&element=All+ministries&type=Regulation"},
                            errback=handle_error, )
                    except:
                        pass

                if response.css('.paging__unit.paging__unit--next a'):
                    next_page = response.css('.paging__unit.paging__unit--next a').attrib['href']
                    if 'http' in next_page:
                        yield scrapy.Request(
                            next_page,
                            callback=self.parse,
                            meta={"dont_retry": True, "download_timeout": cat_timeout, "base_url": response.url},
                            errback=handle_error,
                        )
                    else:
                        yield scrapy.Request(
                            "https://www.government.nl" + next_page,
                            callback=self.parse,
                            meta={"dont_retry": True, "download_timeout": cat_timeout, "base_url": response.url},
                            errback=handle_error,
                        )

    def article_links(self, response):
        if str(response.status) == "200":
            if response.css("body"):

                base_url = response.meta.get('base_url')
                try:
                    data = response.css('.download-chunk')

                    for link in data:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "https://" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": base_url if not base_url.endswith('/') else base_url[:-1],
                                       "title": link.css("h2::text").get().strip().replace("Download", ""),
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": link.css("a").attrib["href"],
                                       }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": base_url if not base_url.endswith('/') else base_url[:-1],
                                       "title": link.css("h2::text").get().strip().replace("Download", ""),
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": "https://www.government.nl" + link.css("a").attrib["href"],
                                       }

                except:
                    pass




class MjusticiaPdfSpider(scrapy.Spider):
    name = "MjusticiaPdfSpider"
    allowed_domains = ["mjusticia.gob.es"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy,"dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                llinks = []
                data = response.css("#ctl00_PlaceHolderMain_ctl08_ctl00__ControlWrapper_RichHtmlField a")
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if str(link.css("a").attrib['href']) not in llinks:
                                llinks.append(str(link.css('a').attrib['href']))
                                if "http" in str(link.css("a").attrib["href"]):
                                    yield {"clean_url": self.allowed_domains[0],
                                    "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                    "title": link.css("a::text").get() if link.css("a::text") else 'na',
                                    "excerpt": 'na',
                                    "published_date":'na',
                                    "pdf_url": link.css("a").attrib["href"].replace("..", "")
                                    }
                                else:
                                    yield {"clean_url": self.allowed_domains[0],
                                    "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                    "title": link.css("a::text").get() if link.css("a::text") else 'na',
                                    "excerpt": 'na',
                                    "published_date":'na',
                                    "pdf_url": "" + link.css("a").attrib["href"].replace("..", "")
                                    }
                    except:
                        pass

        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1




class SenadoPdfSpider(scrapy.Spider):
    name = "SenadoPdfSpider"
    allowed_domains = ["senado.es"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):

                data = response.css(".p_top a")
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "http" in str(link.css("a").attrib["href"]):
                                    yield {"clean_url": self.allowed_domains[0],
                                    "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                    "title": link.css("a::text").get() if link.css("a::text") else 'na',
                                    "excerpt": 'na',
                                    "published_date":'na',
                                    "pdf_url": link.css("a").attrib["href"].replace("..", "")
                                    }

                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url,
                                       "title": link.css("a::text").get() if link.css("a::text") else 'na',
                                        "excerpt": 'na',
                                        "published_date":'na',
                                       "pdf_url": "https://www.senado.es" + link.css("a").attrib["href"],
                                       }

                    except:
                        pass



        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1




class CnmvPdfSpider(scrapy.Spider):
    name = "CnmvPdfSpider"
    allowed_domains = ["cnmv.es"]
    not_allowed_keyword = ['extension:']
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy,"dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                llinks = []

                data = response.css(".P_como_li a")

                for link in data:
                    try:

                            if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                                pass
                            else:
                                if str(link.css("a").attrib['href']) not in llinks:
                                    llinks.append(str(link.css('a').attrib['href']))
                                    if "http" in str(link.css("a").attrib["href"]):
                                        yield {"clean_url": self.allowed_domains[0],
                                       "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title": link.css("a::text").get() if link.css("a::text") else 'na',
                                       "excerpt": 'na',
                                       "published_date":'na',
                                       "pdf_url": link.css("a").attrib["href"].replace("..", "")
                                       }
                                    else:
                                        yield {"clean_url": self.allowed_domains[0],
                                        "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                       "title": link.css("a::text").get() if link.css("a::text") else 'na',
                                       "excerpt": 'na',
                                       "published_date":'na',
                                       "pdf_url": "" + link.css("a").attrib["href"].replace("..", "")
                                       }

                    except:
                        pass


        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1



class Autoriteitpersoonsgegevens2PdfSpider(scrapy.Spider):
    name = "Autoriteitpersoonsgegevens2PdfSpider"
    allowed_domains = ["autoriteitpersoonsgegevens.nl"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                category_links = []
                res = response.css("#main-content a")
                for r in res:
                    try:
                        if "https://" in str(r.css("a").attrib["href"]) or "http://" in str(r.css("a").attrib["href"]):
                            category_links.append(r.css("a").attrib["href"])

                        else:
                            category_links.append("https://autoriteitpersoonsgegevens.nl" + r.css("a").attrib["href"])
                    except:
                        pass
                for i in set(category_links):
                    try:
                        yield scrapy.Request(
                            i,
                            callback=self.article_links,
                            meta={"dont_retry": True, "download_timeout": cat_timeout,
                                  "base_url": response.url},
                            errback=handle_error, )
                    except:
                        pass



    def article_links(self,response):
        base_url = response.meta.get('base_url')
        try:
            data = response.css(".article-list li a")
        except:
            data = []
        for link in data:
            try:
                if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                    pass
                else:
                    if '.pdf' in str(link.css("a").attrib["href"]):
                        if "http" in str(link.css("a").attrib["href"]):
                            yield {"clean_url": self.allowed_domains[0],
                                "base_url": base_url if not base_url.endswith('/') else base_url[:-1],
                                "title": link.css("a::text").get() if link.css("a::text") else 'na',
                                "excerpt": 'na',
                                "published_date":'na',
                                "pdf_url": link.css("a").attrib["href"].replace("..", "")
                                }
                        else:
                            yield {"clean_url": self.allowed_domains[0],
                                "base_url": base_url if not base_url.endswith('/') else base_url[:-1],
                                    "title": link.css("a::text").get() if link.css("a::text") else 'na',
                                    "excerpt": 'na',
                                    "published_date":'na',
                                    "pdf_url": "https://autoriteitpersoonsgegevens.nl" + link.css("a").attrib["href"],
                                    }

            except:
                pass



class MitmaArticleSpider(scrapy.Spider):
    name = "MitmaArticleSpider"
    allowed_domains = ["mitma.es",'boe.es']
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                category_links = []
                res = response.css(".container_generico a")
                for r in res:
                    try:
                        if "https://" in str(r.css("a").attrib["href"]) or "http://" in str(r.css("a").attrib["href"]):
                            category_links.append(r.css("a").attrib["href"])

                        else:
                            category_links.append("https://www.mitma.es" + r.css("a").attrib["href"])
                    except:
                        pass
                for i in set(category_links):
                    try:
                        yield scrapy.Request(
                            i,
                            callback=self.article_links,
                            meta={"dont_retry": True, "download_timeout": cat_timeout,
                                  "base_url": response.url},
                            errback=handle_error, )
                    except:
                        pass
    def article_links(self,response):
        base_url = response.meta.get('base_url')
        try:
            data = response.css(".puntoPDF2 a")
        except:
            data = []
        for link in data:
            try:
                if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                    pass
                else:
                    if '.pdf' in str(link.css("a").attrib["href"]):
                        if "http" in str(link.css("a").attrib["href"]):
                            yield {"clean_url": self.allowed_domains[0],
                                "base_url": base_url if not base_url.endswith('/') else base_url[:-1],
                                "title": link.css("a::text").get() if link.css("a::text") else 'na',
                                "excerpt": 'na',
                                "published_date":'na',
                                "pdf_url": link.css("a").attrib["href"].replace("..", "")
                                }
                        else:
                            yield {"clean_url": self.allowed_domains[0],
                                "base_url": base_url if not base_url.endswith('/') else base_url[:-1],
                                    "title": link.css("a::text").get() if link.css("a::text") else 'na',
                                    "excerpt": 'na',
                                    "published_date":'na',
                                    "pdf_url": "https://www.mitma.es" + link.css("a").attrib["href"],
                                    }

            except:
                pass


class BoeRssSpider(scrapy.Spider):
    name = "BoeRssSpider"
    allowed_domains = ["boe.es"]
    not_allowed_keyword = ['extension:']
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            data = response.css('item')

            for link in data:
                try:
                    if any(n in str(link.css("guid::text").get()) for n in self.not_allowed_keyword):
                        pass
                    else:

                        if "http" in str(link.css("guid::text").get()):
                            yield {"clean_url": self.allowed_domains[0],
                                   "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                   "title": link.css("title::text").get() if link.css("title::text") else 'na',
                                   "excerpt": link.css("description::text").get() if link.css(
                                       "description::text") else 'na',
                                   "published_date": link.css("pubDate::text").get() if link.css(
                                       "pubDate::text") else 'na',
                                   "pdf_url": link.css("guid::text").get()
                                   }
                        else:
                            yield {"clean_url": self.allowed_domains[0],
                                   "base_url": response.url if not response.url.endswith('/') else response.url[:-1],
                                   "title": link.css("title::text").get() if link.css("title::text") else 'na',
                                   "excerpt": link.css("description::text").get() if link.css(
                                       "description::text") else 'na',
                                   "published_date": link.css("pubDate::text").get() if link.css(
                                       "pubDate::text") else 'na',
                                   "pdf_url": "https://boe.es" + link.css("guid::text").get()
                                   }

                except:
                    pass


        else:
            while self.check_ip_category < 2:
                yield response.follow(self.start_urls[0], callback=self.start_requests_ip)
                self.check_ip_category += 1




class SenatPdfSpider(scrapy.Spider):
    name = "SenatPdfSpider"
    allowed_domains = ["senat.fr"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={ "dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                data = response.css('.box-type-02:nth-child(5) :nth-child(1)')
                for link in data:
                    try:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                            if "https://" in str(link.css("a").attrib["href"]):
                                yield {"clean_url": self.allowed_domains[0],
                                    "base_url": self.start_urls[0] if not self.start_urls[0].endswith('/') else self.start_urls[0][:-1],
                                    "title": link.css("a::text").get() if link.css(
                                        "a::text") else 'na',
                                    "excerpt": 'na',
                                    "published_date": 'na',
                                    "pdf_url": link.css("a").attrib["href"],
                                    }
                            else:
                                yield {"clean_url": self.allowed_domains[0],
                                    "base_url": self.start_urls[0] if not self.start_urls[0].endswith('/') else self.start_urls[0][:-1],
                                    "title": link.css("a::text").get() if link.css(
                                        "a::text") else 'na',
                                    "excerpt": 'na',
                                    "published_date": 'na',
                                    "pdf_url": "http://www.senat.fr" + link.css("a").attrib[
                                        "href"],
                                    }
                    except:
                        pass




class Senado3PdfSpider(scrapy.Spider):
    name = "Senado3PdfSpider"
    allowed_domains = ["senado.es"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:

            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                categories_links = []
                res = response.css('.text_b_c2')
                for r in res:
                    try:
                        if "https://" in str(r.css("a").attrib["href"]) or "http://" in str(r.css("a").attrib["href"]):
                            categories_links.append(r.css("a").attrib["href"])
                        else:
                            categories_links.append(""+ r.css("a").attrib["href"])
                    except:
                        pass

                for i in set(categories_links):
                    try:
                        yield scrapy.Request(
                            i,
                            callback=self.article_links,
                            meta={"dont_retry": True, "download_timeout": cat_timeout,"base_url": response.url},
                            errback=handle_error,)
                    except:
                        pass


    def article_links(self, response):
        if str(response.status) == "200":
            if response.css("body"):

                base_url = response.meta.get('base_url')
                try:
                    data = response.css('.text_s_c2')

                    for link in data:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                                if "http" in str(link.css("a").attrib["href"]):
                                    yield {"clean_url": self.allowed_domains[0],
                                       "base_url": base_url if not base_url.endswith('/') else base_url[:-1],
                                       "title": link.css("a::text").get().strip(),
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": link.css("a").attrib["href"],
                                       }
                                else:
                                    yield {"clean_url": self.allowed_domains[0],
                                       "base_url": base_url if not base_url.endswith('/') else base_url[:-1],
                                       "title": link.css("a::text").get().strip(),
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": "https://www.senado.es/" + link.css("a").attrib["href"],
                                       }

                except:
                    pass




class WettenOverheidPdfSpider(scrapy.Spider):
    name = "WettenOverheidPdfSpider"
    allowed_domains = ["wetten.overheid.nl"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0
    start_urls = ["https://wetten.overheid.nl/zoeken/zoekresultaat/rs/2,3,4,7,9/titelf/1/tekstf/1/d/01-02-2023/dx/0"]

    def start_requests(self):
        for i in self.start_urls:

            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                categories_links = []
                res = response.css(".result--title")
                for r in res:
                    try:
                        if "http" in str(r.css("a").attrib["href"]) or "http://" in str(r.css("a").attrib["href"]):
                            categories_links.append(r.css("a").attrib["href"])

                        else:
                            categories_links.append("https://wetten.overheid.nl"+ r.css("a").attrib["href"])
                    except:
                        pass

                for i in set(categories_links):
                    try:
                        yield scrapy.Request(
                            i,
                            callback=self.article_links,
                            meta={"dont_retry": True, "download_timeout": cat_timeout,"base_url": response.url},
                            errback=handle_error,)
                    except:
                        pass

                if response.css('.next a'):
                    next_page = response.css('.next a').attrib['href']
                    if 'http' in next_page:
                        yield scrapy.Request(
                            next_page,
                            callback=self.parse,
                            meta={"dont_retry": True, "download_timeout": cat_timeout, "base_url": response.url},
                            errback=handle_error,
                        )
                    else:
                        yield scrapy.Request(
                            "https://wetten.overheid.nl" + next_page,
                            callback=self.parse,
                            meta={"dont_retry": True, "download_timeout": cat_timeout, "base_url": response.url},
                            errback=handle_error,
                        )



    def article_links(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                base_url = response.meta.get('base_url')
                print(base_url)
                try:
                    data = response.css('.popupexporteren')
                    for link in data:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                                if "http" in str(link.css("a").attrib["href"]):
                                    yield {"clean_url": self.allowed_domains[0],
                                       "base_url": base_url if not base_url.endswith('/') else base_url[:-1],
                                       "title": 'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": link.css("a").attrib["href"],
                                       }
                                else:
                                    yield {"clean_url": self.allowed_domains[0],
                                       "base_url": base_url if not base_url.endswith('/') else base_url[:-1],
                                       "title": 'na',
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": "https://wetten.overheid.nl" + link.css("a").attrib["href"],
                                       }

                except:
                    pass





class HasSanteRssSpider(scrapy.Spider):
    name = "HasSanteRssSpider"
    allowed_domains = ["has-sante.fr"]
    not_allowed_keyword = []
    check_ip_category = 0
    check_ip_article_links = 0

    def start_requests(self):
        for i in self.start_urls:
            res = scrapy.Request(
                i,
                callback=self.parse,
                dont_filter=True,
                meta={"dont_retry": True, "download_timeout": timeout},
                errback=handle_error,
            )
            yield res

    def start_requests_ip(self, arg):
        for i in self.start_urls:
            res_ip = scrapy.Request(
                i,
                meta={"proxy": proxy, "dont_retry": True, "download_timeout": timeout},
                callback=self.parse,
                dont_filter=True,
                errback=handle_error,
            )
            yield res_ip

            logging.info(
                {
                    "proxy": "1",
                    "clean_url": self.allowed_domains[0],
                    "link": i,
                }
            )

    def parse(self, response):
        if str(response.status) == "200":
            categories_links = []
            res = response.css('item')
            for r in res:
                try:
                    if "http" in str(r.css('guid::text').get()):
                        categories_links.append(r.css('guid::text').get())
                    else:
                        categories_links.append("https://www.has-sante.fr"+ r.css('guid::text').get())
                except:
                    pass

            for i in set(categories_links):
                try:
                    yield scrapy.Request(
                        i,
                        callback=self.article_links,
                        meta={"dont_retry": True, "download_timeout": cat_timeout,"base_url": response.url},
                        errback=handle_error,)
                except:
                    pass



    def article_links(self, response):
        if str(response.status) == "200":
            if response.css("body"):
                base_url = response.meta.get('base_url')
                try:
                    data = response.css('.pdf .ctxTooltipCard')
                    for link in data:
                        if any(n in str(link.css("a").attrib["href"]) for n in self.not_allowed_keyword):
                            pass
                        else:
                                if "http" in str(link.css("a").attrib["href"]):
                                    yield {"clean_url": self.allowed_domains[0],
                                       "base_url": base_url if not base_url.endswith('/') else base_url[:-1],
                                       "title": link.css("a::text").get().strip(),
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": link.css("a").attrib["href"],
                                       }
                                else:
                                    yield {"clean_url": self.allowed_domains[0],
                                       "base_url": base_url if not base_url.endswith('/') else base_url[:-1],
                                       "title": link.css("a::text").get().strip(),
                                       "excerpt": 'na',
                                       "published_date": 'na',
                                       "pdf_url": "https://www.has-sante.fr/" + link.css("a").attrib["href"],
                                       }

                except:
                    pass